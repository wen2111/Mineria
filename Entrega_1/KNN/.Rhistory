train_df<- subset(train_df, select = -c(ID, Surname, group))
train_df$Exited <- factor(train_df$Exited, levels = c("1","0"))
ind_col <- which(names(train_df) == "Exited") # columna de indicador
default_idx <- createDataPartition(train_df$Exited, p = 0.7, list = FALSE) # 70% para entrenamiento
X_trainC <- train_df[default_idx, ]
X_testC  <- train_df[-default_idx, ]
y_testC <- X_testC[, ind_col]
X_testC <- X_testC[, -ind_col]
# muestra <- sample(1:nrow(train_df), size = nrow(train_df)/3)
# train<-data.frame(train_df[-muestra,])
# test<-data.frame(train_df[muestra,])
# cl<-data$Exited[-muestra]  # variable output que queremos pronosticar.
library(caret)
# Crear preprocesador para centrado y escalado
preproc <- preProcess(X_trainC, method = c("center", "scale"))
# Aplicar transformación
X_trainC <- predict(preproc, X_trainC)
X_testC <- predict(preproc, X_testC)
y_trainC <- factor(ifelse(as.character(X_trainC$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_testC  <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = levels(y_trainC))
X_train_feat <- X_trainC[, setdiff(names(X_trainC), "Exited")]
X_test_feat  <- X_testC
# Convertir variables categóricas en dummies
dmy <- dummyVars(~ ., data = X_train_feat, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_train_feat))
test_num  <- data.frame(predict(dmy, newdata = X_test_feat))
# Escalar las variables numéricas
preproc <- preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
# kNN CV
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
head(predict(entrenamiento, newdata = X_testC, type = "prob"), n = 10)
save.image(file = "kNNEnvironment.RData")
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
pred_label <- predict(entrenamiento, newdata = testScaled)
head(pred_prob, 10)
pred_label
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/Entrega_1/KNN NaiveBayes/kNNEnvironment.RData")
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
cm
library(caret)
set.seed(123)
ctrl_up <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
sampling = "up",                 # ← oversampling dentro de cada fold
summaryFunction = twoClassSummary
)
fit_knn_up <- train(
x = trainScaled, y = y_trainC,
method = "knn",
metric = "ROC",                  # optimiza por AUC, mejor para desbalanceo
trControl = ctrl_up,
tuneGrid = expand.grid(k = seq(1, 21, by = 2))
)
# Predicción
pred_prob_up <- predict(fit_knn_up, newdata = testScaled, type = "prob")
pred_lab_up  <- predict(fit_knn_up, newdata = testScaled)
# Evaluación (umbral 0.5)
cm_up <- confusionMatrix(pred_lab_up, y_testC, positive = "Yes")
cm_up
# Si quieres ajustar umbral (ej. 0.30) para subir Sensitivity
tau <- 0.30
pred_tau <- factor(ifelse(pred_prob_up$Yes >= tau, "Yes", "No"), levels = levels(y_testC))
confusionMatrix(pred_tau, y_testC, positive = "Yes")
pred_lab_up
cm_up
confusionMatrix(pred_tau, y_testC, positive = "Yes")
# y split con default_idx
X_train_rf <- train_df[default_idx, ]
X_test_rf  <- train_df[-default_idx, ]
# Objetivo con niveles válidos
X_train_rf$Exited <- factor(ifelse(as.character(X_train_rf$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_test_rf <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))
# Asegura que todos los predictores no sean character:
chr_cols <- sapply(X_train_rf, is.character)
X_train_rf[chr_cols] <- lapply(X_train_rf[chr_cols], factor)
X_test_rf[ chr_cols] <- lapply(X_test_rf[ chr_cols], factor)
set.seed(42)
ctrl_up <- trainControl(
method = "repeatedcv", number = 10, repeats = 10,
verboseIter = FALSE,
classProbs = TRUE,
sampling = "up",                # ← oversampling
summaryFunction = twoClassSummary
)
# Entrenamiento RF (puedes usar "rf" o "ranger")
model_rf_over <- train(
Exited ~ ., data = X_train_rf,
method = "rf",
preProcess = NULL,              # RF no necesita center/scale
trControl = ctrl_up,
metric = "ROC"
)
# install.packages("randomForest")
library(randomForest)
set.seed(123)
# y split con default_idx
X_train_rf <- train_df[default_idx, ]
X_test_rf  <- train_df[-default_idx, ]
# Objetivo con niveles válidos
X_train_rf$Exited <- factor(ifelse(as.character(X_train_rf$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_test_rf <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))
# Asegura que todos los predictores no sean character:
chr_cols <- sapply(X_train_rf, is.character)
X_train_rf[chr_cols] <- lapply(X_train_rf[chr_cols], factor)
X_test_rf[ chr_cols] <- lapply(X_test_rf[ chr_cols], factor)
ctrl_up <- trainControl(
method = "repeatedcv", number = 10, repeats = 10,
verboseIter = FALSE,
classProbs = TRUE,
sampling = "up",                # ← oversampling
summaryFunction = twoClassSummary
)
# Entrenamiento RF (puedes usar "rf" o "ranger")
model_rf_over <- train(
Exited ~ ., data = X_train_rf,
method = "rf",
preProcess = NULL,              # RF no necesita center/scale
trControl = ctrl_up,
metric = "ROC"
)
# Probabilidades sobre el test “en crudo”
prob_rf <- predict(model_rf_over, newdata = X_test_rf, type = "prob")
head(prob_rf)
library(caret)
set.seed(123)
ctrl_fast <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
sampling = "up",
summaryFunction = twoClassSummary
)
fit_rf_fast <- train(
Exited ~ ., data = X_train_rf,
method   = "ranger",           # 更快
trControl= ctrl_fast,
metric   = "ROC",
tuneLength = 5,                # 让 caret 自动给一小撮候选超参
importance = "none",
num.trees = 300                # 比默认更快
)
fit_rf_fast
fit_rf_fast
# === 概率预测 ===
prob_rf <- predict(fit_rf_fast, newdata = X_test_rf, type = "prob")
head(prob_rf)
# === 标签预测（默认阈值0.5） ===
pred_rf <- predict(fit_rf_fast, newdata = X_test_rf)
# === 混淆矩阵 ===
cm_rf <- confusionMatrix(pred_rf, X_test_rf$Exited, positive = "Yes")
prob_rf <- predict(fit_rf_fast, newdata = X_test_rf, type = "prob")
head(prob_rf)
pred_rf <- predict(fit_rf_fast, newdata = X_test_rf)
cm_rf <- confusionMatrix(pred_rf, X_test_rf$Exited, positive = "Yes")
pred_rf <- factor(pred_rf, levels = c("Yes", "No"))
pred_rf <- factor(pred_rf, levels = c("Yes", "No"))
X_test_rf$Exited <- factor(X_test_rf$Exited, levels = c("Yes", "No"))
cm_rf <- confusionMatrix(pred_rf, X_test_rf$Exited, positive = "Yes")
cm_rf
head(prob_rf)
prob_rf <- predict(fit_rf_fast, newdata = X_test_rf, type = "prob")
head(prob_rf)
pred_rf <- factor(pred_rf，levels = c("Yes","No"))
pred_rf <- factor(pred_rf, levels = c("Yes","No"))
X_test_rf$Exited <- factor(X_test_rf$Exited, levels = c("Yes","No"))
cm_rf <- confusionMatrix(pred_rf, X_test_rf$Exited, positive = "Yes")
cm_rf
levels(pred_rf); levels(X_test_rf$Exited)
table(pred_rf, useNA = "ifany")
table(X_test_rf$Exited, useNA = "ifany")
prob_rf <- predict(fit_rf_fast, newdata = X_test_rf, type = "prob")
head(prob_rf)
pred_rf <- factor(pred_rf, levels = c("Yes","No"))
X_test_rf$Exited <- factor(X_test_rf$Exited, levels = c("Yes","No"))
cm_rf <- confusionMatrix(pred_rf, X_test_rf$Exited, positive = "Yes")
cm_rf
# install.packages("randomForest")
library(randomForest)
set.seed(123)
# y split con default_idx
X_train_rf <- train_df[default_idx, ]
X_test_rf  <- train_df[-default_idx, ]
# Objetivo con niveles válidos
X_train_rf$Exited <- factor(ifelse(as.character(X_train_rf$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_test_rf <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))
# Asegura que todos los predictores no sean character:
chr_cols <- sapply(X_train_rf, is.character)
X_train_rf[chr_cols] <- lapply(X_train_rf[chr_cols], factor)
X_test_rf[ chr_cols] <- lapply(X_test_rf[ chr_cols], factor)
ctrl_up <- trainControl(
method = "repeatedcv", number = 10, repeats = 10,
verboseIter = FALSE,
classProbs = TRUE,
sampling = "up",                # ← oversampling
summaryFunction = twoClassSummary
)
# # Entrenamiento RF
# model_rf_over <- train(
#   Exited ~ ., data = X_train_rf,
#   method = "rf",
#   preProcess = NULL, # RF no necesita center ni scale
#   trControl = ctrl_up,
#   metric = "ROC"
# )
#
# # Probabilidades sobre el test (en crudo)
# prob_rf <- predict(model_rf_over, newdata = X_test_rf, type = "prob")
# head(prob_rf)
#
# # Etiquetas por defecto (umbral 0.5)
# pred_rf <- predict(model_rf_over, newdata = X_test_rf)
#
# # Matriz de confusión
# cm_over <- confusionMatrix(pred_rf, y_test_rf, positive = "Yes")
# cm_over
#
# # umbral 0.30
# tau <- 0.30
# pred_rf_tau <- factor(ifelse(prob_rf$Yes >= tau, "Yes", "No"), levels = levels(y_test_rf))
# confusionMatrix(pred_rf_tau, y_test_rf, positive = "Yes")
library(caret)
# Crear preprocesador para centrado y escalado
preproc <- preProcess(X_trainC, method = c("center", "scale"))
# Aplicar transformación
X_trainC <- predict(preproc, X_trainC)
X_testC <- predict(preproc, X_testC)
y_trainC <- factor(ifelse(as.character(X_trainC$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_testC  <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = levels(y_trainC))
X_train_feat <- X_trainC[, setdiff(names(X_trainC), "Exited")]
X_test_feat  <- X_testC
# Convertir variables categóricas en dummies
dmy <- dummyVars(~ ., data = X_train_feat, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_train_feat))
test_num  <- data.frame(predict(dmy, newdata = X_test_feat))
# Escalar las variables numéricas
preproc <- preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
# kNN CV
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
# install.packages("randomForest")
library(randomForest)
set.seed(123)
# y split con default_idx
X_train_rf <- train_df[default_idx, ]
X_test_rf  <- train_df[-default_idx, ]
# Objetivo con niveles válidos
X_train_rf$Exited <- factor(ifelse(as.character(X_train_rf$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_test_rf <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))
# Asegura que todos los predictores no sean character:
chr_cols <- sapply(X_train_rf, is.character)
X_train_rf[chr_cols] <- lapply(X_train_rf[chr_cols], factor)
X_test_rf[ chr_cols] <- lapply(X_test_rf[ chr_cols], factor)
ctrl_up <- trainControl(
method = "repeatedcv", number = 10, repeats = 10,
verboseIter = FALSE,
classProbs = TRUE,
sampling = "up",                # ← oversampling
summaryFunction = twoClassSummary
)
# # Entrenamiento RF
# model_rf_over <- train(
#   Exited ~ ., data = X_train_rf,
#   method = "rf",
#   preProcess = NULL, # RF no necesita center ni scale
#   trControl = ctrl_up,
#   metric = "ROC"
# )
#
# # Probabilidades sobre el test (en crudo)
# prob_rf <- predict(model_rf_over, newdata = X_test_rf, type = "prob")
# head(prob_rf)
#
# # Etiquetas por defecto (umbral 0.5)
# pred_rf <- predict(model_rf_over, newdata = X_test_rf)
#
# # Matriz de confusión
# cm_over <- confusionMatrix(pred_rf, y_test_rf, positive = "Yes")
# cm_over
#
# # umbral 0.30
# tau <- 0.30
# pred_rf_tau <- factor(ifelse(prob_rf$Yes >= tau, "Yes", "No"), levels = levels(y_test_rf))
# confusionMatrix(pred_rf_tau, y_test_rf, positive = "Yes")
levels(pred_rf); levels(X_test_rf$Exited)
table(pred_rf, useNA = "ifany")
table(X_test_rf$Exited, useNA = "ifany")
prob_rf <- predict(fit_rf_fast, newdata = X_test_rf, type = "prob")
head(prob_rf)
pred_rf <- factor(pred_rf, levels = c("Yes","No"))
X_test_rf$Exited <- factor(X_test_rf$Exited, levels = c("Yes","No"))
cm_rf <- confusionMatrix(pred_rf, X_test_rf$Exited, positive = "Yes")
cm_rf
library(pROC)
roc_rf <- roc(response = X_test_rf$Exited, predictor = prob_rf$Yes, levels = c("No","Yes"))
levels(pred_rf); levels(X_test_rf$Exited)
table(pred_rf, useNA = "ifany")
table(X_test_rf$Exited, useNA = "ifany")
# install.packages("randomForest")
library(randomForest)
set.seed(123)
# y split con default_idx
X_train_rf <- train_df[default_idx, ]
X_test_rf  <- train_df[-default_idx, ]
# Objetivo con niveles válidos
X_train_rf$Exited <- factor(ifelse(as.character(X_train_rf$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_test_rf <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))
# Asegura que todos los predictores no sean character:
chr_cols <- sapply(X_train_rf, is.character)
X_train_rf[chr_cols] <- lapply(X_train_rf[chr_cols], factor)
X_test_rf[ chr_cols] <- lapply(X_test_rf[ chr_cols], factor)
ctrl_up <- trainControl(
method = "repeatedcv", number = 10, repeats = 10,
verboseIter = FALSE,
classProbs = TRUE,
sampling = "up",                # ← oversampling
summaryFunction = twoClassSummary
)
# # Entrenamiento RF
# model_rf_over <- train(
#   Exited ~ ., data = X_train_rf,
#   method = "rf",
#   preProcess = NULL, # RF no necesita center ni scale
#   trControl = ctrl_up,
#   metric = "ROC"
# )
#
# # Probabilidades sobre el test (en crudo)
# prob_rf <- predict(model_rf_over, newdata = X_test_rf, type = "prob")
# head(prob_rf)
#
# # Etiquetas por defecto (umbral 0.5)
# pred_rf <- predict(model_rf_over, newdata = X_test_rf)
#
# # Matriz de confusión
# cm_over <- confusionMatrix(pred_rf, y_test_rf, positive = "Yes")
# cm_over
#
# # umbral 0.30
# tau <- 0.30
# pred_rf_tau <- factor(ifelse(prob_rf$Yes >= tau, "Yes", "No"), levels = levels(y_test_rf))
# confusionMatrix(pred_rf_tau, y_test_rf, positive = "Yes")
prob_rf <- predict(fit_rf_fast, newdata = X_test_rf, type = "prob")
pred_rf <- predict(fit_rf_fast, newdata = X_test_rf)
## 4) 统一 levels（稳妥做法）
pred_rf          <- factor(pred_rf,          levels = c("Yes","No"))
X_test_rf$Exited <- factor(X_test_rf$Exited, levels = c("Yes","No"))
cm_rf <- confusionMatrix(pred_rf, X_test_rf$Exited, positive = "Yes")
cm_rf
library(pROC)
roc_rf <- roc(response = X_test_rf$Exited, predictor = prob_rf$Yes, levels = c("No","Yes"))
pred_rf
# install.packages("randomForest")
library(randomForest)
set.seed(123)
# y split con default_idx
X_train_rf <- train_df[default_idx, ]
X_test_rf  <- train_df[-default_idx, ]
# Objetivo con niveles válidos
X_train_rf$Exited <- factor(ifelse(as.character(X_train_rf$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_test_rf <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))
# Asegura que todos los predictores no sean character:
chr_cols <- sapply(X_train_rf, is.character)
X_train_rf[chr_cols] <- lapply(X_train_rf[chr_cols], factor)
X_test_rf[ chr_cols] <- lapply(X_test_rf[ chr_cols], factor)
ctrl_up <- trainControl(
method = "repeatedcv", number = 10, repeats = 10,
verboseIter = FALSE,
classProbs = TRUE,
sampling = "up",                # ← oversampling
summaryFunction = twoClassSummary
)
# # Entrenamiento RF
# model_rf_over <- train(
#   Exited ~ ., data = X_train_rf,
#   method = "rf",
#   preProcess = NULL, # RF no necesita center ni scale
#   trControl = ctrl_up,
#   metric = "ROC"
# )
#
# # Probabilidades sobre el test (en crudo)
# prob_rf <- predict(model_rf_over, newdata = X_test_rf, type = "prob")
# head(prob_rf)
#
# # Etiquetas por defecto (umbral 0.5)
# pred_rf <- predict(model_rf_over, newdata = X_test_rf)
#
# # Matriz de confusión
# cm_over <- confusionMatrix(pred_rf, y_test_rf, positive = "Yes")
# cm_over
#
# # umbral 0.30
# tau <- 0.30
# pred_rf_tau <- factor(ifelse(prob_rf$Yes >= tau, "Yes", "No"), levels = levels(y_test_rf))
# confusionMatrix(pred_rf_tau, y_test_rf, positive = "Yes")
X_test_rf$Exited
pred_rf
predict(fit_rf_fast, newdata = X_test_rf)
pred_rf
X_test_rf$Exited
str(pred_Rrf|X_test_rf$Exited)
str(pred_rf|X_test_rf$Exited)
str(pred_rf)
str(X_test_rf$Exited)
prob_rf <- predict(fit_rf_fast, newdata = X_test_rf, type = "prob")
pred_rf <- predict(fit_rf_fast, newdata = X_test_rf)
## 4) 统一 levels（稳妥做法）
pred_rf          <- factor(pred_rf,          levels = c("Yes","No"))
X_test_rf$Exited <- factor(
ifelse(as.character(X_test_rf$Exited) == "1", "Yes", "No"),
levels = c("Yes","No")
)
cm_rf <- confusionMatrix(pred_rf, X_test_rf$Exited, positive = "Yes")
cm_rf
library(pROC)
roc_rf <- roc(response = X_test_rf$Exited, predictor = prob_rf$Yes, levels = c("No","Yes"))
plot(roc_rf, col="blue", main="ROC - Random Forest con Oversampling")
auc(roc_rf)
levels(pred_rf)
levels(X_test_rf$Exited)
table(pred_rf, useNA="ifany")
table(X_test_rf$Exited, useNA="ifany")
ctrl_smote <- ctrl_fast; ctrl_smote$sampling <- "smote"
fit_rf_smote <- train(Exited~., data=X_train_rf, method="ranger",
trControl=ctrl_smote, metric="ROC",
tuneGrid=grid, num.trees=300)
# install.packages("themis")
library(themis)
ctrl_smote <- ctrl_fast; ctrl_smote$sampling <- "smote"
fit_rf_smote <- train(Exited~., data=X_train_rf, method="ranger",
trControl=ctrl_smote, metric="ROC",
tuneGrid=grid, num.trees=300)
## Cargamos los paquetes necesarios
#install.packages("VIM")
library(VIM)
# Cargamos la base de datos necesarios
dd <- data_transformada
summary(dd)
dd[3310,1]<-"positiu"
test <- sample(1:nrow(dd), size = nrow(dd)/3)
dataTrain <- dd[-test,]
dataTest <- dd[test,]
# Creamos unos data frame auxiliares
aux <- dd
aux[test, 1] <- NA
summary(aux)
result <- kNN(aux,k=3,variable="Dictamen") # utilizando Gower, distancia mixta
## Cargamos los paquetes necesarios
#install.packages("VIM")
library(VIM)
# Cargamos la base de datos necesarios
dd <- data_transformada
summary(dd)
dd[3310,1]<-"positiu"
test <- sample(1:nrow(dd), size = nrow(dd)/3)
dataTrain <- dd[-test,]
dataTest <- dd[test,]
# Creamos unos data frame auxiliares
aux <- dd
aux[test, 1] <- NA
summary(aux)
result <- kNN(aux,k=5,variable="Dictamen") # utilizando Gower, distancia mixta
## Cargamos los paquetes necesarios
#install.packages("VIM")
library(VIM)
# Cargamos la base de datos necesarios
dd <- data_transformada
summary(dd)
dd[3310,1]<-"positiu"
test <- sample(1:nrow(dd), size = nrow(dd)/3)
dataTrain <- dd[-test,]
dataTest <- dd[test,]
# Creamos unos data frame auxiliares
aux <- dd
aux[test, 1] <- NA
summary(aux)
result <- kNN(aux,k=7,variable="Dictamen") # utilizando Gower, distancia mixta
table(result$Dictamen[test], dd$Dictamen[test])
table(result[test,1], dd[test,1])
save.image("kNNEnvironment.RData")

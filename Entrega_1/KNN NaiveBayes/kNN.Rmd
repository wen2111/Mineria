---
title: "kNN"
author: "Siling Guo"
date: "2025-10-14"
output:
  html_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
# load("../Mineria/DATA/dataaaaaaaaaaaaaa.RData")
```

## KNN-Clasificador 
```{r}
data <- data_transformada
summary(data)
```

```{r}
plot(data$Exited)
(table(as.character(data$Exited))/sum(table(as.character(data$Exited))))*100
```
En la base de datos data_transformada, hay dos parte de datos, test original y train original. Además, es importante tener en cuenta la base de datos es desbalanceada, en las observaciones donde no sean NA, hay 79,28% No abandona el banco, y 20,71% sí abandona el banco. A la hora de hacer kNN test, se conviene aplicar los métodos Undersampling, Oversampling, ROSE y SMOTE.

## División del TRAIN & TEST
```{r}
set.seed(123)
train_df <- data[data$group == "train" & !is.na(data$Exited), ]
train_df<- subset(train_df, select = -c(ID, Surname, group))
train_df$Exited <- factor(train_df$Exited, levels = c("1","0"))


ind_col <- which(names(train_df) == "Exited") # columna de indicador
default_idx <- createDataPartition(train_df$Exited, p = 0.7, list = FALSE) # 70% para entrenamiento

X_trainC <- train_df[default_idx, ]
X_testC  <- train_df[-default_idx, ]

y_testC <- X_testC[, ind_col]
X_testC <- X_testC[, -ind_col]

# muestra <- sample(1:nrow(train_df), size = nrow(train_df)/3)
# train<-data.frame(train_df[-muestra,])
# test<-data.frame(train_df[muestra,])
# cl<-data$Exited[-muestra]  # variable output que queremos pronosticar.
```

# No balanceado
```{r}
library(caret)

# Crear preprocesador para centrado y escalado
preproc <- preProcess(X_trainC, method = c("center", "scale"))

# Aplicar transformación
X_trainC <- predict(preproc, X_trainC)
X_testC <- predict(preproc, X_testC)
```

```{r}
y_trainC <- factor(ifelse(as.character(X_trainC$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_testC  <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = levels(y_trainC))

X_train_feat <- X_trainC[, setdiff(names(X_trainC), "Exited")]
X_test_feat  <- X_testC

# Convertir variables categóricas en dummies
dmy <- dummyVars(~ ., data = X_train_feat, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_train_feat))
test_num  <- data.frame(predict(dmy, newdata = X_test_feat))

# Escalar las variables numéricas
preproc <- preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
```

# kNN CV
```{r}
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
  x = trainScaled,
  y = y_trainC,
  method = "knn",
  trControl = ctrl,
  tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
```
Se observa que al aumentar k de 1 a 20, el Accuracy augmenta de 0,71 a 0,80, mientras que la velocidad del crecimiento ya se ha moderado bastante a partir de k=17.
El valor de kappa ha estado bajo con todas las k, entre 0,12 y 0,14. Se afirma que la capacidad de classificar los dos tipos (yes exited y no exited) es muy bajo, teniedo en cuenta que se esta trabajando con una base de datos desbalanceada.

```{r}
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
```

```{r}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
head(entrenamiento$results, 16)
```


```{r}
plot(entrenamiento)
```

## KnN con k= 20 (automático)
```{r}
pred_label <- predict(entrenamiento, newdata = testScaled)
cm <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm
```
El modelo kNN obtuve la exactitud global (Accuracy) del 80,71%, 
lo que indica que acierta aproximadamente 80,81% del conjunto de prueba. Pero es trivial tener en cuenta el desequilibrio entre "Yes" (clientes abandonan) y "No" clientes permanencen.
- Se puede ver que la Sensibilidad = 0,1448, muestra que el modelo solo ha podido identificar correctamente un 14,48% de los clientes que abandonan. La gran mayoría de los casos no han sido detectados.
- Debido el sesgo havia la clase mayoritaria "No", la Especificidad es muy alta,  un  98,02%.
- La precisión positiva (Pos Pred Value) es 0,6563 indica que de todos los clientes que el modelo predice como "Yes"alrededor del 65,63% realmente abandonaron.
El índice Kappa es 0,1755 confirma que el acuerdo del modelo con las etiquetas reales es solo ligeramente meor que el azar. Es muy bajo para ser un modelo robusto.
- El modelo tiene exactitud balanceada 0,56, refuerza la idea de que el modelo no consigue equilibrar bien la detección de ambas clases.



# kNN con k=15
```{r}
fit_knn_15 <- train(
  x = trainScaled,
  y = y_trainC,
  method = "knn",
  trControl = trainControl(method="cv", number=5, classProbs=TRUE),
  tuneGrid = data.frame(k = 15)
)

fit_knn_15
confusionMatrix(predict(fit_knn_15, newdata=testScaled), y_testC)

```


# kNN Oversampling 
Considerando que no es una base de datos extremadamente grande, para mejorar este rendimeinto, sería bueno aplicar técnicas como Undersampling, Oversampling, SMOTE, ROSE.
## Simple
```{r}
set.seed(123)
ctrl_up <- trainControl(
  method = "cv", number = 5,
  classProbs = TRUE,
  sampling = "up", # oversampling dentro de cada fold
  summaryFunction = twoClassSummary
)

fit_knn_up <- train(
  x = trainScaled, y = y_trainC,
  method = "knn",
  metric = "ROC",  # optimiza por AUC, mejor para desbalanceo
  trControl = ctrl_up,
  tuneGrid = expand.grid(k = seq(1, 21, by = 2))
)

# Predicción
pred_prob_up <- predict(fit_knn_up, newdata = testScaled, type = "prob")
pred_lab_up  <- predict(fit_knn_up, newdata = testScaled)

# Evaluación (umbral 0.5)
cm_up <- confusionMatrix(pred_lab_up, y_testC, positive = "Yes")
cm_up
```
El modelo kNN con umbral 0,5 obtiene la exactitud global (Accuracy) del 60,76%, ha bajado bastante en comparación con el modelo sin oversampling.

- Se puede ver que la Sensibilidad = 0,6138, muestra que el modelo solo ha podido identificar correctamente un 61,38% de los clientes que abandonan. La gran mayoría de los casos no han sido detectados.

- Debido el sesgo havia la clase mayoritaria "No", la Especificidad es muy alta, un 60,6%.
- La precisión positiva (Pos Pred Value) es 0,2893 es muy bajo en comparación con el modelo sin oversampling.

El índice Kappa tamiñen ha bajado a ser 0,1554.

- El modelo tiene exactitud balanceada 0,6099, es un poco mejor.

```{r}
# umbral 0.30 (para subir Sensitivity)

tau <- 0.30
pred_tau <- factor(ifelse(pred_prob_up$Yes >= tau, "Yes", "No"), levels = levels(y_testC))
cm_tau <- confusionMatrix(pred_tau, y_testC, positive = "Yes")
cm_tau
```
El modelo kNN con umbral 0,3 obtiene la exactitud global (Accuracy) del solo 41,71%.

- La Sensibilidad = 0,1471, muestra que el modelo solo ha podido identificar correctamente un 84,6%.

- La precisión positiva (Pos Pred Value) es 24,13%, ha bajado mucho.

- El índice Kappa es 0,0785 confirma que el acuerdo del modelo con las etiquetas reales es solo ligeramente meor que el azar.Es aún más bajo que el modelo con umbral 0,5.
- La exactitud balanceada 0,5755, refuerza la idea de que el modelo no consigue equilibrar bien la detección de ambas clases.


## Con Random Forest (leeeeeeeeeeeeeeeeeeeeeeeeeeentiiiiiiisimo)
```{r}
# install.packages("randomForest")
# library(randomForest)
# 
# set.seed(123)
# # y split con default_idx
# X_train_rg <- train_df[default_idx, ]
# X_test_rg  <- train_df[-default_idx, ]
# 
# # Objetivo con niveles válidos
# X_train_rg$Exited <- factor(ifelse(as.character(X_train_rg$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
# y_test_rg <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))
# 
# # Asegura que todos los predictores no sean character:
# chr_cols <- sapply(X_train_rg, is.character)
# X_train_rg[chr_cols] <- lapply(X_train_rg[chr_cols], factor)
# X_test_rg[ chr_cols] <- lapply(X_test_rg[ chr_cols], factor)
# 
# ctrl_up <- trainControl(
#   method = "repeatedcv", number = 10, repeats = 10,
#   verboseIter = FALSE,
#   classProbs = TRUE,
#   sampling = "up",                # ← oversampling
#   summaryFunction = twoClassSummary
# )

# # Entrenamiento RF tooooooooooo slowwwwwwww
# model_rf_over <- train(
#   Exited ~ ., data = X_train_rg,
#   method = "rf",
#   preProcess = NULL, # RF no necesita center ni scale
#   trControl = ctrl_up,
#   metric = "ROC"
# )
# 
# # Probabilidades sobre el test (en crudo)
# prob_rg <- predict(model_rf_over, newdata = X_test_rg, type = "prob")
# head(prob_rg)
# 
# # Etiquetas por defecto (umbral 0.5)
# pred_rg <- predict(model_rf_over, newdata = X_test_rg)
# 
# # Matriz de confusión
# cm_over <- confusionMatrix(pred_rg, y_test_rg, positive = "Yes")
# cm_over
# 
# # umbral 0.30
# tau <- 0.30
# pred_rg_tau <- factor(ifelse(prob_rg$Yes >= tau, "Yes", "No"), levels = levels(y_test_rg))
# confusionMatrix(pred_rg_tau, y_test_rg, positive = "Yes")

```

## Ranger
kNN Oversampling con randomforest es muy lento, por lo tanto se utiliza otra manera: Ranger, que es una implementación más rápida de randomForest.
```{r}
# install.packages("randomForest")
library(randomForest)
X_train_rg <- train_df[default_idx, ]
X_test_rg  <- train_df[-default_idx, ]

X_train_rg$Exited <- factor(ifelse(as.character(X_train_rg$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_test_rg <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))

# Asegura que todos los predictores no sean character
chr_cols <- sapply(X_train_rg, is.character)
X_train_rg[chr_cols] <- lapply(X_train_rg[chr_cols], factor)
X_test_rg[chr_cols] <- lapply(X_test_rg[chr_cols], factor)
```


```{r}
set.seed(123)
ctrl_fast <- trainControl(
  method = "cv", number = 5,
  classProbs = TRUE,
  sampling = "up",
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

fit_rg_fast <- train(
  Exited ~ ., data = X_train_rg,
  method   = "ranger",
  trControl= ctrl_fast,
  metric   = "ROC",
  tuneLength = 5, # que el caret busque 5 valores de mtry
  importance = "none", 
  num.trees = 300 # menos árboles para mayor velocidad
)
fit_rg_fast
```

```{r}
prob_rg <- predict(fit_rg_fast, newdata = X_test_rg, type = "prob")
pred_rg <- predict(fit_rg_fast, newdata = X_test_rg)

pred_rg<- factor(pred_rg, levels = c("Yes","No"))
X_test_rg$Exited <- factor(ifelse(as.character(X_test_rg$Exited) == "1", "Yes", "No"),levels = c("Yes","No"))

cm_rg <- confusionMatrix(pred_rg, X_test_rg$Exited, positive = "Yes")
cm_rg
```
- La exactitud global (Accuracy) del 80,57%, lo que indica que acierta aproximadamente 80,57% del conjunto de prueba. Ha bajado en comparación en el modelo sin oversampling.
- La Sensibilidad = 0.31264, (antes era 0,1471). Muestra que el modelo ha podido identificar correctamente un 31,26% de los clientes que abandonan. La gran mayoría de los casos no han sido detectados, pero ha mejorado bastante en comparación con el modelo sin oversampling.
- Debido el sesgo había la clase mamyoritaria "No", la Especificidad es muy alta, un  98,07%.
- La precisión positiva (Pos Pred Value) es 0,6667 indica que de todos los clientes que el modelo predice como "Yes"alrededor del 66,67% realmente abandonaron.
- El índice Kappa es 0,1796 confirma que el acuerdo del modelo con las etiquetas reales es solo ligeramente meor que el azar. Es muy bajo para ser un modelo robusto.
- El témino Exactitud balanceada 0,56, refuerza la idea de que el modelo no consigue equilibrar bien la detección de ambas clases.

Este modelo detecta aproximadamente un tercio de los clientes que efectivamente abandonan, manteniendo una baja tasa de falsos positivos. La técnica de oversampling con opción ranger permite mejorar la detección de la clase minoritaria (en comparación con el modelo KNN anterior).

```{r}
library(pROC)
roc_rf <- roc(response = X_test_rg$Exited, predictor = prob_rg$Yes, levels = c("No","Yes"))
plot(roc_rf, col="blue", main="ROC - Ranger con Oversampling")
auc(roc_rf)
```
Con un AUC de 0,7547, lo que indica una capacidad discriminativa moderadamente alta para distinguir entre clientes que abandonan y los que permanecen.

```{r}
levels(pred_rg)
levels(X_test_rg$Exited)
table(pred_rg, useNA="ifany")
table(X_test_rg$Exited, useNA="ifany")
```

# ROSE
```{r}
X_train_rs <- train_df[default_idx, ]
X_train_rs  <- train_df[-default_idx, ]

X_train_rs$Exited <- factor(ifelse(as.character(X_train_rs$Exited) %in% c("1","Yes"), "Yes", "No"),levels = c("Yes","No"))
X_train_rs$Exited  <- factor(ifelse(as.character(X_train_rs$Exited)  %in% c("1","Yes"), "Yes", "No"),levels = c("Yes","No"))

# convertir niveles de factores para evitar espacios en blanco
clean_factor_levels <- function(df){
  fac <- names(df)[sapply(df, is.factor)]
  for (col in fac) levels(df[[col]]) <- make.names(levels(df[[col]]))
  df
}
X_train_rs <- clean_factor_levels(X_train_rs)
X_train_rs  <- clean_factor_levels(X_train_rs)

# ROSE
library(ROSE)
set.seed(123)
ctrl_rose <- trainControl(method="cv", number=5, classProbs=TRUE,
                          summaryFunction=twoClassSummary, sampling="rose")
fit_rs_rose <- train(Exited ~ ., data=X_train_rs, method="ranger", trControl=ctrl_rose, metric="ROC", tuneLength=5, num.trees=300)

prob_rose <- predict(fit_rs_rose, newdata=X_train_rs, type="prob")[,"Yes"]
pred_rose <- predict(fit_rs_rose, newdata=X_train_rs)

cm_rose <- confusionMatrix(pred_rose, X_train_rs$Exited, positive="Yes")
cm_rose
```
- La exactitud global (Accuracy) indica que acierta aproximadamente 82,05% del conjunto de prueba. Es similar al modelo con oversampling con ranger
- La Sensibilidad = 0,34943, (antes era 0.31264). Muestra que el modelo ha podido identificar correctamente un 34,943% de los clientes que abandonan. La gran mayoría de los casos no han sido detectados, pero ha mejorado bastante en comparación con el modelo oversampling con ranger.
- Debido el sesgo había la clase mamyoritaria "No", la Especificidad sigue siendo muy alta, un 94,35%.
- La precisión positiva (Pos Pred Value) es 0,6178 indica que de todos los clientes que el modelo predice como "Yes"alrededor del 61,78% realmente abandonaron. Ha bajado un poco en comparación con el modelo oversampling con ranger. Se interesa más en detectar la clase minoritaria.
- El índice Kappa es 0,349, es un doble del mismo índice del modelo anterior. Ha mejorado bastante.
- El témino Exactitud balanceada 0,64648,  el modelo consigue equilibrar mejor la detección de ambas clases.


```{r}
tau <- 0.30
pred_rose_tau <- factor(ifelse(prob_rose >= tau, "Yes", "No"),
                        levels = c("Yes","No"))
confusionMatrix(pred_rose_tau, X_train_rs$Exited, positive = "Yes")

```


# SMOTE
```{r}
set.seed(123)

ctrl_smote <- trainControl(
  method = "cv", number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  sampling = "smote"
)

fit_rf_smote <- train(
  Exited ~ ., data = X_train_rg,
  method = "ranger",
  trControl = ctrl_smote,
  metric = "ROC",
  num.trees = 300,
  tuneLength = 5
)

prob_smote <- predict(fit_rf_smote, newdata = X_test_rg, type = "prob")[,"Yes"]
pred_smote <- predict(fit_rf_smote, newdata = X_test_rg)

cm_smote <- confusionMatrix(pred_smote, X_test_rg$Exited, positive = "Yes")
cm_smote
```

```{r}
save.image("kNNEnvironment.RData")
```

# KNN REGRESSION
```{r}
library(MASS)
data(Boston)
str(Boston)
help(Boston)
##Partición de Data usando library(caret)

set.seed(1)
inTraining <- createDataPartition(Boston$medv, p = .80, list = FALSE)
training <- Boston[inTraining,]
testing  <- Boston[-inTraining,]
set.seed(1)
model3 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale")
)
model3
###
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]

predictions = predict(model3, newdata = test.features)
# RMSE
sqrt(mean((test.target - predictions)^2))
# R2
cor(test.target, predictions) ^ 2
### Si agregamos cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 10)

model4 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale"),
  trControl = ctrl
)
model4
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]
predictions = predict(model4, newdata = test.features)
```

# RMSE
```{r}
sqrt(mean((test.target - predictions)^2))

##Tunning the model (GridSearch for k)
set.seed(1)

tuneGrid <- expand.grid(
  k = seq(5, 9, by = 1)
)

model5 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale"),
  trControl = ctrl,
  tuneGrid = tuneGrid
)
model5
plot(model5)
```

# KNN con Predictores MIXTOS
```{r}
## Cargamos los paquetes necesarios
#install.packages("VIM")
library(VIM)


# Cargamos la base de datos necesarios

dd <- data_transformada
summary(dd)
dd[3310,1]<-"positiu"
test <- sample(1:nrow(dd), size = nrow(dd)/3)
dataTrain <- dd[-test,]
dataTest <- dd[test,]


# Creamos unos data frame auxiliares
aux <- dd
aux[test, 1] <- NA
summary(aux)

result <- kNN(aux,k=3,variable="Dictamen") # utilizando Gower, distancia mixta
table(result$Dictamen[test], dd$Dictamen[test])
table(result[test,1], dd[test,1])


result<-kNN(aux, variable = "Dictamen", weightDist = T)
table(result$Dictamen[test], dd$Dictamen[test])


#regression
aux<-dd
aux[test,2]<-NA
result<-kNN(aux, k=1,variable = "Antiguedad.Trabajo", weightDist = T)
table(result[test,2], dd[test,2])
plot(result[test,2], dd[test,2])
plot(result[test,2]- dd[test,2])

# usa Gower para regresión, puede ser la primera aproximación para nuestro modelo,pero no es el óptimo.
y<-dd[test,2]
yp<-result[test,2]
e1<-(y-yp)^2
e2<-abs(y-yp)
summary(e1)
summary(e2)
hist(e2)
plot(density(e2))
densityplot(e2)
mse<-sum((y-yp)^2)/(length(y))
rmse<-sqrt(mse)
```

# transformación de factores mixto (FAMD)


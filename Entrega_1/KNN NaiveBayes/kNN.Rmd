---
title: "kNN"
author: "Siling Guo"
date: "2025-10-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
```

```{r}
# load("../Mineria/DATA/dataaaaaaaaaaaaaa.RData")
str(data_transformada)
```

## KNN-Clasificador 
```{r}
data <- data_transformada
summary(data)
```

```{r}
plot(data$Exited)
(table(as.character(data$Exited))/sum(table(as.character(data$Exited))))*100
```
En la base de datos data_transformada, hay dos parte de datos, test original y train original. Además, es importante tener en cuenta la base de datos es desbalanceada, en las observaciones donde no sean NA, hay 79,28% No abandona el banco, y 20,71% sí abandona el banco. A la hora de hacer kNN test, se conviene aplicar los métodos Undersampling, Oversampling, ROSE y SMOTE.

## División del TRAIN & TEST
```{r}
set.seed(123)
train_df <- data[data$group == "train" & !is.na(data$Exited), ]
train_df<- subset(train_df, select = -c(ID, Surname, group))
train_df$Exited <- factor(train_df$Exited, levels = c("1","0"))


ind_col <- which(names(train_df) == "Exited") # columna de indicador
default_idx <- createDataPartition(train_df$Exited, p = 0.7, list = FALSE) # 70% para entrenamiento

X_trainC <- train_df[default_idx, ]
X_testC  <- train_df[-default_idx, ]

y_testC <- X_testC[, ind_col]
X_testC <- X_testC[, -ind_col]

# muestra <- sample(1:nrow(train_df), size = nrow(train_df)/3)
# train<-data.frame(train_df[-muestra,])
# test<-data.frame(train_df[muestra,])
# cl<-data$Exited[-muestra]  # variable output que queremos pronosticar.
```

```{r}
library(caret)

# Crear preprocesador para centrado y escalado
preproc <- preProcess(X_trainC, method = c("center", "scale"))

# Aplicar transformación
X_trainC <- predict(preproc, X_trainC)
X_testC <- predict(preproc, X_testC)
```

```{r}
y_trainC <- factor(ifelse(as.character(X_trainC$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_testC  <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = levels(y_trainC))

X_train_feat <- X_trainC[, setdiff(names(X_trainC), "Exited")]
X_test_feat  <- X_testC

# Convertir variables categóricas en dummies
dmy <- dummyVars(~ ., data = X_train_feat, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_train_feat))
test_num  <- data.frame(predict(dmy, newdata = X_test_feat))

# Escalar las variables numéricas
preproc <- preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)

# kNN CV
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
  x = trainScaled,
  y = y_trainC,
  method = "knn",
  trControl = ctrl,
  tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
```
Se observa que al aumentar k de 1 a 20, el Accuracy augmenta de 0,71 a 0,80, mientras que la veolocidad del crecimiento ya se ha moderado bastante a partir de k=17.
El valor de kappa ha estado bajo con todas las k, entre 0,12 y 0,16. Se afirma que la capacidad de classificar los dos tipos (yes exited y no exited) es muy bajo, teniedo en cuenta que se esta trabajando con una base de datos desbalanceada.

```{r}
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
```

```{r}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
head(entrenamiento$results, 5)
```


```{r}
plot(entrenamiento)
```

```{r}
pred_label <- predict(entrenamiento, newdata = testScaled)
cm <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm
```
El modelo kNN obtuve la exactitud global (Accuracy) del 80,81%, 
lo que indica que acierta aproximadamente 80,81% del conjunto de prueba. Pero es trivial tener en cuenta el desequilibrio entre "Yes" (clientes abandonan) y "No" clientes permanencen.
- Se puede ver que la Sensibilidad = 0,1471, muestra que el modelo solo ha podido identificar correctamente un 14,71% de los clientes que abandonan. La gran mayoría de los casos no han sido detectados.
- Debido el sesgo havia la clase moyoritaria "No", la Especificidad es muy alta,  un  98,07%.
- La precisión positiva (Pos Pred Value) es 0,6667 indica que de todos los clientes que el modelo predice como "Yes"alrededor del 66,67% realmente abandonaron.
El índice Kappa es 0,1796 confirma que el acuerdo del modelo con las etiquetas reales es solo ligeramente meor que el azar. Es muy bajo para ser un modelo robusto.
El modelo tiene exactitud balanceada 0,56, refuerza la idea de que el modelo no consigue equilibrar bien la detección de ambas clases.

Considerando que no es una base de datos extremadamente grande, para mejorar este rendimeinto, sería bueno aplicar técnicas como Undersampling, Oversampling, SMOTE, ROSE.



# kNN con Oversampling

```{r}
set.seed(123)
ctrl_up <- trainControl(
  method = "cv", number = 5,
  classProbs = TRUE,
  sampling = "up", # oversampling dentro de cada fold
  summaryFunction = twoClassSummary
)

fit_knn_up <- train(
  x = trainScaled, y = y_trainC,
  method = "knn",
  metric = "ROC",  # optimiza por AUC, mejor para desbalanceo
  trControl = ctrl_up,
  tuneGrid = expand.grid(k = seq(1, 21, by = 2))
)

# Predicción
pred_prob_up <- predict(fit_knn_up, newdata = testScaled, type = "prob")
pred_lab_up  <- predict(fit_knn_up, newdata = testScaled)

# Evaluación (umbral 0.5)
cm_up <- confusionMatrix(pred_lab_up, y_testC, positive = "Yes")
cm_up

# umbral0.30 (para subir Sensitivity)
tau <- 0.30
pred_tau <- factor(ifelse(pred_prob_up$Yes >= tau, "Yes", "No"), levels = levels(y_testC))
confusionMatrix(pred_tau, y_testC, positive = "Yes")

```
# kNN con Oversampling usando Random Forest

```{r}
# install.packages("randomForest")
library(randomForest)

set.seed(123)
# y split con default_idx
X_train_rf <- train_df[default_idx, ]
X_test_rf  <- train_df[-default_idx, ]

# Objetivo con niveles válidos
X_train_rf$Exited <- factor(ifelse(as.character(X_train_rf$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_test_rf <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))

# Asegura que todos los predictores no sean character:
chr_cols <- sapply(X_train_rf, is.character)
X_train_rf[chr_cols] <- lapply(X_train_rf[chr_cols], factor)
X_test_rf[ chr_cols] <- lapply(X_test_rf[ chr_cols], factor)

ctrl_up <- trainControl(
  method = "repeatedcv", number = 10, repeats = 10,
  verboseIter = FALSE,
  classProbs = TRUE,
  sampling = "up",                # ← oversampling
  summaryFunction = twoClassSummary
)

# # Entrenamiento RF
# model_rf_over <- train(
#   Exited ~ ., data = X_train_rf,
#   method = "rf",
#   preProcess = NULL, # RF no necesita center ni scale
#   trControl = ctrl_up,
#   metric = "ROC"
# )
# 
# # Probabilidades sobre el test (en crudo)
# prob_rf <- predict(model_rf_over, newdata = X_test_rf, type = "prob")
# head(prob_rf)
# 
# # Etiquetas por defecto (umbral 0.5)
# pred_rf <- predict(model_rf_over, newdata = X_test_rf)
# 
# # Matriz de confusión
# cm_over <- confusionMatrix(pred_rf, y_test_rf, positive = "Yes")
# cm_over
# 
# # umbral 0.30
# tau <- 0.30
# pred_rf_tau <- factor(ifelse(prob_rf$Yes >= tau, "Yes", "No"), levels = levels(y_test_rf))
# confusionMatrix(pred_rf_tau, y_test_rf, positive = "Yes")

```

# kNN con randomdorest más rápido (ranger)
```{r}
ctrl_fast <- trainControl(
  method = "cv", number = 5,
  classProbs = TRUE,
  sampling = "up",
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)

fit_rf_fast <- train(
  Exited ~ ., data = X_train_rf,
  method   = "ranger",
  trControl= ctrl_fast,
  metric   = "ROC",
  tuneLength = 5, # que el caret busque 5 valores de mtry
  importance = "none", 
  num.trees = 300 # menos árboles para mayor velocidad
)
fit_rf_fast
```

```{r}
prob_rf <- predict(fit_rf_fast, newdata = X_test_rf, type = "prob")
pred_rf <- predict(fit_rf_fast, newdata = X_test_rf)

pred_rf<- factor(pred_rf, levels = c("Yes","No"))
X_test_rf$Exited <- factor(ifelse(as.character(X_test_rf$Exited) == "1", "Yes", "No"),
                           levels = c("Yes","No"))

cm_rf <- confusionMatrix(pred_rf, X_test_rf$Exited, positive = "Yes")
cm_rf


library(pROC)
roc_rf <- roc(response = X_test_rf$Exited, predictor = prob_rf$Yes, levels = c("No","Yes"))
plot(roc_rf, col="blue", main="ROC - Random Forest con Oversampling")
auc(roc_rf)
```

```{r}
levels(pred_rf)
levels(X_test_rf$Exited)
table(pred_rf, useNA="ifany")
table(X_test_rf$Exited, useNA="ifany")
```
# ROSE
```{r}
ctrl <- trainControl(method = "repeatedcv", 
                     number = 10, 
                     repeats = 10, 
                     verboseIter = FALSE,
                     sampling = "rose")

set.seed(42)
model_rf_rose <- caret::train(classes ~ .,
                              data = train_data,
                              method = "rf",
                              preProcess = c("scale", "center"),
                              trControl = ctrl)
```

```{r}
final_rose <- data.frame(actual = test_data$classes,
                         predict(model_rf_rose, newdata = test_data, type = "prob"))
final_rose$predict <- ifelse(final_rose$benign > 0.5, "benign", "malignant")
```

```{r}
cm_rose <- confusionMatrix(final_rose$predict, test_data$classes)
```


# SMOTE
```{r}
# install.packages("themis")
library(themis)
ctrl_smote <- ctrl_fast; ctrl_smote$sampling <- "smote"
fit_rf_smote <- train(Exited~., data=X_train_rf, method="ranger",
                      trControl=ctrl_smote, metric="ROC",
                      tuneGrid=grid, num.trees=300)

```

# HASTA AQUÍ




```{r}
set.seed(123)
k_to_try = 1:100
err_k = rep(x = 0, times = length(k_to_try))

for (i in seq_along(k_to_try)) {
  pred <- knn(train = X_trainC, 
             test  = X_testC, 
             cl    = y_trainC, 
             k     = k_to_try[i], 
             prob = T)
  err_k[i] <- calc_class_err(y_testC, pred)
}
```

```{r}
# plot error vs choice of k
plot(err_k, type = "b", col = "dodgerblue", cex = 1, pch = 20, 
     xlab = "k, number of neighbors", ylab = "classification error",
     main = "(Test) Error Rate vs Neighbors")
# add line for min error seen
abline(h = min(err_k), col = "darkorange", lty = 3)
# add line for minority prevalence in test set
abline(h = mean(y_test == "Yes"), col = "grey", lty = 2)
```


```{r}
train_df <- data[is_train, , drop = FALSE]
test_df  <- data[data$group == "test", , drop = FALSE]

# Ordenar y preparar prSummary dentre del caret
train_df$Exited <- as.character(train_df$Exited)
train_df$Exited <- ifelse(train_df$Exited %in% c("1", 1), "Yes",
                          ifelse(train_df$Exited %in% c("0", 0), "No", NA))

train_df$Exited <- factor(train_df$Exited, levels = c("Yes", "No"))
# Descartar columnas no necesarias
drops <- c("ID","group")
# Estandarización de la DATA, solo numéricas
# data_stand <- scale(train_df[, !(names(train_df) %in% drops)])
```


```{r}
# ctrl_base <- trainControl(method="repeatedcv", number=10, repeats=3,
#                           classProbs=TRUE, summaryFunction=twoClassSummary,
#                           savePredictions="final")
# ctrl_none <- ctrl_base
# ctrl_down <- ctrl_base; ctrl_down$sampling <- "down"
# ctrl_up   <- ctrl_base; ctrl_up$sampling   <- "up"
# 
# 
# knn_none <- train(Exited ~ ., data=trainTransformed, method="knn",
#                   trControl=ctrl_none, tuneGrid=data.frame(k=c(3,5,7)),
#                   metric="ROC")
# knn_down <- train(Exited ~ ., data=trainTransformed, method="knn",
#                   trControl=ctrl_down, tuneGrid=data.frame(k=c(3,5,7)),
#                   metric="ROC")
# knn_up   <- train(Exited ~ ., data=trainTransformed, method="knn",
#                   trControl=ctrl_up,   tuneGrid=data.frame(k=c(3,5,7)),
#                   metric="ROC")
# 
# knn_none; knn_down; knn_up
# #control= trainControl(method="repeatedcv",repeats=3) 
# #knnModel <- train(
# #  Exited ~ ., 
# #  data = trainTransformed, 
# #  method = "knn", 
# #  trControl = control , 
# #  tuneLength= 20)
# 
# #############
# ###control <- trainControl(method="repeatedcv",repeats = 3,classProbs=TRUE,summaryFunction = twoClassSummary)
```


```{r}
# best_model<- knn3(
#   Exited ~ .,
#   data = trainTransformed,
#   k = knnModel$bestTune$k
# )
# best_model
# predictions <- predict(best_model, testTransformed,type = "class")
# # Calculate confusion matrix
# cm <- confusionMatrix(predictions, testTransformed$Exited)
# cm
# data.frame(Accuracy = cm$overall["Accuracy"],
#            Sensitivity = cm$byClass["Sensitivity"],
#            Specificity = cm$byClass["Specificity"])
# 
# roc(testTransformed$Exited, as.data.frame(predictions)[,"1"],plot=TRUE)
```


# KNN REGRESSION
```{r}
library(MASS)
data(Boston)
str(Boston)
help(Boston)
##Partición de Data usando library(caret)

set.seed(1)
inTraining <- createDataPartition(Boston$medv, p = .80, list = FALSE)
training <- Boston[inTraining,]
testing  <- Boston[-inTraining,]
set.seed(1)
model3 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale")
)
model3
###
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]

predictions = predict(model3, newdata = test.features)
# RMSE
sqrt(mean((test.target - predictions)^2))
# R2
cor(test.target, predictions) ^ 2
### Si agregamos cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 10)

model4 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale"),
  trControl = ctrl
)
model4
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]
predictions = predict(model4, newdata = test.features)
```

# RMSE
```{r}
sqrt(mean((test.target - predictions)^2))

##Tunning the model (GridSearch for k)
set.seed(1)

tuneGrid <- expand.grid(
  k = seq(5, 9, by = 1)
)

model5 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale"),
  trControl = ctrl,
  tuneGrid = tuneGrid
)
model5
plot(model5)
```

# KNN con Predictores MIXTOS
```{r}
## Cargamos los paquetes necesarios
#install.packages("VIM")
library(VIM)


# Cargamos la base de datos necesarios

dd <- data_transformada
summary(dd)
dd[3310,1]<-"positiu"
test <- sample(1:nrow(dd), size = nrow(dd)/3)
dataTrain <- dd[-test,]
dataTest <- dd[test,]


# Creamos unos data frame auxiliares
aux <- dd
aux[test, 1] <- NA
summary(aux)

result <- kNN(aux,k=3,variable="Dictamen") # utilizando Gower, distancia mixta
table(result$Dictamen[test], dd$Dictamen[test])
table(result[test,1], dd[test,1])


result<-kNN(aux, variable = "Dictamen", weightDist = T)
table(result$Dictamen[test], dd$Dictamen[test])


#regression
aux<-dd
aux[test,2]<-NA
result<-kNN(aux, k=1,variable = "Antiguedad.Trabajo", weightDist = T)
table(result[test,2], dd[test,2])
plot(result[test,2], dd[test,2])
plot(result[test,2]- dd[test,2])

# usa Gower para regresión, puede ser la primera aproximación para nuestro modelo,pero no es el óptimo.
y<-dd[test,2]
yp<-result[test,2]
e1<-(y-yp)^2
e2<-abs(y-yp)
summary(e1)
summary(e2)
hist(e2)
plot(density(e2))
densityplot(e2)
mse<-sum((y-yp)^2)/(length(y))
rmse<-sqrt(mse)
```

# transformación de factores mixto (FAMD)


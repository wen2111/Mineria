---
title: "kNN"
author: "Siling Guo"
date: "2025-10-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
```


```{r}
str(data_imputado)
str(data_reducida)
str(data_transformada)
```

## KNN-Clasificador 
```{r}
set.seed(123)
## y~KNN(x1,..xn), se aconseja que las variables numéricas sean escaladas o normalizadas (estandarizada).
### En este caso está estandarizada.
data <- data_transformada # data <- data_reducida
summary(data)
plot(data$Exited)
(table(as.character(data$Exited))/sum(table(as.character(data$Exited))))*100
```


```{r}
# División del TRAIN & TEST
train_df <- data$group == "train" & !is.na(data$Exited)
muestra <- sample(1:nrow(train_df), size = nrow(trian_df)/3)
train<-data.frame(train_df[-muestra,])
test<-data.frame(train_df[muestra,])
cl<-data$Exited[-muestra]  # variable output que queremos pronosticar.
```


```{r}
train_df <- data[is_train, , drop = FALSE]
test_df  <- data[data$group == "test", , drop = FALSE]

# Ordenar y preparar prSummary dentre del caret
train_df$Exited <- as.character(train_df$Exited)
train_df$Exited <- ifelse(train_df$Exited %in% c("1", 1), "Yes",
                          ifelse(train_df$Exited %in% c("0", 0), "No", NA))

train_df$Exited <- factor(train_df$Exited, levels = c("Yes", "No"))
# Descartar columnas no necesarias
drops <- c("ID","group")
# Estandarización de la DATA, solo numéricas
# data_stand <- scale(train_df[, !(names(train_df) %in% drops)])
```




# Usando library CARET 
```{r}
## procesar data antes de poner en el modelo
is_train <- data$group == "train" & !is.na(data$Exited)
train_df <- data[is_train, , drop = FALSE]
test_df  <- data[data$group == "test", , drop = FALSE]
train_df$Exited <- factor(train_df$Exited, levels = c("0","1"))
levels(train_df$Exited) <- c("No","Yes")

drops <- c("ID","group","Surname")
train_df <- train_df[, setdiff(names(train_df), drops), drop = FALSE]
test_df  <- test_df [, intersect(names(test_df), names(train_df)), drop = FALSE]
```


```{r}
set.seed(123)
preProcValues <- preProcess(train_df, method = c("center", "scale"))

trainTransformed <- predict(preProcValues, train_df)
testTransformed <- predict(preProcValues, test_df)


ctrl <- trainControl(method="repeatedcv",repeats = 3,classProbs=TRUE,summaryFunction = twoClassSummary)

# sobre el 2/3 hacemos el Cross-validation, sobre el trainTransformed entrenamos el modelo
# va a hacer 3 veces con cada valor de k, y nos quedamos con el mejor.

knnModel <- train(
  Exited ~ ., 
  data = trainTransformed, 
  method = "knn", 
  trControl = trainControl(method = "cv"), 
  tuneGrid = data.frame(k = c(3,5,7)))

# Una vez entrenado el modelo, tenemos que decir con qué modelo nos quedamos
knnModel
plot(knnModel)
```


```{r}
ctrl_base <- trainControl(method="repeatedcv", number=10, repeats=3,
                          classProbs=TRUE, summaryFunction=twoClassSummary,
                          savePredictions="final")
ctrl_none <- ctrl_base
ctrl_down <- ctrl_base; ctrl_down$sampling <- "down"
ctrl_up   <- ctrl_base; ctrl_up$sampling   <- "up"


knn_none <- train(Exited ~ ., data=trainTransformed, method="knn",
                  trControl=ctrl_none, tuneGrid=data.frame(k=c(3,5,7)),
                  metric="ROC")
knn_down <- train(Exited ~ ., data=trainTransformed, method="knn",
                  trControl=ctrl_down, tuneGrid=data.frame(k=c(3,5,7)),
                  metric="ROC")
knn_up   <- train(Exited ~ ., data=trainTransformed, method="knn",
                  trControl=ctrl_up,   tuneGrid=data.frame(k=c(3,5,7)),
                  metric="ROC")

knn_none; knn_down; knn_up
#control= trainControl(method="repeatedcv",repeats=3) 
#knnModel <- train(
#  Exited ~ ., 
#  data = trainTransformed, 
#  method = "knn", 
#  trControl = control , 
#  tuneLength= 20)

#############
###control <- trainControl(method="repeatedcv",repeats = 3,classProbs=TRUE,summaryFunction = twoClassSummary)
```


```{r}
best_model<- knn3(
  Exited ~ .,
  data = trainTransformed,
  k = knnModel$bestTune$k
)
best_model
predictions <- predict(best_model, testTransformed,type = "class")
# Calculate confusion matrix
cm <- confusionMatrix(predictions, testTransformed$Exited)
cm
data.frame(Accuracy = cm$overall["Accuracy"],
           Sensitivity = cm$byClass["Sensitivity"],
           Specificity = cm$byClass["Specificity"])

roc(testTransformed$Exited, as.data.frame(predictions)[,"1"],plot=TRUE)
```


# KNN REGRESSION
```{r}
library(MASS)
data(Boston)
str(Boston)
help(Boston)
##Partición de Data usando library(caret)

set.seed(1)
inTraining <- createDataPartition(Boston$medv, p = .80, list = FALSE)
training <- Boston[inTraining,]
testing  <- Boston[-inTraining,]
set.seed(1)
model3 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale")
)
model3
###
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]

predictions = predict(model3, newdata = test.features)
# RMSE
sqrt(mean((test.target - predictions)^2))
# R2
cor(test.target, predictions) ^ 2
### Si agregamos cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 10)

model4 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale"),
  trControl = ctrl
)
model4
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]
predictions = predict(model4, newdata = test.features)
```

# RMSE
```{r}
sqrt(mean((test.target - predictions)^2))

##Tunning the model (GridSearch for k)
set.seed(1)

tuneGrid <- expand.grid(
  k = seq(5, 9, by = 1)
)

model5 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale"),
  trControl = ctrl,
  tuneGrid = tuneGrid
)
model5
plot(model5)
```

# KNN con Predictores MIXTOS
```{r}
## Cargamos los paquetes necesarios
#install.packages("VIM")
library(VIM)


# Cargamos la base de datos necesarios

dd <- read.table("credscoClean.csv",header = T,sep=";",stringsAsFactors=TRUE)
summary(dd)
dd[3310,1]<-"positiu"
test <- sample(1:nrow(dd), size = nrow(dd)/3)
dataTrain <- dd[-test,]
dataTest <- dd[test,]


# Creamos unos data frame auxiliares
aux <- dd
aux[test, 1] <- NA
summary(aux)

result <- kNN(aux,k=3,variable="Dictamen") # utilizando Gower, distancia mixta
table(result$Dictamen[test], dd$Dictamen[test])
table(result[test,1], dd[test,1])


result<-kNN(aux, variable = "Dictamen", weightDist = T)
table(result$Dictamen[test], dd$Dictamen[test])


#regression
aux<-dd
aux[test,2]<-NA
result<-kNN(aux, k=1,variable = "Antiguedad.Trabajo", weightDist = T)
table(result[test,2], dd[test,2])
plot(result[test,2], dd[test,2])
plot(result[test,2]- dd[test,2])

# usa Gower para regresión, puede ser la primera aproximación para nuestro modelo,pero no es el óptimo.
y<-dd[test,2]
yp<-result[test,2]
e1<-(y-yp)^2
e2<-abs(y-yp)
summary(e1)
summary(e2)
hist(e2)
plot(density(e2))
densityplot(e2)
mse<-sum((y-yp)^2)/(length(y))
rmse<-sqrt(mse)
```

# transformación de factores mixto (FAMD)


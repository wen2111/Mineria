---
title: "kNN"
author: "Siling Guo"
date: "2025-10-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
```


```{r}
str(data_transformada)
```

## KNN-Clasificador 
```{r}
data <- data_transformada
summary(data)
```

```{r}
plot(data$Exited)
(table(as.character(data$Exited))/sum(table(as.character(data$Exited))))*100
```
En la base de datos data_transformada, hay dos parte de datos, test original y train original. Además, es importante tener en cuenta la base de datos es desbalanceada, en las observaciones donde no sean NA, hay 79,28% No abandona el banco, y 20,71% sí abandona el banco. A la hora de hacer kNN test, se conviene aplicar los métodos Undersampling, Oversampling, ROSE y SMOTE.

## División del TRAIN & TEST
```{r}
set.seed(123)
train_df <- data[data$group == "train" & !is.na(data$Exited), ]
train_df<- subset(train_df, select = -c(ID, Surname, group))
train_df$Exited <- factor(train_df$Exited, levels = c("1","0"))


ind_col <- which(names(train_df) == "Exited") # columna de indicador
default_idx <- createDataPartition(train_df$Exited, p = 0.7, list = FALSE) # 70% para entrenamiento

X_trainC <- train_df[default_idx, ]
X_testC  <- train_df[-default_idx, ]

y_testC <- X_testC[, ind_col]
X_testC <- X_testC[, -ind_col]

# muestra <- sample(1:nrow(train_df), size = nrow(train_df)/3)
# train<-data.frame(train_df[-muestra,])
# test<-data.frame(train_df[muestra,])
# cl<-data$Exited[-muestra]  # variable output que queremos pronosticar.
```

```{r}
library(caret)

# Crear preprocesador para centrado y escalado
preproc <- preProcess(X_trainC, method = c("center", "scale"))

# Aplicar transformación
X_trainC <- predict(preproc, X_trainC)
X_testC <- predict(preproc, X_testC)
```

```{r}
y_trainC <- factor(ifelse(as.character(X_trainC$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_testC  <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = levels(y_trainC))

X_train_feat <- X_trainC[, setdiff(names(X_trainC), "Exited")]
X_test_feat  <- X_testC

# Convertir variables categóricas en dummies
dmy <- dummyVars(~ ., data = X_train_feat, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_train_feat))
test_num  <- data.frame(predict(dmy, newdata = X_test_feat))

# Escalar las variables numéricas
preproc <- preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)

# kNN CV
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
  x = trainScaled,
  y = y_trainC,
  method = "knn",
  trControl = ctrl,
  tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
```
Se observa que al aumentar k de 1 a 20, el Accuracy augmenta de 0,71 a 0,80, mientras que la veolocidad del crecimiento ya se ha moderado bastante a partir de k=17.
El valor de kappa ha estado bajo con todas las k, entre 0,12 y 0,16. Se afirma que la capacidad de classificar los dos tipos (yes exited y no exited) es muy bajo, teniedo en cuenta que se esta trabajando con una base de datos desbalanceada.

```{r}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
head(entrenamiento$results, 5)
```


```{r}
plot(entrenamiento)
```

```{r}
pred_label <- predict(entrenamiento, newdata = testScaled)
cm <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm
```
El modelo kNN obtuve la exactitud global (Accuracy) del 80,81%, 
lo que indica que acierta aproximadamente 80,81% del conjunto de prueba. Pero es trivial tener en cuenta el desequilibrio entre "Yes" (clientes abandonan) y "No" clientes permanencen.
- Se puede ver que la Sensibilidad = 0,1471, muestra que el modelo solo ha podido identificar correctamente un 14,71% de los clientes que abandonan. La gran mayoría de los casos no han sido detectados.
- Debido el sesgo havia la clase moyoritaria "No", la Especificidad es muy alta,  un  98,07%.
- La precisión positiva (Pos Pred Value) es 0,6667 indica que de todos los clientes que el modelo predice como "Yes"alrededor del 66,67% realmente abandonaron.
El índice Kappa es 0,1796 confirma que el acuerdo del modelo con las etiquetas reales es solo ligeramente meor que el azar. Es muy bajo para ser un modelo robusto.
El modelo tiene exactitud balanceada 0,56, refuerza la idea de que el modelo no consigue equilibrar bien la detección de ambas clases.

Para mejorar este rendimeinto, sería bueno aplicar técnicas como Undersampling, Oversampling, SMOTE, ROSE.




# Naaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
















```{r}
set.seed(123)
k_to_try = 1:100
err_k = rep(x = 0, times = length(k_to_try))

for (i in seq_along(k_to_try)) {
  pred <- knn(train = X_trainC, 
             test  = X_testC, 
             cl    = y_trainC, 
             k     = k_to_try[i], 
             prob = T)
  err_k[i] <- calc_class_err(y_testC, pred)
}
```

```{r}
# plot error vs choice of k
plot(err_k, type = "b", col = "dodgerblue", cex = 1, pch = 20, 
     xlab = "k, number of neighbors", ylab = "classification error",
     main = "(Test) Error Rate vs Neighbors")
# add line for min error seen
abline(h = min(err_k), col = "darkorange", lty = 3)
# add line for minority prevalence in test set
abline(h = mean(y_test == "Yes"), col = "grey", lty = 2)
```


```{r}
train_df <- data[is_train, , drop = FALSE]
test_df  <- data[data$group == "test", , drop = FALSE]

# Ordenar y preparar prSummary dentre del caret
train_df$Exited <- as.character(train_df$Exited)
train_df$Exited <- ifelse(train_df$Exited %in% c("1", 1), "Yes",
                          ifelse(train_df$Exited %in% c("0", 0), "No", NA))

train_df$Exited <- factor(train_df$Exited, levels = c("Yes", "No"))
# Descartar columnas no necesarias
drops <- c("ID","group")
# Estandarización de la DATA, solo numéricas
# data_stand <- scale(train_df[, !(names(train_df) %in% drops)])
```


# Usando library CARET 
```{r}
## procesar data antes de poner en el modelo
is_train <- data$group == "train" & !is.na(data$Exited)
train_df <- data[is_train, , drop = FALSE]
test_df  <- data[data$group == "test", , drop = FALSE]
train_df$Exited <- factor(train_df$Exited, levels = c("0","1"))
levels(train_df$Exited) <- c("No","Yes")

drops <- c("ID","group","Surname")
train_df <- train_df[, setdiff(names(train_df), drops), drop = FALSE]
test_df  <- test_df [, intersect(names(test_df), names(train_df)), drop = FALSE]
```


```{r}
set.seed(123)
preProcValues <- preProcess(train_df, method = c("center", "scale"))

trainTransformed <- predict(preProcValues, train_df)
testTransformed <- predict(preProcValues, test_df)


ctrl <- trainControl(method="repeatedcv",repeats = 3,classProbs=TRUE,summaryFunction = twoClassSummary)

# sobre el 2/3 hacemos el Cross-validation, sobre el trainTransformed entrenamos el modelo
# va a hacer 3 veces con cada valor de k, y nos quedamos con el mejor.

knnModel <- train(
  Exited ~ ., 
  data = trainTransformed, 
  method = "knn", 
  trControl = trainControl(method = "cv"), 
  tuneGrid = data.frame(k = c(3,5,7)))

# Una vez entrenado el modelo, tenemos que decir con qué modelo nos quedamos
knnModel
plot(knnModel)
```


```{r}
ctrl_base <- trainControl(method="repeatedcv", number=10, repeats=3,
                          classProbs=TRUE, summaryFunction=twoClassSummary,
                          savePredictions="final")
ctrl_none <- ctrl_base
ctrl_down <- ctrl_base; ctrl_down$sampling <- "down"
ctrl_up   <- ctrl_base; ctrl_up$sampling   <- "up"


knn_none <- train(Exited ~ ., data=trainTransformed, method="knn",
                  trControl=ctrl_none, tuneGrid=data.frame(k=c(3,5,7)),
                  metric="ROC")
knn_down <- train(Exited ~ ., data=trainTransformed, method="knn",
                  trControl=ctrl_down, tuneGrid=data.frame(k=c(3,5,7)),
                  metric="ROC")
knn_up   <- train(Exited ~ ., data=trainTransformed, method="knn",
                  trControl=ctrl_up,   tuneGrid=data.frame(k=c(3,5,7)),
                  metric="ROC")

knn_none; knn_down; knn_up
#control= trainControl(method="repeatedcv",repeats=3) 
#knnModel <- train(
#  Exited ~ ., 
#  data = trainTransformed, 
#  method = "knn", 
#  trControl = control , 
#  tuneLength= 20)

#############
###control <- trainControl(method="repeatedcv",repeats = 3,classProbs=TRUE,summaryFunction = twoClassSummary)
```


```{r}
best_model<- knn3(
  Exited ~ .,
  data = trainTransformed,
  k = knnModel$bestTune$k
)
best_model
predictions <- predict(best_model, testTransformed,type = "class")
# Calculate confusion matrix
cm <- confusionMatrix(predictions, testTransformed$Exited)
cm
data.frame(Accuracy = cm$overall["Accuracy"],
           Sensitivity = cm$byClass["Sensitivity"],
           Specificity = cm$byClass["Specificity"])

roc(testTransformed$Exited, as.data.frame(predictions)[,"1"],plot=TRUE)
```


# KNN REGRESSION
```{r}
library(MASS)
data(Boston)
str(Boston)
help(Boston)
##Partición de Data usando library(caret)

set.seed(1)
inTraining <- createDataPartition(Boston$medv, p = .80, list = FALSE)
training <- Boston[inTraining,]
testing  <- Boston[-inTraining,]
set.seed(1)
model3 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale")
)
model3
###
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]

predictions = predict(model3, newdata = test.features)
# RMSE
sqrt(mean((test.target - predictions)^2))
# R2
cor(test.target, predictions) ^ 2
### Si agregamos cross-validation
ctrl <- trainControl(
  method = "cv",
  number = 10)

model4 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale"),
  trControl = ctrl
)
model4
test.features = subset(testing, select=-c(medv))
test.target = subset(testing, select=medv)[,1]
predictions = predict(model4, newdata = test.features)
```

# RMSE
```{r}
sqrt(mean((test.target - predictions)^2))

##Tunning the model (GridSearch for k)
set.seed(1)

tuneGrid <- expand.grid(
  k = seq(5, 9, by = 1)
)

model5 <- train(
  medv ~ .,
  data = training,
  method = 'knn',
  preProcess = c("center", "scale"),
  trControl = ctrl,
  tuneGrid = tuneGrid
)
model5
plot(model5)
```

# KNN con Predictores MIXTOS
```{r}
## Cargamos los paquetes necesarios
#install.packages("VIM")
library(VIM)


# Cargamos la base de datos necesarios

dd <- read.table("credscoClean.csv",header = T,sep=";",stringsAsFactors=TRUE)
summary(dd)
dd[3310,1]<-"positiu"
test <- sample(1:nrow(dd), size = nrow(dd)/3)
dataTrain <- dd[-test,]
dataTest <- dd[test,]


# Creamos unos data frame auxiliares
aux <- dd
aux[test, 1] <- NA
summary(aux)

result <- kNN(aux,k=3,variable="Dictamen") # utilizando Gower, distancia mixta
table(result$Dictamen[test], dd$Dictamen[test])
table(result[test,1], dd[test,1])


result<-kNN(aux, variable = "Dictamen", weightDist = T)
table(result$Dictamen[test], dd$Dictamen[test])


#regression
aux<-dd
aux[test,2]<-NA
result<-kNN(aux, k=1,variable = "Antiguedad.Trabajo", weightDist = T)
table(result[test,2], dd[test,2])
plot(result[test,2], dd[test,2])
plot(result[test,2]- dd[test,2])

# usa Gower para regresión, puede ser la primera aproximación para nuestro modelo,pero no es el óptimo.
y<-dd[test,2]
yp<-result[test,2]
e1<-(y-yp)^2
e2<-abs(y-yp)
summary(e1)
summary(e2)
hist(e2)
plot(density(e2))
densityplot(e2)
mse<-sum((y-yp)^2)/(length(y))
rmse<-sqrt(mse)
```

# transformación de factores mixto (FAMD)


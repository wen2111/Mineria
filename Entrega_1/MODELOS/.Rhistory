data <- data %>% select(Exited, everything())
# Extraccion de train i test(kaggle)
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group")
train <- train[, !(names(train) %in% vars_drop)]
test <- test[,  !(names(test) %in% vars_drop)]
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# TRAIN I TEST NUESTRO
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
# MODELADO
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "rose")
hiperparametros <- data.frame(usekernel = FALSE, fL = 0, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
set.seed(123)
mod <- train(y=train2$Exited, x= train2[,c(2:9)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
data<-data_reducida
data <- data %>% select(Exited, everything())
# Extraccion de train i test(kaggle)
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group")
train <- train[, !(names(train) %in% vars_drop)]
test <- test[,  !(names(test) %in% vars_drop)]
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# TRAIN I TEST NUESTRO
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
View(data_reducida)
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "rose")
hiperparametros <- data.frame(usekernel = FALSE, fL = 0, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
set.seed(123)
mod <- train(y=train2$Exited, x= train2[,c(2:7)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# Predicciones
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "up")
hiperparametros <- data.frame(usekernel = FALSE, fL = 0, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
set.seed(123)
mod <- train(y=train2$Exited, x= train2[,c(2:7)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# Predicciones
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
#DATA
data<-data_imputado
data<-data_imputado
data <- data %>% select(Exited, everything())
# Extraccion de train i test(kaggle)
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group")
train <- train[, !(names(train) %in% vars_drop)]
test <- test[,  !(names(test) %in% vars_drop)]
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# TRAIN I TEST NUESTRO
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
View(test2)
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
# MODELADO
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
)
#sampling = "up"
hiperparametros <- data.frame(usekernel = FALSE, fL = 0, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
set.seed(123)
mod <- train(y=train2$Exited, x= train2[,c(2:20)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# Predicciones
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
data<-data_reducida_plus
data <- data %>% select(Exited, everything())
# Extraccion de train i test(kaggle)
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group")
train <- train[, !(names(train) %in% vars_drop)]
test <- test[,  !(names(test) %in% vars_drop)]
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# TRAIN I TEST NUESTRO
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
)
#sampling = "up"
hiperparametros <- data.frame(usekernel = FALSE, fL = 0, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
set.seed(123)
mod <- train(y=train2$Exited, x= train2[,c(2:9)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# Predicciones
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
#DATA
data<-data_imputado
data <- data %>% select(Exited, everything())
#DATA
data<-data_imputado
data <- data %>% select(Exited, everything())
# Extraccion de train i test(kaggle)
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group")
train <- train[, !(names(train) %in% vars_drop)]
test <- test[,  !(names(test) %in% vars_drop)]
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# TRAIN I TEST NUESTRO
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
# MODELADO
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "up")
hiperparametros <- data.frame(usekernel = FALSE, fL = 0, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
set.seed(123)
mod <- train(y=train2$Exited, x= train2[,c(2:20)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# Predicciones
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
mod <- train(y=train2$Exited, x= train2[,c(2:20)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# Predicciones
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "smote")
hiperparametros <- data.frame(usekernel = FALSE, fL = 0, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
#set.seed(123)
mod <- train(y=train2$Exited, x= train2[,c(2:20)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# Bayes
# librerias
library("caret")
library("naivebayes")
library("ggplot2")
library(e1071)
library(klaR)
load("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
#DATA
data<-data_reducida
data <- data %>% select(Exited, everything())
library("caret")
library("naivebayes")
library("ggplot2")
library(e1071)
library(klaR)
library(dplyr)
data <- data %>% select(Exited, everything())
data$ID <- data_imputado$ID # necesaria en test kaggle para hacer el submission, luego se quita de train
# Extraccion de train i test(kaggle)
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group", "ID")
train <- train[, !(names(train) %in% vars_drop)]
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
train_famd <- FAMD(train[, !names(train) %in% "Exited"],ncp = 20,
graph = FALSE)
train_famd <- FAMD(train[, !names(train) %in% "Exited"],ncp = 25,
graph = FALSE)
library(FactoMineR)
library(factoextra)
train_famd <- FAMD(train[, !names(train) %in% "Exited"],ncp = 25,
graph = FALSE)
get_eigenvalue(train.famd)
train_famd <- FAMD(train[, !names(train) %in% "Exited"],ncp = 25,
graph = FALSE)
get_eigenvalue(train.famd)
train_famd <- FAMD(train[, !names(train) %in% "Exited"],ncp = 25,
graph = FALSE)
get_eigenvalue(train_famd)
train_famd_coord <- as.data.frame(train_famd$ind$coord)
train_famd <- FAMD(train[, !names(train) %in% "Exited"],ncp = 25,
graph = FALSE)
train_famd_coord <- as.data.frame(train_famd$ind$coord)
train_famd_coord$Exited <- train$Exited
test_famd <- FAMD(test[, !names(test) %in% "Exited"],ncp=25,
graph = FALSE)
test_famd_coord <- as.data.frame(test_famd$ind$coord)
test_famd_coord$Exited <- test$Exited
get_eigenvalue(train_famd)
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "up")
hiperparametros <- data.frame(usekernel = FALSE, fL = 0, adjust=0)
train<-train_famd
test<-test_famd
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
View(train)
train<-train_famd_coord
train<-train_famd_coord
test<-test_famd_coord
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "up")
hiperparametros <- data.frame(usekernel = FALSE, fL = 0, adjust=0)
View(train2)
View(data_reducida)
View(data_reducida_plus)
View(train2)
# tener cuidado con la x, hay que tener en cuenta el rango de datos de train2
#set.seed(123)
mod <- train(y=train2$Exited, x= train2[,c(1:8)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
# Predicciones
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
load("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/Entrega_1/famd/famd_data.Rdata")
View(famd_features_imp)

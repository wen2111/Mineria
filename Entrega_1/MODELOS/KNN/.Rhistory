y_testC  <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = levels(y_trainC))
X_train_feat <- X_trainC[, setdiff(names(X_trainC), "Exited")]
X_test_feat  <- X_testC
# Convertir variables categóricas en dummies
dmy <- dummyVars(~ ., data = X_train_feat, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_train_feat))
test_num  <- data.frame(predict(dmy, newdata = X_test_feat))
# Escalar las variables numéricas
preproc <- preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
get_best_result = function(caret_fit) {
best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
best_result = caret_fit$results[best, ]
rownames(best_result) = NULL
best_result
}
head(entrenamiento$results, 16)
plot(entrenamiento)
fit_knn_15 <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = trainControl(method="cv", number=5, classProbs=TRUE),
tuneGrid = data.frame(k = 15)
)
fit_knn_15
confusionMatrix(predict(fit_knn_15, newdata=testScaled), y_testC)
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
# load("../Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data <- data_reducida
summary(data)
plot(data$Exited)
(table(as.character(data$Exited))/sum(table(as.character(data$Exited))))*100
set.seed(123)
df <- subset(data, !is.na(Exited))
df$Exited <- factor(ifelse(df$Exited == 1, "Yes", "No"), levels = c("Yes","No"))
idx <- caret::createDataPartition(df$Exited, p = 0.7, list = FALSE)
train_df <- df[idx, ]
test_df <- df[-idx,]
y_trainC <- train_df$Exited
y_testC <- test_df$Exited
X_trainC <- train_df[, setdiff(names(train_df), "Exited")]
X_testC <- test_df[, setdiff(names(test_df), "Exited")]
# Convertir variables categóricas en dummies
dmy <- caret::dummyVars(~ ., data = X_trainC, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_trainC))
test_num  <- data.frame(predict(dmy, newdata = X_testC))
# estandarizar las variables
preproc <- caret::preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
plot(entrenamiento)
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
# load("../Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data <- data_reducida
summary(data)
plot(data$Exited)
(table(as.character(data$Exited))/sum(table(as.character(data$Exited))))*100
set.seed(123)
df <- subset(data, !is.na(Exited))
df$Exited <- factor(ifelse(df$Exited == 1, "Yes", "No"), levels = c("Yes","No"))
idx <- caret::createDataPartition(df$Exited, p = 0.7, list = FALSE)
train_df <- df[idx, ]
test_df <- df[-idx,]
y_trainC <- train_df$Exited
y_testC <- test_df$Exited
X_trainC <- train_df[, setdiff(names(train_df), "Exited")]
X_testC <- test_df[, setdiff(names(test_df), "Exited")]
# Convertir variables categóricas en dummies
dmy <- caret::dummyVars(~ ., data = X_trainC, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_trainC))
test_num  <- data.frame(predict(dmy, newdata = X_testC))
# estandarizar las variables
preproc <- caret::preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
plot(entrenamiento)
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
save.image("KNN reducida.RData")
get_best_result = function(caret_fit) {
best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
best_result = caret_fit$results[best, ]
rownames(best_result) = NULL
best_result
}
head(entrenamiento$results, 16)
pred_label <- predict(entrenamiento, newdata = testScaled)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
# load("../Mineria/DATA/dataaaaaaaaaaaaaa.RData")
set.seed(123)
df <- subset(data, !is.na(Exited))
df$Exited <- factor(ifelse(df$Exited == 1, "Yes", "No"), levels = c("Yes","No"))
idx <- caret::createDataPartition(df$Exited, p = 0.7, list = FALSE)
train_df <- df[idx, ]
test_df <- df[-idx,]
y_trainC <- train_df$Exited
y_testC <- test_df$Exited
X_trainC <- train_df[, setdiff(names(train_df), "Exited")]
X_testC <- test_df[, setdiff(names(test_df), "Exited")]
# Convertir variables categóricas en dummies
dmy <- caret::dummyVars(~ ., data = X_trainC, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_trainC))
test_num  <- data.frame(predict(dmy, newdata = X_testC))
# estandarizar las variables
preproc <- caret::preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
plot(entrenamiento)
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
get_best_result = function(caret_fit) {
best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
best_result = caret_fit$results[best, ]
rownames(best_result) = NULL
best_result
}
head(entrenamiento$results, 16)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
set.seed(123)
df <- subset(data, !is.na(Exited))
df$Exited <- factor(ifelse(df$Exited == 1, "Yes", "No"), levels = c("Yes","No"))
idx <- createDataPartition(df$Exited, p = 0.7, list = FALSE)
train_df <- df[idx, ]
test_df <- df[-idx,]
y_trainC <- train_df$Exited
y_testC <- test_df$Exited
X_trainC <- train_df[, setdiff(names(train_df), "Exited")]
X_testC <- test_df[, setdiff(names(test_df), "Exited")]
# Convertir variables categóricas en dummies
dmy <- caret::dummyVars(~ ., data = X_trainC, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_trainC))
test_num  <- data.frame(predict(dmy, newdata = X_testC))
# estandarizar las variables
preproc <- caret::preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
plot(entrenamiento)
get_best_result = function(caret_fit) {
best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
best_result = caret_fit$results[best, ]
rownames(best_result) = NULL
best_result
}
head(entrenamiento$results, 16)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
get_best_result
head(entrenamiento$results, 16)
get_best_result(entrenamiento)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
set.seed(123)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
set.seed(123)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
set.seed(123)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
set.seed(123)
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
set.seed(123)
df <- subset(data, !is.na(Exited))
df$Exited <- factor(ifelse(df$Exited == 1, "Yes", "No"), levels = c("Yes","No"))
idx <- createDataPartition(df$Exited, p = 0.7, list = FALSE)
train_df <- df[idx, ]
test_df <- df[-idx,]
y_trainC <- train_df$Exited
y_testC <- test_df$Exited
X_trainC <- train_df[, setdiff(names(train_df), "Exited")]
X_testC <- test_df[, setdiff(names(test_df), "Exited")]
# Convertir variables categóricas en dummies
dmy <- dummyVars(~ ., data = X_trainC, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_trainC))
test_num  <- data.frame(predict(dmy, newdata = X_testC))
# estandarizar las variables
preproc <- preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
train_num
head(entrenamiento$results, 16)
set.seed(123)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
plot(entrenamiento)
set.seed(123)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
head(entrenamiento$results, 16)
set.seed(123)
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
set.seeed(123)
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
set.seeed(123)
set.seed(123)
set.seed(123)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
knn_7 <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = trainControl(method="cv", number=5, classProbs=TRUE),
tuneGrid = data.frame(k = 7)
)
fit_knn_15
knn_7 <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = trainControl(method="cv", number=5, classProbs=TRUE),
tuneGrid = data.frame(k = 7)
)
knn_7
confusionMatrix(predict(knn_7, newdata=testScaled), y_testC)
pred_label
y_testC
testScaled
head(entrenamiento$results, 16)
set.seed(123)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
set.seed(123)
knn_7 <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = trainControl(method="cv", number=5, classProbs=TRUE),
tuneGrid = data.frame(k = 7)
)
knn_7 <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = trainControl(method="cv", number=5, classProbs=TRUE),
tuneGrid = data.frame(k = 7)
)
knn_7
confusionMatrix(predict(knn_7, newdata=testScaled), y_testC)
set.seed(123)
knn_7 <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = trainControl(method="cv", number=5, classProbs=TRUE),
tuneGrid = data.frame(k = 9)
)
knn_7
confusionMatrix(predict(knn_7, newdata=testScaled), y_testC)
cm_cv_7<-confusionMatrix(predict(knn_7, newdata=testScaled), y_testC)
set.seed(123)
ctrl_up <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
sampling = "up", # oversampling dentro de cada fold
summaryFunction = twoClassSummary
)
fit_knn_up <- train(
x = trainScaled, y = y_trainC,
method = "knn",
metric = "ROC",  # optimiza por AUC, mejor para desbalanceo
trControl = ctrl_up,
tuneGrid = expand.grid(k = seq(1, 21, by = 2))
)
# Predicción
pred_prob_up <- predict(fit_knn_up, newdata = testScaled, type = "prob")
pred_lab_up  <- predict(fit_knn_up, newdata = testScaled)
# Evaluación (umbral 0.5)
cm_up <- confusionMatrix(pred_lab_up, y_testC, positive = "Yes")
cm_up
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
set.seed(123)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
# load("../Mineria/DATA/dataaaaaaaaaaaaaa.RData")
cm_cv_7
set.seed(123)
ctrl_up <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
sampling = "up", # oversampling dentro de cada fold
summaryFunction = twoClassSummary
)
fit_knn_up <- train(
x = trainScaled, y = y_trainC,
method = "knn",
metric = "ROC",  # optimiza por AUC, mejor para desbalanceo
trControl = ctrl_up,
tuneGrid = expand.grid(k = seq(1, 21, by = 2))
)
# Predicción
pred_prob_up <- predict(fit_knn_up, newdata = testScaled, type = "prob")
pred_lab_up  <- predict(fit_knn_up, newdata = testScaled)
# Evaluación (umbral 0.5)
cm_up <- confusionMatrix(pred_lab_up, y_testC, positive = "Yes")
cm_up
# umbral 0.30 (para subir Sensitivity)
tau <- 0.30
pred_tau <- factor(ifelse(pred_prob_up$Yes >= tau, "Yes", "No"), levels = levels(y_testC))
cm_tau <- confusionMatrix(pred_tau, y_testC, positive = "Yes")
cm_tau
# install.packages("randomForest")
library(randomForest)
X_train_rg <- train_df[default_idx, ]
set.seed(123)
df <- subset(data, !is.na(Exited))
df$Exited <- factor(ifelse(df$Exited == 1, "Yes", "No"), levels = c("Yes","No"))
idx <- createDataPartition(df$Exited, p = 0.7, list = FALSE)
train_df <- df[idx, ]
test_df <- df[-idx,]
y_trainC <- train_df$Exited
y_testC <- test_df$Exited
X_trainC <- train_df[, setdiff(names(train_df), "Exited")]
X_testC <- test_df[, setdiff(names(test_df), "Exited")]
# Convertir variables categóricas en dummies
dmy <- dummyVars(~ ., data = X_trainC, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_trainC))
test_num  <- data.frame(predict(dmy, newdata = X_testC))
# estandarizar las variables
preproc <- preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
# install.packages("randomForest")
library(randomForest)
X_train_rg <- df[idx, ]
X_test_rg  <- df[-idx, ]
X_train_rg$Exited <- factor(ifelse(as.character(X_train_rg$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_test_rg <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = c("Yes","No"))
# Asegura que todos los predictores no sean character
chr_cols <- sapply(X_train_rg, is.character)
X_train_rg[chr_cols] <- lapply(X_train_rg[chr_cols], factor)
X_test_rg[chr_cols] <- lapply(X_test_rg[chr_cols], factor)
chr_cols
set.seed(123)
ctrl_fast <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
sampling = "up",
summaryFunction = twoClassSummary,
savePredictions = "final"
)
fit_rg_fast <- train(
Exited ~ ., data = X_train_rg,
method   = "ranger",
trControl= ctrl_fast,
metric   = "ROC",
tuneLength = 5, # que el caret busque 5 valores de mtry
importance = "none",
num.trees = 300 # menos árboles para mayor velocidad
)
y_test_rg
library(randomForest)
X_train_rg <- df[idx, ]
X_test_rg  <- df[-idx, ]
set.seed(123)
ctrl_fast <- trainControl(
method = "cv", number = 5,
classProbs = TRUE,
sampling = "up",
summaryFunction = twoClassSummary,
savePredictions = "final"
)
fit_rg_fast <- train(
Exited ~ ., data = X_train_rg,
method   = "ranger",
trControl= ctrl_fast,
metric   = "ROC",
tuneLength = 5, # que el caret busque 5 valores de mtry
importance = "none",
num.trees = 300 # menos árboles para mayor velocidad
)
fit_rg_fast
save.image("kNNEnvironment_reducida.RData")
set.seed(123)
prob_rg <- predict(fit_rg_fast, newdata = X_test_rg, type = "prob")
pred_rg <- predict(fit_rg_fast, newdata = X_test_rg)
pred_rg<- factor(pred_rg, levels = c("Yes","No"))
X_test_rg$Exited <- factor(ifelse(as.character(X_test_rg$Exited) == "1", "Yes", "No"),levels = c("Yes","No"))
cm_rg <- confusionMatrix(pred_rg, X_test_rg$Exited, positive = "Yes")
cm_rg
setwd("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/Entrega_1/KNN")
save.image("kNNEnvironment_reducida.RData")
set.seed(123)
pred_rg <- predict(fit_rg_fast, newdata = X_test_rg)
pred_rg<- factor(pred_rg, levels = c("Yes","No"))
X_test_rg$Exited <- factor(ifelse(as.character(X_test_rg$Exited) == "1", "Yes", "No"),levels = c("Yes","No"))
cm_rg <- confusionMatrix(pred_rg, X_test_rg$Exited, positive = "Yes")
cm_rg
save.image("kNNEnvironment_reducida.RData")

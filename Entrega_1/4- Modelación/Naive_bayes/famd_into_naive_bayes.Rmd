---
title: "Naive Bayes Classificator with FAMD"
author: "Melissa Vargas"
date: "2025-11-05"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# libraries
library("FactoMineR")
library("factoextra")
library("caret")
library("naivebayes")
#library("reshape")
library("ggplot2")
library(e1071)
library(klaR)
#library(naniar)
```

## Load data
```{r}
load("dataaaaaaaaaaaaaa.RData")
```

## Data preparation:
```{r}
train_reducida <- subset(data_reducida, group == "train") # 7000 obs
test_reducida  <- subset(data_reducida,
                         group == "test") # 3000 obs variable respuesta vacia

vars_drop <- c("ID", "Surname", "group")
train_reducida <- train_reducida[, !(names(train_reducida) %in% vars_drop)]
test_reducida  <- test_reducida[,  !(names(test_reducida) %in% vars_drop)]

train_reducida$Exited <- factor(train_reducida$Exited,
                                levels = c("1","0"),
                                labels = c("Yes","No"))
```

## Partition
```{r}
set.seed(123)
index <- createDataPartition(train_reducida$Exited, p = 0.7, list = FALSE)
train_reducida2 <- train_reducida[index, ] # train interno
test_reducida2  <- train_reducida[-index, ] # test interno
```

## FAMD
```{r}
train_famd <- FAMD(train_reducida2[, !names(train_reducida2) %in% "Exited"],
                   graph = FALSE)
fviz_screeplot(train_famd, addlabels = TRUE, ylim = c(0, 20)) +
  ggtitle("Varianza explicada por dimensión (FAMD)")

train_famd_coord <- as.data.frame(train_famd$ind$coord)
train_famd_coord$Exited <- train_reducida2$Exited

test_famd <- FAMD(test_reducida2[, !names(test_reducida2) %in% "Exited"],
                   graph = FALSE)
test_famd_coord <- as.data.frame(test_famd$ind$coord)
test_famd_coord$Exited <- test_reducida2$Exited
fviz_screeplot(test_famd, addlabels = TRUE, ylim = c(0, 20)) +
  ggtitle("Varianza explicada por dimensión (FAMD)")

# esto es porque daba un error porque los nombre de las dims no eran iguales en train y test
names(train_famd_coord) <- c("Dim.1", "Dim.2", "Dim.3", "Dim.4", "Dim.5", 
                            "Dim.6", "Dim.7", "Dim.8", "Exited")
```

## Modelind

```{r, warning=FALSE}
# cross-validation
control <- trainControl(method = "repeatedcv", repeats = 3)
hiperparametros <- data.frame(usekernel = FALSE, fL = 1, adjust = 0)

mod_nb_famd <- train(
  y = train_famd_coord$Exited, 
  x = train_famd_coord[, 1:8],  # Usar las primeras 8 dimensiones (se puede cambiar pero primero cambiar ncp en el FAMD)
  method = "nb", 
  tuneGrid = hiperparametros, 
  metric = "Accuracy",	
  trControl = control
)

# Predicciones
train_pred <- predict(mod_nb_famd, train_famd_coord, type = "raw")
test_pred <- predict(mod_nb_famd, test_famd_coord, type = "raw")

# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train_famd_coord$Exited)
conf_test <- confusionMatrix(test_pred, test_famd_coord$Exited)

# F1-score
f1_score <- function(cm){
  precision <- cm$byClass["Precision"]
  recall <- cm$byClass["Sensitivity"]
  f1 <- 2 * (precision * recall) / (precision + recall)
  return(as.numeric(f1))
}

f1_train <- f1_score(conf_train)
f1_test <- f1_score(conf_test)
```

## KPI's
```{r}
# KPIs
resultados_famd <- data.frame(
  Dataset = c("Train", "Test"),
  Error_rate = c(1 - conf_train$overall["Accuracy"], 1 - conf_test$overall["Accuracy"]),
  Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
  Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
  Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
  Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
  F1_Score = c(f1_train, f1_test)
)
resultados_famd
```


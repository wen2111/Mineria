---
title: "CART reducido_plus"
output:
  pdf_document: default
  html_document: default
date: "2025-11-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
library(rpart)
library(caret)
library(rpart.plot)
```

```{r}
data4tree<-data_reducida_plus[0:7000,]
datatest<-data_reducida_plus[7001:10000,]
ind <- sample(1:nrow(data4tree), 0.7*nrow(data4tree))
train <- data4tree[ind,]
test <- data4tree[-ind,]
```

# Método cp=0

```{r}
tree <- rpart(Exited ~ ., data = train, cp = 0)
printcp(tree)
plotcp(tree)
```

## Elección cp óptimo

```{r}
xerror <- tree$cptable[,"xerror"]
xerror
imin.xerror <- which.min(xerror)
imin.xerror
tree$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + tree$cptable[imin.xerror, "xstd"]
upper.xerror
```
### Cp mínimo

```{r}
tree2 <- prune(tree, cp = 0.002967359)
importance <- tree2$variable.importance
importance <- round(100*importance/sum(importance), 1)
importance
```
Matriz de confusión para train
```{r}
p <- predict(tree2, train, type = 'class')
(conf_train<-confusionMatrix(p, train$Exited, positive="1"))
```
Matriz de confusión para test
```{r}
p2 <- predict(tree2, test, type = 'class')
(conf_test<-confusionMatrix(p2, test$Exited, positive="1"))
```

F1 score
```{r}
 f1_score <- function(cm){
 precision <- cm$byClass["Precision"]
 recall <- cm$byClass["Sensitivity"]
 f1 <- 2 * (precision * recall) / (precision + recall)
 return(as.numeric(f1))
 }
 print(f1_train <- f1_score(conf_train))
 print(f1_test <- f1_score(conf_test))
```
Los resultados no son satisfactorios:
- Valores pobres para F1score y recall
- Indicios de overfitting (0.4702809>>0.3987915)

### Cp mínimo+ error estándar

```{r}
icp <- which(tree$cptable[, "xerror"] <= upper.xerror)[1]
cp_optimo_1se <- tree$cptable[icp, "CP"]
tree3 <- prune(tree, cp = cp_optimo_1se)
importance <- tree3$variable.importance
importance <- round(100*importance/sum(importance), 1)
importance
```
Vemos como han cambiado los valores de importancia. Ahora se comprovará si mejoran los KPI.
Matriz de confusión para train
```{r}
p <- predict(tree3, train, type = 'class')
conf_train<-confusionMatrix(p, train$Exited, positive="1")
```
Matriz de confusión para test
```{r}
p2 <- predict(tree3, test, type = 'class')
conf_test<-confusionMatrix(p2, test$Exited, positive="1")
```

F1 score
```{r}
 print(f1_train <- f1_score(conf_train))
 print(f1_test <- f1_score(conf_test))
```
No hay overfitting, pero el f1 score es notablemente peor.

# Método Caret

```{r}
caret.rpart <- train(Exited ~ ., method = "rpart", data = train, 
                     tuneLength = 20,
                     trControl = trainControl(method = "cv", number = 10)) 
ggplot(caret.rpart)
rpart.plot(caret.rpart$finalModel)
```
Importancia de las variables:
```{r}
var.imp <- varImp(caret.rpart)
plot(var.imp)
```
Las predicciones:

Matriz de confusión datos train
```{r}
pred1 <- predict(caret.rpart, newdata = train)
(conf_test<-confusionMatrix(pred1, train$Exited, positive="1"))
```
Matriz de confusión datos test:
```{r}
pred <- predict(caret.rpart, newdata = test)
(conf_train<-confusionMatrix(pred, test$Exited, positive="1"))
```

Los F1score:
```{r}
 print(f1_train <- f1_score(conf_train))
 print(f1_test <- f1_score(conf_test))
```
- Valores muy pobres de F1 y recall
- Indicios de overfitting

# Conclusiones para data_reducido_plus sin balancear

Resultados bastante mejorables tanto con el paquete Caret como encontrando el Cp óptimo a partir del árbol más grande (Cp=0). Sumar la desviación típica tampoco mejora los resultados. Probaremos balanceando los datos.


---
title: "Classification Tree bbdd imputado_bsd"
author: "Grupo 5"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
geometry: "left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm"
header-includes:
  - \usepackage{tocloft}
  - \renewcommand{\contentsname}{\hfill\LARGE\textbf{ÍNDICE}\hfill}
  - \renewcommand{\cftaftertoctitle}{\hfill}
  - \addtocontents{toc}{\protect\thispagestyle{empty}}
  - \addtocontents{toc}{\protect\setcounter{tocdepth}{3}}
  - \renewcommand{\cftsecafterpnum}{\vspace{8pt}}
  - \renewcommand{\cftsubsecafterpnum}{\vspace{5pt}}
  - \renewcommand{\cftsubsubsecafterpnum}{\vspace{3pt}}
  - \renewcommand{\cftdotsep}{1.5}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
load("~/Documents/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
library(rpart)
library(caret)
library(rpart.plot)
library(ROSE)
```

# Base de datos reducido_plus SIN BALANCEAR
```{r}
data4tree<-data_imputado[0:7000,]
datatest<-data_imputado[7001:10000,]
ind <- sample(1:nrow(data4tree), 0.7*nrow(data4tree))
train <- data4tree[ind,]
test <- data4tree[-ind,]
```

## Método cp=0

```{r}
tree <- rpart(Exited ~ ., data = train, cp = 0)
printcp(tree)
plotcp(tree)
```

Mirando el gráfico, el mínimo se encuentra aproximadamente en la región donde Lambda es alrededor de 0.0032 - 0.0019, Número de variables = 34 - 64, Error relativo = 0.95

El punto más bajo parece estar alrededor de lambda = 0.0032 con aproximadamente 34-51 variables en el modelo.

### Elección cp óptimo

```{r}
xerror <- tree$cptable[,"xerror"]
xerror
imin.xerror <- which.min(xerror)
imin.xerror
tree$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + tree$cptable[imin.xerror, "xstd"]
upper.xerror
```
Los valores son bastante similados para el caso de la base de datos
*balanceado plus*  El mínimo error es 0.9341 en la posición 6, que corresponde a un árbol con 14 divisiones y un CP = 0.004491018.

#### Cp mínimo

```{r}
tree2 <- prune(tree, cp = 0.003940887)
importance <- tree2$variable.importance
importance <- round(100*importance/sum(importance), 1)
importance
```
Los resultados muestran que NumOfProducts (33.8%) y Age (38.9%) son las variables dominantes, explicando conjuntamente el 72.7% de la capacidad predictiva del modelo para identificar clientes que abandonarán el banco. 
Esto indica que el número de productos contratados y la edad del cliente son los factores más determinantes en la decisión de abandono. Las siguientes variables en importancia, IsActiveMember (4.9 %) y Balance (8%), tienen un impacto considerablemente menor, mientras que factores como Geography (0.4%) resultan prácticamente irrelevantes para el modelo. 

Matriz de confusión para train
```{r, echo=FALSE}
p <- predict(tree2, train, type = 'class')
(conf_train<-confusionMatrix(p, train$Exited, positive="1"))
```
Matriz de confusión para test
```{r, echo=FALSE}
p2 <- predict(tree2, test, type = 'class')
(conf_test<-confusionMatrix(p2, test$Exited, positive="1"))
```

F1 score
```{r, echo=FALSE}
 f1_score <- function(cm){
 precision <- cm$byClass["Precision"]
 recall <- cm$byClass["Sensitivity"]
 f1 <- 2 * (precision * recall) / (precision + recall)
 return(as.numeric(f1))
 }
 cat("f1 datos train", f1_train <- f1_score(conf_train))
 cat("f1 datos test", f1_score(conf_test))
```
Los resultados no son satisfactorios:
- Valores pobres para F1score y recall
- Hay indicios de ligero overfitting: el F1 en train es claramente mayor que en test (~5 puntos), aunque la diferencia no es grande.

#### Cp mínimo+ error estándar

```{r}
icp <- which(tree$cptable[, "xerror"] <= upper.xerror)[1]
cp_optimo_1se <- tree$cptable[icp, "CP"]
tree3 <- prune(tree, cp = cp_optimo_1se)
importance <- tree3$variable.importance
importance <- round(100*importance/sum(importance), 1)
importance
```
El peso de las variables ha augmentado en algunos casos. 

Matriz de confusión para train
```{r, echo=FALSE}
p <- predict(tree3, train, type = 'class')
conf_train<-confusionMatrix(p, train$Exited, positive="1")
```
Matriz de confusión para test
```{r, echo=FALSE}
p2 <- predict(tree3, test, type = 'class')
conf_test<-confusionMatrix(p2, test$Exited, positive="1")
```

F1 score
```{r, echo=FALSE}
 cat("f1 datos train", f1_train <- f1_score(conf_train))
 cat("f1 datos test", f1_score(conf_test))
```
No hay overfitting, el f1 ha disminuido para este caso, no obstante los dos valores se podrian considerar parecidos

## Método Caret

```{r}
caret.rpart <- train(Exited ~ ., method = "rpart", data = train,
 tuneLength = 20,
 trControl = trainControl(method = "cv", number = 10))
ggplot(caret.rpart)
rpart.plot(caret.rpart$finalModel)
```
El árbol muestra que la edad y el número de productos son los factores clave: los clientes más jóvenes (<42) casi siempre permanecen, mientras que los pocos casos con muchos productos y alta actividad o mayores con >=3 productos tienen mayor probabilidad de marcharse.

Importancia de las variables:
```{r, echo=FALSE}
var.imp <- varImp(caret.rpart)
plot(var.imp)
```
Las predicciones:

Matriz de confusión datos train
```{r, echo=FALSE}
pred1 <- predict(caret.rpart, newdata = train)
(conf_test<-confusionMatrix(pred1, train$Exited, positive="1"))
```
Matriz de confusión datos test:
```{r, echo=FALSE}
pred <- predict(caret.rpart, newdata = test)
(conf_train<-confusionMatrix(pred, test$Exited, positive="1"))
```

Los F1score:
```{r, echo=FALSE}
 cat("f1 datos train", f1_train <- f1_score(conf_train))
 cat("f1 datos test", f1_score(conf_test))
```
- Valores muy pobres de F1 y recall
- Indicios de overfitting

## Conclusiones para data imputado sin balancear

Con los datos imputados, los resultados siguen siendo claramente mejorables: ni el tuning con caret ni ajustar el Cp óptimo aportan mejoras sustanciales. Esto indica que el problema no está en el modelo sino en la estructura del conjunto imputado, por lo que balancear las clases es el siguiente paso adecuado para intentar mejorar el rendimiento.

# Base de datos reducido_plus BALANCEO 

A continuación se buscará para diferentes niveles de balanceo dónde se obtienen mejores kpi con caret

```{r, echo=FALSE,warning=FALSE}
library(ROSE)
library(caret)
library(dplyr)
library(ggplot2)

set.seed(1271)

# Función F1-Score robusta (evita NaN)
f1_score <- function(conf_matrix) {
  precision <- conf_matrix$byClass["Pos Pred Value"]
  recall <- conf_matrix$byClass["Sensitivity"]
  
  if (is.na(precision) | is.na(recall) | (precision + recall) == 0) {
    return(0)
  } else {
    return(2 * (precision * recall) / (precision + recall))
  }
}

# Semillas para reproducibilidad en cross-validation
seeds <- vector(mode = "list", length = 6) 
for(i in 1:5) seeds[[i]] <- sample.int(1000, 10) 
seeds[[6]] <- sample.int(1000, 1)

# Data frame para almacenar resultados
resultados <- data.frame()

# Bucle sobre distintos valores de p
for(p in seq(0.05, 0.50, by = 0.05)) {
  set.seed(123)
  
  # Generar dataset balanceado con ROSE
  data_balanced <- ROSE(Exited ~ ., data = train, p = p, seed = 123)$data
  
  # Entrenar modelo árbol de decisión
  model <- train(
    Exited ~ ., 
    method = "rpart", 
    data = data_balanced,
    tuneLength = 10, 
    trControl = trainControl(method = "cv", number = 5, seeds = seeds)
  )
  
  # Predicciones
  pred_train <- predict(model, train)
  pred_test <- predict(model, test)
  
  # Matrices de confusión
  cm_train <- confusionMatrix(pred_train, train$Exited, positive = "1")
  cm_test <- confusionMatrix(pred_test, test$Exited, positive = "1")
  
  # Guardar métricas
  nueva_fila <- data.frame(
    p = p,
    F1_Train = f1_score(cm_train),
    F1_Test = f1_score(cm_test),
    Sensitivity_Train = cm_train$byClass["Sensitivity"],
    Sensitivity_Test = cm_test$byClass["Sensitivity"],
    Specificity_Train = cm_train$byClass["Specificity"],
    Specificity_Test = cm_test$byClass["Specificity"],
    Accuracy_Train = cm_train$overall["Accuracy"],
    Accuracy_Test = cm_test$overall["Accuracy"]
  )
  
  resultados <- rbind(resultados, nueva_fila)
}

# Mostrar resultados
print(resultados)

# Gráfico F1-Test vs p
ggplot(resultados, aes(x = p, y = F1_Test)) +
  geom_line(color = "blue") +
  geom_point(color = "red") +
  labs(title = "F1-Score Test vs p (ROSE)", x = "p (proporción de clase positiva)", y = "F1-Score Test") +
  theme_minimal()

```

## Conclusiones balanceo

Para valores bajos de p ( < 0.2) el modelo casi no predice la clase positiva, dando F1 muy bajos.

Un balance cercano a p = 0.4 - 0.5 maximiza F1-Test y sensibilidad, siendo óptimo para detectar la clase minoritaria.

```{r}
library(ggplot2)
library(reshape2)

# Seleccionar métricas para graficar
resultados_long <- melt(resultados, id.vars = "p", 
                        measure.vars = c("F1_Test", "Sensitivity_Test", "Accuracy_Test"),
                        variable.name = "Metric", value.name = "Value")

# Gráfico
ggplot(resultados_long, aes(x = p, y = Value, color = Metric)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  labs(title = "Métricas Test vs p (ROSE)", x = "p (proporción clase positiva)", y = "Valor") +
  scale_color_manual(values = c("F1_Test" = "blue", 
                                "Sensitivity_Test" = "red", 
                                "Accuracy_Test" = "green")) +
  theme_minimal()
```




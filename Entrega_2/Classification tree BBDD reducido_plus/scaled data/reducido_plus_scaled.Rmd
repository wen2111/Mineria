---
title: "Classification Tree bbdd reducido_plus"
author: "Grupo 5"
output:
  html_document:
    toc: true
    toc_depth: '3'
    df_print: paged
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
geometry: left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm
header-includes:
- \usepackage{tocloft}
- \renewcommand{\contentsname}{\hfill\LARGE\textbf{ÍNDICE}\hfill}
- \renewcommand{\cftaftertoctitle}{\hfill}
- \addtocontents{toc}{\protect\thispagestyle{empty}}
- \addtocontents{toc}{\protect\setcounter{tocdepth}{3}}
- \renewcommand{\cftsecafterpnum}{\vspace{8pt}}
- \renewcommand{\cftsubsecafterpnum}{\vspace{5pt}}
- \renewcommand{\cftsubsubsecafterpnum}{\vspace{3pt}}
- \renewcommand{\cftdotsep}{1.5}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include=FALSE}
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
library(rpart)
library(caret)
library(rpart.plot)
library(ROSE)
library(dplyr)
set.seed(12345)
```

# Base de datos reducido_plus SIN BALANCEAR ESCALADA
```{r}
data_reducida_plus <- data_reducida_plus %>%
  mutate(across(where(is.numeric), ~ as.numeric(scale(.x))))
data4tree<-data_reducida_plus[0:7000,]
datatest<-data_reducida_plus[7001:10000,]
ind <- sample(1:nrow(data4tree), 0.7*nrow(data4tree))
train <- data4tree[ind,]
test <- data4tree[-ind,]
```

## Método cp=0

```{r}
tree <- rpart(Exited ~ ., data = train, cp = 0)
printcp(tree)
plotcp(tree)
```

### Elección cp óptimo

```{r}
xerror <- tree$cptable[,"xerror"]
xerror
imin.xerror <- which.min(xerror)
imin.xerror
tree$cptable[imin.xerror, ]
upper.xerror <- xerror[imin.xerror] + tree$cptable[imin.xerror, "xstd"]
upper.xerror
```

#### Cp mínimo

```{r}
tree2 <- prune(tree, cp = 0.002967359)
importance <- tree2$variable.importance
importance <- round(100*importance/sum(importance), 1)
importance
```
Matriz de confusión para train
```{r, echo=FALSE}
p <- predict(tree2, train, type = 'prob')
pclass<-c()
for(i in 1:nrow(train)){
pclass[i]<-ifelse(p[i,2]>=0.2071429,1,0)
}
pclass<-as.factor(pclass)
(conf_train<-confusionMatrix(pclass, train$Exited, positive="1"))
```
Matriz de confusión para test
```{r, echo=FALSE}
p <- predict(tree2, test, type = 'prob')
pclass2<-c()
for(i in 1:nrow(p)){
pclass2[i]<-ifelse(p[i,2]>=0.2071429,1,0)
}
pclass2<-as.factor(pclass2)
(conf_test<-confusionMatrix(pclass2, test$Exited, positive="1"))
```

F1 score
```{r, echo=FALSE}
 f1_score <- function(cm){
 precision <- cm$byClass["Precision"]
 recall <- cm$byClass["Sensitivity"]
 f1 <- 2 * (precision * recall) / (precision + recall)
 return(as.numeric(f1))
 }
 cat("f1 datos train",  f1_score(conf_train), "f1 datos test", f1_score(conf_test))
```
Los resultados no son satisfactorios: hay mucho overfitting

#### Cp mínimo+ error estándar

```{r}
icp <- which(tree$cptable[, "xerror"] <= upper.xerror)[1]
cp_optimo_1se <- tree$cptable[icp, "CP"]
tree3 <- prune(tree, cp = cp_optimo_1se)
importance <- tree3$variable.importance
importance <- round(100*importance/sum(importance), 1)
importance
```
Vemos como han cambiado los valores de importancia. Ahora se comprovará si mejoran los KPI.
Matriz de confusión para train
```{r, echo=FALSE}
p <- predict(tree3, train, type = 'prob')
pclass<-c()
for(i in 1:nrow(train)){
pclass[i]<-ifelse(p[i,2]>=0.2071429,1,0)
}
pclass<-as.factor(pclass)
(conf_train<-confusionMatrix(pclass, train$Exited, positive="1"))
```
Matriz de confusión para test
```{r, echo=FALSE}
p <- predict(tree3, test, type = 'prob')
pclass2<-c()
for(i in 1:nrow(p)){
pclass2[i]<-ifelse(p[i,2]>=0.2071429,1,0)
}
pclass2<-as.factor(pclass2)
(conf_test<-confusionMatrix(pclass2, test$Exited, positive="1"))
```

F1 score
```{r, echo=FALSE}
 cat("f1 datos train",f1_score(conf_train), "f1 datos test", f1_score(conf_test))
```
Sigue habiendo mucho overfitting

## Método Caret

```{r}
caret.rpart <- train(Exited ~ ., method = "rpart", data = train,
 tuneLength = 20,
 trControl = trainControl(method = "cv", number = 10))
ggplot(caret.rpart)
rpart.plot(caret.rpart$finalModel)
```
Importancia de las variables:
```{r, echo=FALSE}
var.imp <- varImp(caret.rpart)
plot(var.imp)
```
Las predicciones:

Matriz de confusión datos train
```{r, echo=FALSE}
pred1 <- predict(caret.rpart, newdata = train, type = "prob")
pclass<-c()
for(i in 1:nrow(train)){
pclass[i]<-ifelse(pred1[i,2]>=0.2071429,1,0)
}
pclass<-as.factor(pclass)
(conf_train<-confusionMatrix(pclass, train$Exited, positive="1"))
```
Matriz de confusión datos test:
```{r, echo=FALSE}
pred <- predict(caret.rpart, newdata = test, type="prob")
pclass2<-c()
for(i in 1:nrow(p)){
pclass2[i]<-ifelse(p[i,2]>=0.2071429,1,0)
}
pclass2<-as.factor(pclass2)
(conf_test<-confusionMatrix(pclass2, test$Exited, positive="1"))
```

Los F1score:
```{r, echo=FALSE}
 cat("f1 datos train",f1_score(conf_train), "f1 datos test", f1_score(conf_test))
```
- Valores muy pobres de F1 y recall
- Indicios de overfitting

## Conclusiones para data_reducido_plus sin balancear

Resultados bastante mejorables tanto con el paquete Caret como encontrando el Cp óptimo a partir del árbol más grande (Cp=0). Sumar la desviación típica tampoco mejora los resultados. Probaremos balanceando los datos.


# Base de datos reducido_plus BALANCEO 

A continuación se buscará para diferentes niveles de balanceo dónde se obtienen mejores kpi con caret

```{r, echo=FALSE}
library(ROSE)
library(caret)
library(dplyr)
set.seed(1271)
# Función F1-Score
f1_score <- function(conf_matrix) {
  precision <- conf_matrix$byClass["Pos Pred Value"]
  recall <- conf_matrix$byClass["Sensitivity"]
  2 * (precision * recall) / (precision + recall)
}
seeds <- vector(mode = "list", length = 6) 
for(i in 1:5) seeds[[i]] <- sample.int(1000, 10) 
seeds[[6]] <- sample.int(1000, 1)

resultados <- data.frame()

for(p in seq(0.05, 0.50, by = 0.05)) {
  set.seed(123)
  
  data_balanced <- ROSE(Exited ~ ., data = train, p = p, seed = 123)$data
  
  model <- train(Exited ~ ., method = "rpart", data = data_balanced,
                 tuneLength = 10, trControl = trainControl(method = "cv", number = 5,seeds = seeds))
  
  pred_train <- predict(model, train, type="prob")
  pclass<-c()
for(i in 1:nrow(train)){
pclass[i]<-ifelse(pred_train[i,2]>=p,1,0)
}
pclass<-as.factor(pclass)
  pred_test <- predict(model, test, type="prob")
    pclass2<-c()
for(i in 1:nrow(test)){
pclass2[i]<-ifelse(pred_test[i,2]>=p,1,0)
}
pclass2<-as.factor(pclass2)
  cm_train <- confusionMatrix(pclass, train$Exited, positive = "1")
  cm_test <- confusionMatrix(pclass2, test$Exited, positive = "1")
  
  nueva_fila <- data.frame(
    F1_Train = f1_score(cm_train),
    F1_Test = f1_score(cm_test),
    Sensitivity_Train = cm_train$byClass["Sensitivity"],
    Sensitivity_Test = cm_test$byClass["Sensitivity"],
    Specificity_Train = cm_train$byClass["Specificity"],
    Specificity_Test = cm_test$byClass["Specificity"],
    Accuracy_Train = cm_train$overall["Accuracy"],
    Accuracy_Test = cm_test$overall["Accuracy"]
  )
  
  rownames(nueva_fila) <- paste0("p_", p)
  resultados <- rbind(resultados, nueva_fila)
}

print(resultados)
```



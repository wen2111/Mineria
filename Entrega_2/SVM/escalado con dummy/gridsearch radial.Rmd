---
title: "gridsearch radial"
output: html_document
date: "2025-12-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(recipes)
library(e1071)
library(mlbench)
library(ggplot2)
library(ISLR)
library(caret)
library(pROC)
library(dplyr)
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
bd<-data_reducida

```
# Escalado y dummyficado

```{r}
rec <- recipe(Exited ~ ., data = bd) %>%
  step_dummy(all_nominal_predictors(), -group, one_hot = TRUE, keep_original_cols = FALSE)%>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())
bd<- prep(rec) %>% bake(new_data = NULL)


```

# Train y test

```{r}
trainbase<-bd[bd$group=="train",]
trainbase$group<-NULL
testbase<-bd[bd$group=="test",]
testbase$group<-NULL
ind <- sample(1:nrow(trainbase), 0.7*nrow(trainbase))
train <- trainbase[ind,]
test <- trainbase[-ind,]
```

#gridsearch radial

```{r}
svm_cv_rbf <- tune(
  svm,
  Exited ~ .,
  data = train,
  kernel = "radial",
  ranges = list(
    cost  = c(0.1, 1, 3, 5, 10, 20, 50),
    gamma = c(0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2)
  ),
  tunecontrol = tune.control(),
  cross = 10,
  maxit = 100000
)

summary(svm_cv_rbf)

# -------------------------------------------------------------------
# 2️⃣ Visualizar desempeño de todas las combinaciones (ggplot)
# -------------------------------------------------------------------

perf <- svm_cv_rbf$performances

ggplot(data = perf, aes(x = cost, y = error, color = as.factor(gamma))) +
  geom_line() +
  geom_point() +
  scale_x_continuous(trans = "log10") +  # log scale para cost
  labs(
    title = "Error de clasificación vs hiperparámetros C y gamma (SVM RBF)",
    x = "Cost (C, log scale)",
    y = "Error de clasificación",
    color = "Gamma"
  ) +
  theme_bw() +
  theme(legend.position = "bottom")


```
Resulta evidente a la vista del gráfico que los mejores parámetros son gamma=0.2 y coste=10.
Procedemos a generar y evaluar el modelo ganador
```{r}
best_cost <- 10
best_gamma <- 0.2

modelo_svm_rbf <- svm(
  Exited ~ ., 
  data = train, 
  kernel = "radial", 
  cost = best_cost,
  gamma = best_gamma,
  probability = TRUE
)

svm.pred_prob_matrix <- predict(modelo_svm_rbf, test, probability = TRUE)
svm.pred_prob <- attr(svm.pred_prob_matrix, "probabilities")[, "1"]

thresholds <- seq(0.01, 0.9, by = 0.01)

f1_scores <- sapply(thresholds, function(t){
  pred_class <- ifelse(svm.pred_prob >= t, "1", "0")
  cm <- confusionMatrix(
    factor(pred_class, levels = c("0","1")),
    factor(test$Exited, levels = c("0","1")),
    positive = "1"
  )
  cm$byClass["F1"]
})

best_threshold <- thresholds[which.max(f1_scores)]
best_threshold
```
```{r}
svm.pred_class <- ifelse(svm.pred_prob >= best_threshold, "1", "0")
cm <- confusionMatrix(
  factor(svm.pred_class, levels = c("0","1")),
  factor(test$Exited, levels = c("0","1")),
  positive = "1"
)

roc_obj <- roc(test$Exited, svm.pred_prob, quiet = TRUE)
auc_val <- auc(roc_obj)

kpi_table <- data.frame(
  KPI = c("Accuracy", "Precision", "Recall", "Specificity", "F1 Score", "AUC"),
  Valor = round(c(
    cm$overall["Accuracy"],
    cm$byClass["Precision"],
    cm$byClass["Recall"],
    cm$byClass["Specificity"],
    cm$byClass["F1"],
    auc_val
  ), 4)
)

print(kpi_table)
```
Accuracy: 0.8029
Precision 0.5409
Recall: 0.4607
Specificity: 0.8949
AUC:0.6798



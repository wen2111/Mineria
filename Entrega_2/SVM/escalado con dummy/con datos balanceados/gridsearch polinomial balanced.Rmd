---
title: "balanced data"
output: html_document
date: "2025-12-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include=FALSE}
library(recipes)
library(e1071)
library(mlbench)
library(ggplot2)
library(ISLR)
library(caret)
library(pROC)
library(dplyr)
library(smotefamily)
library(themis)
library(recipes)
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
bd<-data_reducida
bd$group<-NULL

```
# dummyficado

```{r}
dummies <- dummyVars(Exited ~ ., data = bd, fullRank = TRUE)

bd_dummy <- data.frame(predict(dummies, bd))
bd_dummy$Exited <- bd$Exited
```
# trainbase y testbase

```{r}
trainbase <- bd_dummy[1:7000, ]
testbase  <- bd_dummy[7001:10000, ]
```


# Balanceado con SMOTE

```{r}
trainbase$Exited <- as.factor(trainbase$Exited)
trainbase <- trainbase[, c(setdiff(names(trainbase), "Exited"), "Exited")]

recipe_balance <- recipe(Exited ~ ., data = trainbase) %>%
  step_smote(Exited, over_ratio = 0.6667)  # 40/60 = 0.6667

prep_recipe <- prep(recipe_balance, training = trainbase)
trainbase_smote <- bake(prep_recipe, new_data = NULL)

prop.table(table(trainbase_smote$Exited))
traibase<-trainbase_smote
```


```{r}
ind <- sample(1:nrow(trainbase), 0.7*nrow(trainbase))
train <- trainbase[ind,]
test <- trainbase[-ind,]
```

# Grid search polinomial

```{r}
svm_cv_poly <- tune(
  svm,
  Exited ~ .,
  data = train,
  kernel = "polynomial",
  ranges = list(
    cost  = c(5, 7, 10, 12),
    degree = c(2, 3, 4),
    coef0 = c(0, 1, 2)
  ),
  tunecontrol = tune.control(
    sampling = "cross",
    cross = 10,
    best.model = TRUE,
    performances = TRUE
  ),
  probability = TRUE,
  scale = TRUE,
  maxit = 100000
)

perf <- svm_cv_poly$performances

p1 <- ggplot(data = perf, aes(x = as.factor(degree), y = error, 
                              group = as.factor(cost), color = as.factor(cost))) +
  geom_line(size = 1.2) +
  geom_point(size = 4) +
  facet_wrap(~ coef0) +
  labs(
    title = "Error de clasificación en validación cruzada (10-fold)",
    subtitle = paste("Grid Search: cost =", 
                    paste(c(5, 7, 10, 12), collapse = ", "),
                    "degree =", paste(c(2, 3, 4), collapse = ", "),
                    "coef0 =", paste(c(0, 1, 2), collapse = ", ")),
    x = "Degree",
    y = "Error de clasificación",
    color = "Cost"
  ) +
  theme_bw(base_size = 12) +
  theme(
    legend.position = "bottom",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, color = "gray50"),
    panel.grid.minor = element_blank()
  ) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1))

print(p1)


best_cost <- 5 
best_degree <- 5

modelo_svm_poly <- svm(
  Exited ~ ., 
  data = train, 
  kernel = "polynomial", 
  cost = best_cost,
  degree = best_degree,
  coef0 = best_coef0,
  probability = TRUE,
  scale = TRUE
)

svm.pred_prob_matrix <- predict(modelo_svm_poly, test, probability = TRUE)
svm.pred_prob <- attr(svm.pred_prob_matrix, "probabilities")[, "1"]

thresholds <- seq(0.01, 0.99, by = 0.01)

threshold_results <- data.frame(
  threshold = thresholds,
  f1_score = NA,
  accuracy = NA,
  precision = NA,
  recall = NA
)

for(i in seq_along(thresholds)){
  t <- thresholds[i]
  pred_class <- ifelse(svm.pred_prob >= t, "1", "0")
  cm <- confusionMatrix(
    factor(pred_class, levels = c("0", "1")),
    factor(test$Exited, levels = c("0", "1")),
    positive = "1"
  )
  
  threshold_results$f1_score[i] <- cm$byClass["F1"]
  threshold_results$accuracy[i] <- cm$overall["Accuracy"]
  threshold_results$precision[i] <- cm$byClass["Precision"]
  threshold_results$recall[i] <- cm$byClass["Recall"]
}
best_threshold<-0.18
#best_threshold <- thresholds[which.max(threshold_results$f1_score)]
#best_f1 <- max(threshold_results$f1_score, na.rm = TRUE)


svm.pred_class <- ifelse(svm.pred_prob >= best_threshold, "1", "0")
cm <- confusionMatrix(
  factor(svm.pred_class, levels = c("0", "1")),
  factor(test$Exited, levels = c("0", "1")),
  positive = "1"
)

roc_obj <- roc(test$Exited, svm.pred_prob, quiet = FALSE)
auc_val <- auc(roc_obj)

# Gráfico ROC
p4 <- ggroc(roc_obj, color = "steelblue", size = 1.5) +
  geom_abline(intercept = 1, slope = 1, linetype = "dashed", color = "gray50") +
  geom_point(aes(x = 1 - cm$byClass["Specificity"], 
                y = cm$byClass["Recall"]),
            color = "red", size = 4) +
  annotate("text", x = 0.7, y = 0.3, 
           label = paste("AUC =", round(auc_val, 4)),
           size = 5, color = "darkred") +
  labs(
    title = "Curva ROC - Kernel Polinomial",
    subtitle = paste("Degree:", best_degree, "| Coef0:", best_coef0, "| Threshold usado:", round(best_threshold, 3)),
    x = "1 - Especificidad (False Positive Rate)",
    y = "Sensibilidad (True Positive Rate)"
  ) +
  theme_bw(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, color = "gray50")
  )

print(p4)

kpi_table <- data.frame(
  KPI = c("Kernel", "Degree", "Coef0", "Cost", "Threshold óptimo", "Accuracy", "Precision", "Recall", 
          "Specificity", "F1 Score", "AUC", "Balanced Accuracy"),
  Valor = round(c(
    NA,  # Placeholder para kernel
    best_degree,
    best_coef0,
    best_cost,
    best_threshold,
    cm$overall["Accuracy"],
    cm$byClass["Precision"],
    cm$byClass["Recall"],
    cm$byClass["Specificity"],
    cm$byClass["F1"],
    auc_val,
    cm$byClass["Balanced Accuracy"]
  ), 4),
  Descripción = c(
    "Tipo de kernel SVM",
    "Grado del polinomio",
    "Término independiente del polinomio",
    "Parámetro de penalización",
    "Threshold que maximiza el F1-score",
    "Proporción de predicciones correctas",
    "Proporción de verdaderos positivos entre los predichos positivos",
    "Proporción de verdaderos positivos detectados",
    "Proporción de verdaderos negativos detectados",
    "Media armónica de Precision y Recall",
    "Área bajo la curva ROC",
    "Media de Sensibilidad y Especificidad"
  )
)

kpi_table$Valor[1] <- "Polynomial"

print(kpi_table)
```
Kernel	Polynomial	
Degree	4	
Coef0	1	
Cost	5	
Threshold óptimo	0.21	
Accuracy	0.7962	
Precision	0.5113	
Recall	0.4151	
Specificity	0.896	
F1 Score	0.4582
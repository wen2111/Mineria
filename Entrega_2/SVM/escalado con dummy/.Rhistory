qnorm(6.976772499)
pnorm(6.876772499)
power.t.test(n=98.5, delta=6, sd=sqrt(317.81), sig.level=0.05)
pnorm(0.40290)
pnorm(5.05783)
alpha <- 0.05
delta <- 4
sigma2 <- 284.53
n <- 109.5
# Cálculo normal
Z_alpha <- qnorm(1 - alpha/2)
EE <- sqrt(2 * sigma2 / n)
Z_effect <- delta / EE
potencia_normal <- pnorm(Z_effect - Z_alpha)
potencia_normal
alpha <- 0.05
delta <- 4
sigma2 <- 284.53
n <- 109.5
# Cálculo normal
Z_alpha <- qnorm(1 - alpha/2)
EE <- sqrt(2 * sigma2 / n)
Z_effect <- delta / EE
potencia_normal <- pnorm(Z_effect - Z_alpha)
potencia_normal
setwd("~/Documents/2025-2026/asegurances/prac2/datosPrac2")
knitr::opts_chunk$set(echo = TRUE)
sever<-read_csv("severidad.csv")
library(knitr)
library(plyr)
library(MASS)
library(pscl)
library(gamlss)
library(survival)
library(VIM)
library(lubridate)
library(stringr)
library(psych)
library(MASS)
library(VGAM)
library(fitdistrplus)
library(actuar)
library(readr)
sever<-read_csv("severidad.csv")
summary(sever)
(fitGm <- fitdistr(sever$x, "gamma"))
library(readr)
severidad <- read_csv("severidad.csv")
View(severidad)
library(readr)
sever<-read_csv("severidad.csv",delimiter=",")
library(readr)
sever <- read.csv("severidad.csv", header = TRUE, sep = ",", row.names = NULL)
(fitGm <- fitdistr(sever$x, "gamma"))
setwd("~/Documents/2025-2026/asegurances/prac2/datosPrac2")
setwd("~/Documents/2025-2026/asegurances/prac2/datosPrac2")
setwd("~/Documents/2025-2026/asegurances/prac2/datosPrac2")
library(readxl)
# Antes hemos convertido el csv en excel
sever <- read_excel("severidad.xlsx")
x <- as.numeric(sever[[1]])
(fitGm <- fitdistr(x, "gamma"))
library(readxl)
# Antes hemos convertido el csv en excel
sever <- read_excel("severidad.xlsx")
X <- as.numeric(sever[[1]])
library(readxl)
# Antes hemos convertido el csv en excel
sever <- read_excel("severidad.xlsx")
X <- as.numeric(sever[[1]])
summary(X)
(fitGm <- fitdistr(X, "gamma",method = "mle"))
(fitGm <- fitdist(X, "gamma", method = "mle"))
library(readr)
claims <- read_csv("claims.csv")
summary(claims)
setwd("~/Documents/GitHub/Mineria/Entrega_2/SVM/escalado con dummy")
load("~/Documents/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
knitr::opts_chunk$set(echo = TRUE)
library(recipes)
library(e1071)
library(mlbench)
library(ggplot2)
library(ISLR)
library(caret)
library(pROC)
library(dplyr)
load("~/Documents/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
bd<-data_reducida
set.seed
rec <- recipe(Exited ~ ., data = bd) %>%
step_dummy(all_nominal_predictors(), -group, one_hot = TRUE, keep_original_cols = FALSE)%>%
step_center(all_numeric_predictors()) %>%
bd<- prep(rec) %>% bake(new_data = NULL)
knitr::opts_chunk$set(echo = TRUE)
library(recipes)
library(e1071)
library(mlbench)
library(ggplot2)
library(ISLR)
library(caret)
library(pROC)
library(dplyr)
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
set.seed
rec <- recipe(Exited ~ ., data = bd) %>%
step_dummy(all_nominal_predictors(), -group, one_hot = TRUE, keep_original_cols = FALSE)%>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors())
bd<- prep(rec) %>% bake(new_data = NULL)
library(recipes)
library(e1071)
library(mlbench)
library(ggplot2)
library(ISLR)
library(caret)
library(pROC)
library(dplyr)
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
set.seed(123)
rec <- recipe(Exited ~ ., data = bd) %>%
step_dummy(all_nominal_predictors(), -group, one_hot = TRUE, keep_original_cols = FALSE)
# Prepara y aplica a un NUEVO objeto
bd_procesado <- prep(rec) %>% bake(new_data = NULL)
load("~/Documents/GitHub/Mineria/DATA/famd/famd_data.Rdata")
famd
load("~/Documents/GitHub/Mineria/DATA/famd/famd_data.Rdata")
fd<-famd_features_red
load("~/Documents/GitHub/Mineria/DATA/famd/famd_data.Rdata")
fd<-famd_features_red
trainbase<-fd[fd$group=="train",]
summary(fd)
# Ver estructura de bd
str(bd)
# Ver nombres de columnas
names(bd)
# Ver primeras filas
head(bd)
# Ver si Exited es factor o numérico
class(bd$Exited)
table(bd$Exited)
knitr::opts_chunk$set(echo = TRUE)
library(recipes)
library(e1071)
library(caret)
library(pROC)
library(dplyr)
load("~/Documents/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
bd <- data_reducida
# MEJORA 1: Preprocesamiento más completo
set.seed(123)
rec <- recipe(Exited ~ ., data = bd) %>%
step_dummy(all_nominal_predictors(), -group, one_hot = TRUE, keep_original_cols = FALSE) %>%
step_center(all_numeric_predictors()) %>%     # Centrar
step_scale(all_numeric_predictors())          # Escalar (importante para SVM)
bd_procesado <- prep(rec) %>% bake(new_data = NULL)
bd <- bd_procesado
# Dividir la base de datos
trainbase <- bd[bd$group == "train",]
trainbase$group <- NULL
testbase <- bd[bd$group == "test",]
testbase$group <- NULL
set.seed(123)
ind <- sample(1:nrow(trainbase), 0.7*nrow(trainbase))
train <- trainbase[ind,]
test <- trainbase[-ind,]
# MEJORA 2: Balancear clases con class.weights
table(train$Exited)
peso_0 <- 1
peso_1 <- sum(train$Exited == "0") / sum(train$Exited == "1")  # Penalizar más la clase minoritaria
# MEJORA 3: Función mejorada con class.weights y ajuste de threshold óptimo
evaluar_kernel_mejorado <- function(
kernel_name,
cost_val = 10,
gamma_val = 0.1,
degree_val = 3,
coef0_val = 0.0,
train_data,
test_data,
class_weights = NULL
) {
svm.model <- svm(
Exited ~ .,
data = train_data,
cost = cost_val,
kernel = kernel_name,
gamma = gamma_val,
degree = degree_val,
coef0 = coef0_val,
class.weights = class_weights,  # Nuevo parámetro
probability = TRUE
)
svm.pred_prob_matrix <- predict(svm.model, test_data, probability = TRUE)
svm.pred_prob <- attr(svm.pred_prob_matrix, "probabilities")[, "1"]
# Encontrar threshold óptimo basado en curva ROC
roc_obj <- roc(test_data$Exited, svm.pred_prob, quiet = TRUE)
coords_roc <- coords(roc_obj, "best", best.method = "youden")
threshold_optimo <- coords_roc$threshold
# Predicción con threshold óptimo
svm.pred_class <- ifelse(svm.pred_prob >= threshold_optimo, "1", "0")
cm <- confusionMatrix(
as.factor(svm.pred_class),
as.factor(test_data$Exited),
positive = "1"
)
auc_val <- auc(roc_obj)
return(data.frame(
Kernel = kernel_name,
Threshold = round(threshold_optimo, 4),
Accuracy = cm$overall["Accuracy"],
Precision = cm$byClass["Precision"],
Recall = cm$byClass["Recall"],
Specificity = cm$byClass["Specificity"],
F1_Score = cm$byClass["F1"],
AUC = auc_val
))
}
# MEJORA 4: Grid Search para mejores hiperparámetros
kernels_a_probar <- c("linear", "polynomial", "radial", "sigmoid")
# Mejores valores según el kernel
config_kernels <- list(
linear = list(cost = c(0.1, 1, 10, 100)),
polynomial = list(cost = c(1, 10, 100), degree = c(2, 3, 4), coef0 = c(0, 1)),
radial = list(cost = c(1, 10, 100), gamma = c(0.01, 0.1, 1)),
sigmoid = list(cost = c(1, 10, 100), gamma = c(0.01, 0.1), coef0 = c(0, 1))
)
# Probar polynomial con diferentes configuraciones (el mejor kernel)
resultados_poly <- expand.grid(
cost = c(1, 10, 50, 100),
degree = c(2, 3, 4),
coef0 = c(0, 0.5, 1)
) %>%
rowwise() %>%
do({
evaluar_kernel_mejorado(
kernel_name = "polynomial",
cost_val = .$cost,
gamma_val = 0.1,
degree_val = .$degree,
coef0_val = .$coef0,
train_data = train,
test_data = test,
class_weights = c("0" = peso_0, "1" = peso_1)
) %>%
mutate(Cost = .$cost, Degree = .$degree, Coef0 = .$coef0)
}) %>%
ungroup() %>%
arrange(desc(F1_Score))

---
title: "sigmoide grid search"
output: html_document
date: "2025-12-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include=FALSE}
library(recipes)
library(e1071)
library(mlbench)
library(ggplot2)
library(ISLR)
library(caret)
library(pROC)
library(dplyr)
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
bd<-data_reducida

```
# Escalado y dummyficado

```{r}
set.seed(1234)
rec <- recipe(Exited ~ ., data = bd) %>%
  step_dummy(all_nominal_predictors(), -group, one_hot = TRUE, keep_original_cols = FALSE)%>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())
bd<- prep(rec) %>% bake(new_data = NULL)


```

# Train y test

```{r}
trainbase<-bd[bd$group=="train",]
trainbase$group<-NULL
testbase<-bd[bd$group=="test",]
testbase$group<-NULL
ind <- sample(1:nrow(trainbase), 0.7*nrow(trainbase))
train <- trainbase[ind,]
test <- trainbase[-ind,]
```


```{r}
# Grid search para SVM con kernel sigmoide
svm_cv_sigmoid <- tune(
  svm,
  Exited ~ .,
  data = train,
  kernel = "sigmoid",
  ranges = list(
    cost  = c(0.1, 0.5, 1),
gamma = c(0.001, 0.005, 0.01)
  ),
  tunecontrol = tune.control(),
  cross = 10,
  maxit = 100000
)

# Guardamos las performances
perf_sigmoid <- svm_cv_sigmoid$performances

# Visualización
library(ggplot2)
ggplot(data = perf_sigmoid, aes(x = gamma, y = error, color = as.factor(cost))) +
  geom_line() +
  geom_point(size = 3) +
  scale_x_continuous(trans = "log10") +
  labs(
    title = "Error de clasificación vs gamma y cost (SVM Sigmoid)",
    x = "Gamma (log scale)",
    y = "Error de clasificación",
    color = "Cost"
  ) +
  theme_bw() +
  theme(legend.position = "bottom")

```
```{r}
best_cost <- 1
best_gamma <- 0.01

modelo_svm_sigmoid <- svm(
  Exited ~ ., 
  data = train, 
  kernel = "sigmoid", 
  cost = best_cost,
  gamma = best_gamma,
  probability = TRUE
)

svm.pred_prob_matrix <- predict(modelo_svm_sigmoid, test, probability = TRUE)
svm.pred_prob <- attr(svm.pred_prob_matrix, "probabilities")[, "1"]

thresholds <- seq(0.01, 0.9, by = 0.01)

f1_scores <- sapply(thresholds, function(t){
  pred_class <- ifelse(svm.pred_prob >= t, "1", "0")
  cm <- confusionMatrix(
    factor(pred_class, levels = c("0","1")),
    factor(test$Exited, levels = c("0","1")),
    positive = "1"
  )
  cm$byClass["F1"]
})

best_threshold <- thresholds[which.max(f1_scores)]
best_threshold

svm.pred_class <- ifelse(svm.pred_prob >= best_threshold, "1", "0")

cm <- confusionMatrix(
  factor(svm.pred_class, levels = c("0","1")),
  factor(test$Exited, levels = c("0","1")),
  positive = "1"
)

roc_obj <- roc(test$Exited, svm.pred_prob, quiet = TRUE)
auc_val <- auc(roc_obj)

kpi_table <- data.frame(
  KPI = c("Accuracy", "Precision", "Recall", "Specificity", "F1 Score", "AUC"),
  Valor = round(c(
    cm$overall["Accuracy"],
    cm$byClass["Precision"],
    cm$byClass["Recall"],
    cm$byClass["Specificity"],
    cm$byClass["F1"],
    auc_val
  ), 4)
)

print(kpi_table)
```

Accuracy: 0.3395
F1: 0.3576
Recall: 0.8853
---
title: "glm_unbalanced_cloglog"
output: html_document
date: "2025-12-08"
---

```{r}
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
datatrain <- data_reducida_plus[1:7000, !(names(data_reducida_plus) %in% "group")]
datatest<-data_reducida_plus[7001:10000,!(names(data_reducida_plus) %in% "group")]
```

```{r}
library(caret)

set.seed(123)

# Convertimos la variable de salida a factor
datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))

# Control de cross-validation
train_ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = TRUE
)

# Entrenamos el modelo logístico con enlace cloglog y CV
fit_cv <- train(
  Exited ~ .,
  data = datatrain,
  method = "glm",
  family = binomial(link = "cloglog"), 
  trControl = train_ctrl,
  metric = "ROC"
)

# Obtenemos las probabilidades out-of-fold
prob_cv <- fit_cv$pred$pos
y_cv    <- fit_cv$pred$obs

```
```{r}
performance_metrics <- function(y_true, prob, cutoff=0.5, dec=3){
  
  # Convertim y_true a neg/pos
  y_true <- factor(y_true, levels=c(0,1), labels=c("neg","pos"))
  
  # Predicció com a factor amb nivells fixos
  pred <- factor(ifelse(prob > cutoff, "pos", "neg"),
                 levels=c("neg","pos"))
  
  # Calculem la taula manualment garantint files/columnes
  cm <- matrix(0, nrow=2, ncol=2,
               dimnames=list(Predicted=c("neg","pos"),
                             Actual=c("neg","pos")))
  
  tab <- table(Predicted=pred, Actual=y_true)
  
  cm[rownames(tab), colnames(tab)] <- tab
  
  TN <- cm["neg","neg"]
  FP <- cm["pos","neg"]
  FN <- cm["neg","pos"]
  TP <- cm["pos","pos"]
  
  Sens <- TP/(TP+FN)
  Spec <- TN/(TN+FP)
  PPV  <- TP/(TP+FP)
  NPV  <- TN/(TN+FN)
  PLR  <- PPV/(1-NPV)
  NLR  <- NPV/(1-PPV)
  Acc  <- (TP+TN) / sum(cm)
  
  list(
    Sensitivity = round(Sens, dec),
    Specificity = round(Spec, dec),
    PPV = round(PPV, dec),
    NPV = round(NPV, dec),
    PLR = round(PLR, dec),
    NLR = round(NLR, dec),
    Accuracy = round(Acc, dec),
    ConfusionMatrix = cm
  )
}
prob_alineada <- numeric(nrow(datatrain))

prob_alineada[fit_cv$pred$rowIndex] <- fit_cv$pred$pos

#tornem a transformar a numèrica la variable sortida
y_numeric <- ifelse(datatrain$Exited == "pos", 1, 0)
cutpoints <- seq(0.1, 0.5, by=0.01)
results   <- data.frame()

results <- data.frame(
  Cutpoint    = numeric(),
  Sensitivity = numeric(),
  Specificity = numeric(),
  PPV         = numeric(),
  NPV         = numeric(),
  PLR         = numeric(),
  NLR         = numeric(),
  Accuracy    = numeric(),
  F1          = numeric()
)


f1_score_cm <- function(cm){
  TP <- cm["pos","pos"]
  FP <- cm["pos","neg"]
  FN <- cm["neg","pos"]
  
  precision <- TP / (TP + FP)
  recall    <- TP / (TP + FN)
  
  f1 <- 2 * (precision * recall) / (precision + recall)
  return(round(f1, 3))
}


for(c in cutpoints){
  m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
  
  results <- rbind(results, data.frame(
    Cutpoint    = c, 
    Sensitivity = m$Sensitivity,
    Specificity = m$Specificity,
    PPV         = m$PPV,
    NPV         = m$NPV, 
    PLR         = m$PLR, 
    NLR         = m$NLR,
    Accuracy    = m$Accuracy,
    F1          = f1_score_cm(m$ConfusionMatrix)
  ))
}

results
```


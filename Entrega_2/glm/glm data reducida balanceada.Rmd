---
title: "Untitled"
output:
  pdf_document: default
  html_document: default
date: "2025-11-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

En este script se realiza un modelo de clasificación binario con caret (cv=10) Y CON loocv.

Se utiliza la base de datos desbalanceada y se juega con el cutoff para hacer frente al desbalanceo.

# Caret CV=10

Preparación de los datos
```{r, echo=FALSE}
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
datatrain <- data_reducida_plus[1:7000, !(names(data_reducida_plus) %in% "group")]
datatest<-data_reducida_plus[7001:10000,!(names(data_reducida_plus) %in% "group")]
datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
datatrain$NumOfProducts_grupo <- as.character(datatrain$NumOfProducts_grupo) 
datatrain$NumOfProducts_grupo[datatrain$NumOfProducts_grupo == "3 o más"] <- "3_plus" 
datatrain$NumOfProducts_grupo <- factor(datatrain$NumOfProducts_grupo)      
```

Trabajaremos a partir de datatrain, donde tenemos la respuesta Exited para todas las filas

La idea es probar varios niveles de balanceo y de cutoff para encontrar el óptimo para ambos parámetros.
```{r, include=FALSE}
library(caret)
library(ROSE)
library(pROC)

set.seed(123)

rose_ratios <- seq(0.25, 0.50, by = 0.05)
compute_kpis <- function(obs, pred_class, prob_pos) {
  
  cm <- confusionMatrix(pred_class, obs, positive = "pos")
  
  auc <- roc(obs, prob_pos)$auc
  
  data.frame(
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    PPV         = cm$byClass["Pos Pred Value"],
    NPV         = cm$byClass["Neg Pred Value"],
    Accuracy    = cm$overall["Accuracy"],
    F1          = cm$byClass["F1"],
    AUC         = auc
  )
}
results_all <- data.frame()
cutpoints <- seq(0.1, 0.9, by = 0.05)

for (p in rose_ratios) {
  
  train_ctrl <- trainControl(
    method = "cv",
    number = 10,
    sampling = function(x, y) {
      data_temp <- data.frame(x, Exited = y)
      data_rose <- ROSE(Exited ~ ., data = data_temp, p = p, seed = 1)$data
      list(
        x = data_rose[, -which(names(data_rose)=="Exited")],
        y = data_rose$Exited
      )
    },
    classProbs = TRUE,
    summaryFunction = twoClassSummary,
    savePredictions = "final"
  )
  
  # Entrenar model
  fit <- train(
    Exited ~ .,
    data = datatrain,
    method = "glm",
    family = binomial,
    trControl = train_ctrl,
    metric = "ROC"
  )
  
  
  prob_train <- fit$pred$pos
  obs_train  <- fit$pred$obs
  

  prob_test  <- predict(fit, datatrain, type="prob")[, "pos"]
  obs_test   <- datatrain$Exited
  

  for (cut in cutpoints) {
    
    # TRAIN
    pred_train_cut <- factor(ifelse(prob_train > cut, "pos", "neg"),
                             levels = c("neg","pos"))
    kpi_train <- compute_kpis(obs_train, pred_train_cut, prob_train)
    
    # TEST
    pred_test_cut <- factor(ifelse(prob_test > cut, "pos", "neg"),
                            levels = c("neg","pos"))
    kpi_test <- compute_kpis(obs_test, pred_test_cut, prob_test)
    
    
    results_all <- rbind(results_all, data.frame(
  ROSE_p = p,
  Cutoff = cut,
  

  F1_Train   = kpi_train$F1,
  F1_Test    = kpi_test$F1,
  Sens_Train = kpi_train$Sensitivity,
  Sens_Test  = kpi_test$Sensitivity,
  Spec_Train = kpi_train$Specificity,
  Spec_Test  = kpi_test$Specificity,
  
  

  PPV_Train  = kpi_train$PPV,
  PPV_Test   = kpi_test$PPV,
  NPV_Train  = kpi_train$NPV,
  NPV_Test   = kpi_test$NPV,
  ACC_Train  = kpi_train$Accuracy,
  ACC_Test   = kpi_test$Accuracy,
  AUC_Train  = kpi_train$AUC,
  AUC_Test   = kpi_test$AUC
))
  }
}
```
```{r}
results_all
```
ROSE 25, cutoff 25
ROSE 0.35	cutoff 0.35	
ROSE 0.40	cutoff 0.40

# GLM link probit

```{r, include=FALSE}
results_all <- data.frame()
cutpoints <- seq(0.1, 0.9, by = 0.05)

for (p in rose_ratios) {
  
  train_ctrl <- trainControl(
    method = "cv",
    number = 10,
    sampling = function(x, y) {
      data_temp <- data.frame(x, Exited = y)
      data_rose <- ROSE(Exited ~ ., data = data_temp, p = p, seed = 1)$data
      list(
        x = data_rose[, -which(names(data_rose)=="Exited")],
        y = data_rose$Exited
      )
    },
    classProbs = TRUE,
    summaryFunction = twoClassSummary,
    savePredictions = "final"
  )
  
  # Entrenar model
  fit <- train(
    Exited ~ .,
    data = datatrain,
    method = "glm",
    family = binomial,
    trControl = train_ctrl,
    metric = "ROC"
  )
  
  
  prob_train <- fit$pred$pos
  obs_train  <- fit$pred$obs
  

  prob_test  <- predict(fit, datatrain, type="prob")[, "pos"]
  obs_test   <- datatrain$Exited
  

  for (cut in cutpoints) {
    
    # TRAIN
    pred_train_cut <- factor(ifelse(prob_train > cut, "pos", "neg"),
                             levels = c("neg","pos"))
    kpi_train <- compute_kpis(obs_train, pred_train_cut, prob_train)
    
    # TEST
    pred_test_cut <- factor(ifelse(prob_test > cut, "pos", "neg"),
                            levels = c("neg","pos"))
    kpi_test <- compute_kpis(obs_test, pred_test_cut, prob_test)
    
    
    results_all <- rbind(results_all, data.frame(
  ROSE_p = p,
  Cutoff = cut,
  

  F1_Train   = kpi_train$F1,
  F1_Test    = kpi_test$F1,
  Sens_Train = kpi_train$Sensitivity,
  Sens_Test  = kpi_test$Sensitivity,
  Spec_Train = kpi_train$Specificity,
  Spec_Test  = kpi_test$Specificity,
  
  

  PPV_Train  = kpi_train$PPV,
  PPV_Test   = kpi_test$PPV,
  NPV_Train  = kpi_train$NPV,
  NPV_Test   = kpi_test$NPV,
  ACC_Train  = kpi_train$Accuracy,
  ACC_Test   = kpi_test$Accuracy,
  AUC_Train  = kpi_train$AUC,
  AUC_Test   = kpi_test$AUC
))
  }
}
```

```{r}
results_all
```

Parece que los resultados no varían mucho cambiando la función link. Logit y Probit dan resultados similares.

#LOOCV (leave One Out Cross Validation)

```{r, include=FALSE}
library(caret)
library(pROC)
library(ROSE)

rose_ratios <- seq(0.25, 0.5, by = 0.05)


cutpoints <- seq(0.1, 0.9, by = 0.05)


results_all <- data.frame()

n <- nrow(datatrain)

for (p in rose_ratios) {
  
  cat("======= PROVANT BALANCEIG p =", p, "=======\n")
  
  # Probabilitats out-of-fold inicialitzades
  prob_loocv <- numeric(n)
  
  # Loop LOOCV
  for (i in 1:n) {
    
    # Train = totes les files menys la i
    train_idx <- setdiff(1:n, i)
    test_idx  <- i
    
    train_data <- datatrain[train_idx, ]
    test_data  <- datatrain[test_idx, ]
    
    # Aplicar ROSE al training set
    train_bal <- ROSE(Exited ~ ., data = train_data, p = p, seed = 123)$data
    
    # Entrenar GLM
    fit <- glm(Exited ~ ., data = train_bal, family = binomial)
    
    # Predicció per la fila deixada fora
    prob_loocv[i] <- predict(fit, newdata = test_data, type = "response")
    
  }
  
  # Un cop tenim probabilitats out-of-fold, calculem KPI per cada cutoff
  for (c in cutpoints) {
    
    pred_class <- factor(ifelse(prob_loocv > c, "pos", "neg"),
                         levels = c("neg","pos"))
    
    kpi <- compute_kpis(datatrain$Exited, pred_class, prob_loocv)
    
    results_all <- rbind(results_all, data.frame(
      ROSE_p      = p,
      Cutpoint    = c,
      F1_Train    = kpi$F1,
      F1_Test     = kpi$F1,          # LOOCV només té out-of-fold
      Spec_Train  = kpi$Specificity,
      Spec_Test   = kpi$Specificity,
      Sens_Train  = kpi$Sensitivity,
      Sens_Test   = kpi$Sensitivity,
      PPV         = kpi$PPV,
      NPV         = kpi$NPV,
      Accuracy    = kpi$Accuracy,
      AUC         = kpi$AUC
    ))
  }
}

```

```{r}
results_all
```

# Bootsrap


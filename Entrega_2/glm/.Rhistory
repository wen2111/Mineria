<<<<<<< HEAD
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.15, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.15, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
fit_rf <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "ROC",          # optimizar AUC
trControl = ctrl_rf,
tuneGrid = tuneGrid,
ntree = 500,        # número de árboles
nodesize=80, maxnodes=100
)
plot(fit_rf)
fit_rf
pred <- predict(fit_rf, newdata = test2,type = 'raw')
ptrain <- predict(fit_rf, newdata=train2, type = 'raw')
confusionMatrix(ptrain, train2$Exited, positive="Yes")
confusionMatrix(pred, test2$Exited, positive="Yes")
pred <- predict(fit_rf, newdata = test2,type = 'prob')
pred <- pred[,2]
r <- multiclass.roc(test2$Exited, pred, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,print.auc=TRUE,
auc.polygon=TRUE,
grid=c(0.1, 0.2),
grid.col=c("green", "red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.15, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.15, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
fit_rf <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "ROC",          # optimizar AUC
trControl = ctrl_rf,
tuneGrid = tuneGrid,
ntree = 500,        # número de árboles
nodesize=60, maxnodes=80
)
plot(fit_rf)
fit_rf
pred <- predict(fit_rf, newdata = test2,type = 'raw')
ptrain <- predict(fit_rf, newdata=train2, type = 'raw')
confusionMatrix(ptrain, train2$Exited, positive="Yes")
confusionMatrix(pred, test2$Exited, positive="Yes")
pred <- predict(fit_rf, newdata = test2,type = 'prob')
pred <- pred[,2]
r <- multiclass.roc(test2$Exited, pred, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,print.auc=TRUE,
auc.polygon=TRUE,
grid=c(0.1, 0.2),
grid.col=c("green", "red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.15, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.15, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.1, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.1, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
fit_rf <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "ROC",          # optimizar AUC
trControl = ctrl_rf,
tuneGrid = tuneGrid,
ntree = 500,        # número de árboles
nodesize=40, maxnodes=60
)
plot(fit_rf)
fit_rf
pred <- predict(fit_rf, newdata = test2,type = 'raw')
ptrain <- predict(fit_rf, newdata=train2, type = 'raw')
confusionMatrix(ptrain, train2$Exited, positive="Yes")
confusionMatrix(pred, test2$Exited, positive="Yes")
pred <- predict(fit_rf, newdata = test2,type = 'prob')
pred <- pred[,2]
r <- multiclass.roc(test2$Exited, pred, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,print.auc=TRUE,
auc.polygon=TRUE,
grid=c(0.1, 0.2),
grid.col=c("green", "red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.1, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.1, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.2, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.2, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
fit_rf <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "ROC",          # optimizar AUC
trControl = ctrl_rf,
tuneGrid = tuneGrid,
ntree = 500,        # número de árboles
nodesize=30, maxnodes=50
)
plot(fit_rf)
fit_rf
pred <- predict(fit_rf, newdata = test2,type = 'raw')
ptrain <- predict(fit_rf, newdata=train2, type = 'raw')
confusionMatrix(ptrain, train2$Exited, positive="Yes")
confusionMatrix(pred, test2$Exited, positive="Yes")
pred <- predict(fit_rf, newdata = test2,type = 'prob')
pred <- pred[,2]
r <- multiclass.roc(test2$Exited, pred, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,print.auc=TRUE,
auc.polygon=TRUE,
grid=c(0.1, 0.2),
grid.col=c("green", "red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.2, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.2, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.15, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.15, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
mydata <- data_transformada
mydata$group<-NULL
mydata$Surname<-NULL
mydata$ID<-NULL
# SEPARAR TRAIN Y TEST
train <- mydata[1:7000,]
test <- mydata[7001:10000,]  # 3000 obs
# LABELS PARA EXITED
train$Exited <- factor(train$Exited,
levels = c("0","1"),
labels = c("No","Yes"))
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
rf <- randomForest(Exited ~ ., data = train2,)
varImpPlot(rf)
library(caret)
library(ROSE)
library(smotefamily)
data <- data_reducida
# SEPARAR TRAIN Y TEST
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
# ELIMINAR "GROUP"
train$group <- NULL
test$group  <- NULL
# LABELS PARA EXITED
train$Exited <- factor(train$Exited,
levels = c("0","1"),
labels = c("No","Yes"))
table(train$Exited)
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index,]     # Entrenamiento interno
test2  <- train[-index, ]    # Validación interna
#str(data_imputado)
# BALANCEO SOLO EN TRAIN2
train2_balanceada <- ROSE(
Exited ~ .,
data = train2,
p = 0.4,      # 40% Yes, 60% No
seed = 123
)$data
# COMPROBAR BALANCEO
(table(train2$Exited))                     # Antes del balanceo
(table(train2_balanceada$Exited))  # Después del balanceo
View(train2_balanceada)
# Logit
modelo_logit <- glm(Exited ~ ., data = train2, family = binomial(link = "logit"))
# Probit
modelo_probit <- glm(Exited ~ ., data = train2, family = binomial(link = "probit"))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2, type = "response")
prop.table(table(train2_balanceada$Exited))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2, type = "response")
class_logit_train2 <- ifelse(pred_logit_train2 > 0.2, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.2, 1, 0)
(conf_logit_train2 <- confusionMatrix(as.factor(class_logit_train2), train2$Exited,positive = "1"))
pred_logit_train2
table(class_logit_train2)
table(train2_balanceada)
# Logit
modelo_logit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "logit"))
# Probit
modelo_probit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "probit"))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2_balanceada, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2_balanceada, type = "response")
class_logit_train2 <- ifelse(pred_logit_train2 > 0.2, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.2, 1, 0)
(conf_logit_train2 <- confusionMatrix(as.factor(class_logit_train2), train2_balanceada$Exited,positive = "1"))
class_logit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
(conf_logit_train2 <- confusionMatrix(as.factor(class_logit_train2), train2_balanceada$Exited,positive = "1"))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "1"))
table(class_logit_train2)
table(train2_balanceada$Exited)
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
(conf_probit_train2 <- confusionMatrix(class_probit_train2, train2_balanceada$Exited,positive = "Yes"))
class_probit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
(conf_probit_train2 <- confusionMatrix(class_probit_train2, train2_balanceada$Exited,positive = "Yes"))
# Logit
modelo_logit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "logit"))
# Probit
modelo_probit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "probit"))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2_balanceada, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2_balanceada, type = "response")
class_logit_train2 <- ifelse(pred_logit_train2 > 0.2, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.2, 1, 0)
class_logit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
class_probit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
# Logit
modelo_logit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "logit"))
# Probit
modelo_probit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "probit"))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2_balanceada, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2_balanceada, type = "response")
class_logit_train2 <- ifelse(pred_logit_train2 > 0.2, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.2, 1, 0)
class_logit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
class_probit_train2 <- factor(class_probit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
(conf_probit_train2 <- confusionMatrix(class_probit_train2, train2_balanceada$Exited,positive = "Yes"))
# Función para calcular F1 desde confusionMatrix
calcular_f1 <- function(conf){
precision <- conf$byClass["Precision"]
recall    <- conf$byClass["Recall"]
f1        <- 2 * (precision * recall) / (precision + recall)
return(f1)
}
(f1_logit_train2 <- calcular_f1(conf_logit_train2))
(f1_probit_train2 <- calcular_f1(conf_probit_train2))
# Predicciones de TEST2
pred_logit_test2  <- predict(modelo_logit,  newdata = test2, type = "response")
pred_probit_test2 <- predict(modelo_probit, newdata = test2, type = "response")
mejor_umbral_f1 <- function(prob, real){
real <- as.factor(real)
umbrales <- seq(0.11, 1, by = 0.01)
f1_scores <- sapply(umbrales, function(t){
pred <- ifelse(prob > t, "Yes", "No")
pred <- factor(pred, levels = levels(real))
F1 <- caret::F_meas(pred, real)
return(F1)
})
# Mejor umbral y mejor F1
umbral_opt <- umbrales[which.max(f1_scores)]
f1_opt     <- max(f1_scores)
return(list(
umbral = umbral_opt,
F1     = f1_opt,
tabla  = data.frame(umbral = umbrales, f1 = f1_scores)
))
}
res_probit <- mejor_umbral_f1(pred_probit_test2, test2$Exited)
res_probit$umbral
res_probit$F1
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.67, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.67, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
a<-confusionMatrix(as.factor(class_probit_test2), test2$Exited,positive = "1")
a<-confusionMatrix(class_probit_test2, test2$Exited,positive = "Yes")
# Función para calcular F1 desde confusionMatrix
calcular_f1 <- function(conf){
precision <- conf$byClass["Precision"]
recall    <- conf$byClass["Recall"]
f1        <- 2 * (precision * recall) / (precision + recall)
return(f1)
}
(f1_logit_train2 <- calcular_f1(conf_logit_train2))
(f1_probit_train2 <- calcular_f1(conf_probit_train2))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
(conf_probit_train2 <- confusionMatrix(class_probit_train2, train2_balanceada$Exited,positive = "Yes"))
# Predicciones de TEST2
pred_logit_test2  <- predict(modelo_logit,  newdata = test2, type = "response")
pred_probit_test2 <- predict(modelo_probit, newdata = test2, type = "response")
mejor_umbral_f1 <- function(prob, real){
real <- as.factor(real)
umbrales <- seq(0.01, 1, by = 0.01)
f1_scores <- sapply(umbrales, function(t){
pred <- ifelse(prob > t, "Yes", "No")
pred <- factor(pred, levels = levels(real))
F1 <- caret::F_meas(pred, real)
return(F1)
})
# Mejor umbral y mejor F1
umbral_opt <- umbrales[which.max(f1_scores)]
f1_opt     <- max(f1_scores)
return(list(
umbral = umbral_opt,
F1     = f1_opt,
tabla  = data.frame(umbral = umbrales, f1 = f1_scores)
))
}
res_probit <- mejor_umbral_f1(pred_probit_test2, test2$Exited)
res_probit$umbral
res_probit$F1
# Predicciones de TEST2
pred_logit_test2  <- predict(modelo_logit,  newdata = test2, type = "response")
pred_probit_test2 <- predict(modelo_probit, newdata = test2, type = "response")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.2, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.2, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
z
f1_probit_test2 <- calcular_f1(z)
cat("F1 Probit - test2:", f1_probit_test2, "\n")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.5, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.5, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
# --- F1 de test2
f1_logit_test2  <- calcular_f1(z)
cat("F1 Logit - test2:", f1_logit_test2, "\n")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.7, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.7, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.5, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
# --- F1 de test2
f1_logit_test2  <- calcular_f1(z)
cat("F1 Logit - test2:", f1_logit_test2, "\n")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.4, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.4, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
a<-confusionMatrix(class_probit_test2, test2$Exited,positive = "Yes")
# --- F1 de test2
f1_logit_test2  <- calcular_f1(z)
f1_probit_test2 <- calcular_f1(a)
cat("F1 Logit - test2:", f1_logit_test2, "\n")
z
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
class_logit_train2 <- ifelse(pred_logit_train2 > 0.5, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.5, 1, 0)
class_logit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
class_probit_train2 <- factor(class_probit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
=======
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
results
results
library(caret)
set.seed(123)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
#  Convertimos Exited a factor neg/pos
datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
#  Control LOOCV
ctrl_loocv <- trainControl(
method = "LOOCV",
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE   # importante para obtener predicciones out-of-fold
)
# Entrenamos modelo logístico con LOOCV
fit_loocv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = "binomial",
trControl = ctrl_loocv,
metric = "ROC"
)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
library(caret)
set.seed(123)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
#  Convertimos Exited a factor neg/pos
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
#  Control LOOCV
ctrl_loocv <- trainControl(
method = "LOOCV",
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE   # importante para obtener predicciones out-of-fold
)
# Entrenamos modelo logístico con LOOCV
fit_loocv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = "binomial",
trControl = ctrl_loocv,
metric = "ROC"
)
#  Probabilidades out-of-fold en el orden original
prob_loocv <- numeric(nrow(datatrain))
prob_loocv[fit_loocv$pred$rowIndex] <- fit_loocv$pred$pos
#  Convertimos Exited a 0/1 para métricas
y_numeric <- ifelse(datatrain$Exited == "pos", 1, 0)
cutpoints <- seq(0.1, 0.9, by=0.05)
results   <- data.frame()
results <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
results <- rbind(results, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
results
results
results
y_numeric
prob_alineada[fit_cv$pred$rowIndex]
cutpoints <- seq(0.1, 0.9, by=0.05)
results   <- data.frame()
resultsCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
results <- rbind(results, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsCV
performance_metrics <- function(y_true, prob, cutoff=0.5, dec=3){
# Convertim y_true a neg/pos
y_true <- factor(y_true, levels=c(0,1), labels=c("neg","pos"))
# Predicció com a factor amb nivells fixos
pred <- factor(ifelse(prob > cutoff, "pos", "neg"),
levels=c("neg","pos"))
# Calculem la taula manualment garantint files/columnes
cm <- matrix(0, nrow=2, ncol=2,
dimnames=list(Predicted=c("neg","pos"),
Actual=c("neg","pos")))
tab <- table(Predicted=pred, Actual=y_true)
cm[rownames(tab), colnames(tab)] <- tab
TN <- cm["neg","neg"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
TP <- cm["pos","pos"]
Sens <- TP/(TP+FN)
Spec <- TN/(TN+FP)
PPV  <- TP/(TP+FP)
NPV  <- TN/(TN+FN)
PLR  <- PPV/(1-NPV)
NLR  <- NPV/(1-PPV)
Acc  <- (TP+TN) / sum(cm)
list(
Sensitivity = round(Sens, dec),
Specificity = round(Spec, dec),
PPV = round(PPV, dec),
NPV = round(NPV, dec),
PLR = round(PLR, dec),
NLR = round(NLR, dec),
Accuracy = round(Acc, dec),
ConfusionMatrix = cm
)
}
#Per tenir les probabilitats en l'ordre de la variable sortida:
prob_alineada <- numeric(nrow(datatrain))
prob_alineada[fit_cv$pred$rowIndex] <- fit_cv$pred$pos
#tornem a transformar a numèrica la variable sortida
y_numeric <- ifelse(datatrain$Exited == "pos", 1, 0)
cutpoints <- seq(0.1, 0.9, by=0.05)
results   <- data.frame()
resultsCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
results <- rbind(results, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsCV
cutpoints <- seq(0.1, 0.9, by=0.05)
results   <- data.frame()
results <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
results <- rbind(results, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
results
cutpoints <- seq(0.1, 0.9, by=0.05)
resultsCV   <- data.frame()
resultsCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
resultsCV <- rbind(resultsCV, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsCV
library(caret)
set.seed(123)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
#  Convertimos Exited a factor neg/pos
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
#  Control LOOCV
ctrl_loocv <- trainControl(
method = "LOOCV",
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE   # importante para obtener predicciones out-of-fold
)
# Entrenamos modelo logístico con LOOCV
fit_loocv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = "binomial",
trControl = ctrl_loocv,
metric = "ROC"
)
#  Probabilidades out-of-fold en el orden original
prob_loocv <- numeric(nrow(datatrain))
prob_loocv[fit_loocv$pred$rowIndex] <- fit_loocv$pred$pos
#  Convertimos Exited a 0/1 para métricas
y_numeric <- ifelse(datatrain$Exited == "pos", 1, 0)
cutpoints <- seq(0.1, 0.9, by=0.05)
resultsLOOCV   <- data.frame()
resultsLOOCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
resultsLOOCV <- rbind(resultsLOOCV, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsLOOCV
save.image("data_reducida_balanceada_CV_LOOCV.RData")
cutpoints <- seq(0.1, 0.9, by=0.05)
resultsCV   <- data.frame()
resultsCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
resultsCV <- rbind(resultsCV, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsCV
knitr::opts_chunk$set(echo = TRUE)
# load("~/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
library(caret)
set.seed(123)
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
# Control de cross-validation
train_ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE
)
# Entrenem el model logístic amb CV
fit_cv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = binomial,
trControl = train_ctrl,
metric = "ROC"
)
# load("~/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
library(caret)
set.seed(123)
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
# Control de cross-validation
train_ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE
)
# Entrenem el model logístic amb CV
fit_cv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = binomial,
trControl = train_ctrl,
metric = "ROC"
)
table(data_reducida_balanceada$Exited)
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/Entrega_2/glm/data_reducida_balanceada_CV_LOOCV.RData")
# load("~/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
library(caret)
set.seed(123)
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
# Control de cross-validation
train_ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE
)
# Entrenem el model logístic amb CV
fit_cv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = binomial,
trControl = train_ctrl,
metric = "ROC"
)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/reducida_balanceada.RData")
# load("~/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
table(data_reducida_balanceada$Exited)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
table(datatrain$Exited$Exited)
table(datatrain$Exited)
data_reducida_balanceada[7001:10000]
dim(data_reducida_balanceada)
view(data_reducida_balanceada)
View(data_reducida_balanceada)
# 1. 读数据
#load(".../reducida_balanceada.RData")   # 你原来的路径
# 2. 把 Exited 变成 neg/pos（Yes = 正类）
data_reducida_balanceada$Exited <- factor(
data_reducida_balanceada$Exited,
levels = c("No", "Yes"),
labels = c("neg", "pos")
)
# 3. 这里就直接用整张表做训练数据，不再切 1:7000 / 7001:10000
datatrain <- data_reducida_balanceada   # 如果没有 group 列，就直接这样
# 如果还有 group 列，就：
# datatrain <- data_reducida_balanceada[ , !(names(data_reducida_balanceada) %in% "group")]
# 4.（可选）检查是否有 NA
colSums(is.na(datatrain))
# 5. CV 控制
set.seed(123)
train_ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE
)
# 6. 训练 GLM（现在这里就能跑了）
fit_cv <- train(
Exited ~ .,
data   = datatrain,
method = "glm",
family = binomial,
trControl = train_ctrl,
metric = "ROC"
)
unique(data_reducida_balanceada$Exited)
table(data_reducida_balanceada$Exited, useNA="ifany")
>>>>>>> 4278ed4c46df3feb32a221530c19aad3a2f3ebb2

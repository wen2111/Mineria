NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
results
results
library(caret)
set.seed(123)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
#  Convertimos Exited a factor neg/pos
datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
#  Control LOOCV
ctrl_loocv <- trainControl(
method = "LOOCV",
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE   # importante para obtener predicciones out-of-fold
)
# Entrenamos modelo logístico con LOOCV
fit_loocv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = "binomial",
trControl = ctrl_loocv,
metric = "ROC"
)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
library(caret)
set.seed(123)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
#  Convertimos Exited a factor neg/pos
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
#  Control LOOCV
ctrl_loocv <- trainControl(
method = "LOOCV",
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE   # importante para obtener predicciones out-of-fold
)
# Entrenamos modelo logístico con LOOCV
fit_loocv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = "binomial",
trControl = ctrl_loocv,
metric = "ROC"
)
#  Probabilidades out-of-fold en el orden original
prob_loocv <- numeric(nrow(datatrain))
prob_loocv[fit_loocv$pred$rowIndex] <- fit_loocv$pred$pos
#  Convertimos Exited a 0/1 para métricas
y_numeric <- ifelse(datatrain$Exited == "pos", 1, 0)
cutpoints <- seq(0.1, 0.9, by=0.05)
results   <- data.frame()
results <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
results <- rbind(results, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
results
results
results
y_numeric
prob_alineada[fit_cv$pred$rowIndex]
cutpoints <- seq(0.1, 0.9, by=0.05)
results   <- data.frame()
resultsCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
results <- rbind(results, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsCV
performance_metrics <- function(y_true, prob, cutoff=0.5, dec=3){
# Convertim y_true a neg/pos
y_true <- factor(y_true, levels=c(0,1), labels=c("neg","pos"))
# Predicció com a factor amb nivells fixos
pred <- factor(ifelse(prob > cutoff, "pos", "neg"),
levels=c("neg","pos"))
# Calculem la taula manualment garantint files/columnes
cm <- matrix(0, nrow=2, ncol=2,
dimnames=list(Predicted=c("neg","pos"),
Actual=c("neg","pos")))
tab <- table(Predicted=pred, Actual=y_true)
cm[rownames(tab), colnames(tab)] <- tab
TN <- cm["neg","neg"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
TP <- cm["pos","pos"]
Sens <- TP/(TP+FN)
Spec <- TN/(TN+FP)
PPV  <- TP/(TP+FP)
NPV  <- TN/(TN+FN)
PLR  <- PPV/(1-NPV)
NLR  <- NPV/(1-PPV)
Acc  <- (TP+TN) / sum(cm)
list(
Sensitivity = round(Sens, dec),
Specificity = round(Spec, dec),
PPV = round(PPV, dec),
NPV = round(NPV, dec),
PLR = round(PLR, dec),
NLR = round(NLR, dec),
Accuracy = round(Acc, dec),
ConfusionMatrix = cm
)
}
#Per tenir les probabilitats en l'ordre de la variable sortida:
prob_alineada <- numeric(nrow(datatrain))
prob_alineada[fit_cv$pred$rowIndex] <- fit_cv$pred$pos
#tornem a transformar a numèrica la variable sortida
y_numeric <- ifelse(datatrain$Exited == "pos", 1, 0)
cutpoints <- seq(0.1, 0.9, by=0.05)
results   <- data.frame()
resultsCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
results <- rbind(results, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsCV
cutpoints <- seq(0.1, 0.9, by=0.05)
results   <- data.frame()
results <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
results <- rbind(results, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
results
cutpoints <- seq(0.1, 0.9, by=0.05)
resultsCV   <- data.frame()
resultsCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
resultsCV <- rbind(resultsCV, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsCV
library(caret)
set.seed(123)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
#  Convertimos Exited a factor neg/pos
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
#  Control LOOCV
ctrl_loocv <- trainControl(
method = "LOOCV",
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE   # importante para obtener predicciones out-of-fold
)
# Entrenamos modelo logístico con LOOCV
fit_loocv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = "binomial",
trControl = ctrl_loocv,
metric = "ROC"
)
#  Probabilidades out-of-fold en el orden original
prob_loocv <- numeric(nrow(datatrain))
prob_loocv[fit_loocv$pred$rowIndex] <- fit_loocv$pred$pos
#  Convertimos Exited a 0/1 para métricas
y_numeric <- ifelse(datatrain$Exited == "pos", 1, 0)
cutpoints <- seq(0.1, 0.9, by=0.05)
resultsLOOCV   <- data.frame()
resultsLOOCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
resultsLOOCV <- rbind(resultsLOOCV, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsLOOCV
save.image("data_reducida_balanceada_CV_LOOCV.RData")
cutpoints <- seq(0.1, 0.9, by=0.05)
resultsCV   <- data.frame()
resultsCV <- data.frame(
Cutpoint    = numeric(),
Sensitivity = numeric(),
Specificity = numeric(),
PPV         = numeric(),
NPV         = numeric(),
PLR         = numeric(),
NLR         = numeric(),
Accuracy    = numeric(),
F1          = numeric()
)
f1_score_cm <- function(cm){
TP <- cm["pos","pos"]
FP <- cm["pos","neg"]
FN <- cm["neg","pos"]
precision <- TP / (TP + FP)
recall    <- TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
return(round(f1, 3))
}
for(c in cutpoints){
m <- performance_metrics(y_numeric, prob_alineada, cutoff=c)
resultsCV <- rbind(resultsCV, data.frame(
Cutpoint    = c,
Sensitivity = m$Sensitivity,
Specificity = m$Specificity,
PPV         = m$PPV,
NPV         = m$NPV,
PLR         = m$PLR,
NLR         = m$NLR,
Accuracy    = m$Accuracy,
F1          = f1_score_cm(m$ConfusionMatrix)
))
}
resultsCV
knitr::opts_chunk$set(echo = TRUE)
# load("~/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
library(caret)
set.seed(123)
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
# Control de cross-validation
train_ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE
)
# Entrenem el model logístic amb CV
fit_cv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = binomial,
trControl = train_ctrl,
metric = "ROC"
)
# load("~/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
library(caret)
set.seed(123)
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
# Control de cross-validation
train_ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE
)
# Entrenem el model logístic amb CV
fit_cv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = binomial,
trControl = train_ctrl,
metric = "ROC"
)
table(data_reducida_balanceada$Exited)
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/Entrega_2/glm/data_reducida_balanceada_CV_LOOCV.RData")
# load("~/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
library(caret)
set.seed(123)
# datatrain$Exited <- factor(datatrain$Exited, levels=c(0,1), labels=c("neg","pos"))
# Control de cross-validation
train_ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE
)
# Entrenem el model logístic amb CV
fit_cv <- train(
Exited ~ .,
data = datatrain,
method = "glm",
family = binomial,
trControl = train_ctrl,
metric = "ROC"
)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/reducida_balanceada.RData")
# load("~/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data_reducida_balanceada$Exited <- factor(data_reducida_balanceada$Exited, levels=c("No","Yes"), labels=c("neg","pos"))
table(data_reducida_balanceada$Exited)
datatrain <- data_reducida_balanceada[1:7000, !(names(data_reducida_balanceada) %in% "group")]
datatest<-data_reducida_balanceada[7001:10000,!(names(data_reducida_balanceada) %in% "group")]
table(datatrain$Exited$Exited)
table(datatrain$Exited)
data_reducida_balanceada[7001:10000]
dim(data_reducida_balanceada)
view(data_reducida_balanceada)
View(data_reducida_balanceada)
# 1. 读数据
#load(".../reducida_balanceada.RData")   # 你原来的路径
# 2. 把 Exited 变成 neg/pos（Yes = 正类）
data_reducida_balanceada$Exited <- factor(
data_reducida_balanceada$Exited,
levels = c("No", "Yes"),
labels = c("neg", "pos")
)
# 3. 这里就直接用整张表做训练数据，不再切 1:7000 / 7001:10000
datatrain <- data_reducida_balanceada   # 如果没有 group 列，就直接这样
# 如果还有 group 列，就：
# datatrain <- data_reducida_balanceada[ , !(names(data_reducida_balanceada) %in% "group")]
# 4.（可选）检查是否有 NA
colSums(is.na(datatrain))
# 5. CV 控制
set.seed(123)
train_ctrl <- trainControl(
method = "cv",
number = 10,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = TRUE
)
# 6. 训练 GLM（现在这里就能跑了）
fit_cv <- train(
Exited ~ .,
data   = datatrain,
method = "glm",
family = binomial,
trControl = train_ctrl,
metric = "ROC"
)
unique(data_reducida_balanceada$Exited)
table(data_reducida_balanceada$Exited, useNA="ifany")

max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.15, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.15, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
fit_rf <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "ROC",          # optimizar AUC
trControl = ctrl_rf,
tuneGrid = tuneGrid,
ntree = 500,        # número de árboles
nodesize=80, maxnodes=100
)
plot(fit_rf)
fit_rf
pred <- predict(fit_rf, newdata = test2,type = 'raw')
ptrain <- predict(fit_rf, newdata=train2, type = 'raw')
confusionMatrix(ptrain, train2$Exited, positive="Yes")
confusionMatrix(pred, test2$Exited, positive="Yes")
pred <- predict(fit_rf, newdata = test2,type = 'prob')
pred <- pred[,2]
r <- multiclass.roc(test2$Exited, pred, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,print.auc=TRUE,
auc.polygon=TRUE,
grid=c(0.1, 0.2),
grid.col=c("green", "red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.15, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.15, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
fit_rf <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "ROC",          # optimizar AUC
trControl = ctrl_rf,
tuneGrid = tuneGrid,
ntree = 500,        # número de árboles
nodesize=60, maxnodes=80
)
plot(fit_rf)
fit_rf
pred <- predict(fit_rf, newdata = test2,type = 'raw')
ptrain <- predict(fit_rf, newdata=train2, type = 'raw')
confusionMatrix(ptrain, train2$Exited, positive="Yes")
confusionMatrix(pred, test2$Exited, positive="Yes")
pred <- predict(fit_rf, newdata = test2,type = 'prob')
pred <- pred[,2]
r <- multiclass.roc(test2$Exited, pred, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,print.auc=TRUE,
auc.polygon=TRUE,
grid=c(0.1, 0.2),
grid.col=c("green", "red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.15, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.15, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.1, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.1, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
fit_rf <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "ROC",          # optimizar AUC
trControl = ctrl_rf,
tuneGrid = tuneGrid,
ntree = 500,        # número de árboles
nodesize=40, maxnodes=60
)
plot(fit_rf)
fit_rf
pred <- predict(fit_rf, newdata = test2,type = 'raw')
ptrain <- predict(fit_rf, newdata=train2, type = 'raw')
confusionMatrix(ptrain, train2$Exited, positive="Yes")
confusionMatrix(pred, test2$Exited, positive="Yes")
pred <- predict(fit_rf, newdata = test2,type = 'prob')
pred <- pred[,2]
r <- multiclass.roc(test2$Exited, pred, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,print.auc=TRUE,
auc.polygon=TRUE,
grid=c(0.1, 0.2),
grid.col=c("green", "red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.1, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.1, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.2, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.2, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
fit_rf <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "ROC",          # optimizar AUC
trControl = ctrl_rf,
tuneGrid = tuneGrid,
ntree = 500,        # número de árboles
nodesize=30, maxnodes=50
)
plot(fit_rf)
fit_rf
pred <- predict(fit_rf, newdata = test2,type = 'raw')
ptrain <- predict(fit_rf, newdata=train2, type = 'raw')
confusionMatrix(ptrain, train2$Exited, positive="Yes")
confusionMatrix(pred, test2$Exited, positive="Yes")
pred <- predict(fit_rf, newdata = test2,type = 'prob')
pred <- pred[,2]
r <- multiclass.roc(test2$Exited, pred, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,print.auc=TRUE,
auc.polygon=TRUE,
grid=c(0.1, 0.2),
grid.col=c("green", "red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.2, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.2, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.15, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.15, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
mydata <- data_transformada
mydata$group<-NULL
mydata$Surname<-NULL
mydata$ID<-NULL
# SEPARAR TRAIN Y TEST
train <- mydata[1:7000,]
test <- mydata[7001:10000,]  # 3000 obs
# LABELS PARA EXITED
train$Exited <- factor(train$Exited,
levels = c("0","1"),
labels = c("No","Yes"))
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
rf <- randomForest(Exited ~ ., data = train2,)
varImpPlot(rf)
library(caret)
library(ROSE)
library(smotefamily)
data <- data_reducida
# SEPARAR TRAIN Y TEST
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
# ELIMINAR "GROUP"
train$group <- NULL
test$group  <- NULL
# LABELS PARA EXITED
train$Exited <- factor(train$Exited,
levels = c("0","1"),
labels = c("No","Yes"))
table(train$Exited)
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index,]     # Entrenamiento interno
test2  <- train[-index, ]    # Validación interna
#str(data_imputado)
# BALANCEO SOLO EN TRAIN2
train2_balanceada <- ROSE(
Exited ~ .,
data = train2,
p = 0.4,      # 40% Yes, 60% No
seed = 123
)$data
# COMPROBAR BALANCEO
(table(train2$Exited))                     # Antes del balanceo
(table(train2_balanceada$Exited))  # Después del balanceo
View(train2_balanceada)
# Logit
modelo_logit <- glm(Exited ~ ., data = train2, family = binomial(link = "logit"))
# Probit
modelo_probit <- glm(Exited ~ ., data = train2, family = binomial(link = "probit"))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2, type = "response")
prop.table(table(train2_balanceada$Exited))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2, type = "response")
class_logit_train2 <- ifelse(pred_logit_train2 > 0.2, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.2, 1, 0)
(conf_logit_train2 <- confusionMatrix(as.factor(class_logit_train2), train2$Exited,positive = "1"))
pred_logit_train2
table(class_logit_train2)
table(train2_balanceada)
# Logit
modelo_logit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "logit"))
# Probit
modelo_probit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "probit"))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2_balanceada, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2_balanceada, type = "response")
class_logit_train2 <- ifelse(pred_logit_train2 > 0.2, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.2, 1, 0)
(conf_logit_train2 <- confusionMatrix(as.factor(class_logit_train2), train2_balanceada$Exited,positive = "1"))
class_logit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
(conf_logit_train2 <- confusionMatrix(as.factor(class_logit_train2), train2_balanceada$Exited,positive = "1"))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "1"))
table(class_logit_train2)
table(train2_balanceada$Exited)
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
(conf_probit_train2 <- confusionMatrix(class_probit_train2, train2_balanceada$Exited,positive = "Yes"))
class_probit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
(conf_probit_train2 <- confusionMatrix(class_probit_train2, train2_balanceada$Exited,positive = "Yes"))
# Logit
modelo_logit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "logit"))
# Probit
modelo_probit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "probit"))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2_balanceada, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2_balanceada, type = "response")
class_logit_train2 <- ifelse(pred_logit_train2 > 0.2, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.2, 1, 0)
class_logit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
class_probit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
# Logit
modelo_logit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "logit"))
# Probit
modelo_probit <- glm(Exited ~ ., data = train2_balanceada, family = binomial(link = "probit"))
# matrix para train2
pred_logit_train2 <- predict(modelo_logit, newdata = train2_balanceada, type = "response")
pred_probit_train2 <- predict(modelo_probit, newdata = train2_balanceada, type = "response")
class_logit_train2 <- ifelse(pred_logit_train2 > 0.2, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.2, 1, 0)
class_logit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
class_probit_train2 <- factor(class_probit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
(conf_probit_train2 <- confusionMatrix(class_probit_train2, train2_balanceada$Exited,positive = "Yes"))
# Función para calcular F1 desde confusionMatrix
calcular_f1 <- function(conf){
precision <- conf$byClass["Precision"]
recall    <- conf$byClass["Recall"]
f1        <- 2 * (precision * recall) / (precision + recall)
return(f1)
}
(f1_logit_train2 <- calcular_f1(conf_logit_train2))
(f1_probit_train2 <- calcular_f1(conf_probit_train2))
# Predicciones de TEST2
pred_logit_test2  <- predict(modelo_logit,  newdata = test2, type = "response")
pred_probit_test2 <- predict(modelo_probit, newdata = test2, type = "response")
mejor_umbral_f1 <- function(prob, real){
real <- as.factor(real)
umbrales <- seq(0.11, 1, by = 0.01)
f1_scores <- sapply(umbrales, function(t){
pred <- ifelse(prob > t, "Yes", "No")
pred <- factor(pred, levels = levels(real))
F1 <- caret::F_meas(pred, real)
return(F1)
})
# Mejor umbral y mejor F1
umbral_opt <- umbrales[which.max(f1_scores)]
f1_opt     <- max(f1_scores)
return(list(
umbral = umbral_opt,
F1     = f1_opt,
tabla  = data.frame(umbral = umbrales, f1 = f1_scores)
))
}
res_probit <- mejor_umbral_f1(pred_probit_test2, test2$Exited)
res_probit$umbral
res_probit$F1
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.67, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.67, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
a<-confusionMatrix(as.factor(class_probit_test2), test2$Exited,positive = "1")
a<-confusionMatrix(class_probit_test2, test2$Exited,positive = "Yes")
# Función para calcular F1 desde confusionMatrix
calcular_f1 <- function(conf){
precision <- conf$byClass["Precision"]
recall    <- conf$byClass["Recall"]
f1        <- 2 * (precision * recall) / (precision + recall)
return(f1)
}
(f1_logit_train2 <- calcular_f1(conf_logit_train2))
(f1_probit_train2 <- calcular_f1(conf_probit_train2))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
(conf_probit_train2 <- confusionMatrix(class_probit_train2, train2_balanceada$Exited,positive = "Yes"))
# Predicciones de TEST2
pred_logit_test2  <- predict(modelo_logit,  newdata = test2, type = "response")
pred_probit_test2 <- predict(modelo_probit, newdata = test2, type = "response")
mejor_umbral_f1 <- function(prob, real){
real <- as.factor(real)
umbrales <- seq(0.01, 1, by = 0.01)
f1_scores <- sapply(umbrales, function(t){
pred <- ifelse(prob > t, "Yes", "No")
pred <- factor(pred, levels = levels(real))
F1 <- caret::F_meas(pred, real)
return(F1)
})
# Mejor umbral y mejor F1
umbral_opt <- umbrales[which.max(f1_scores)]
f1_opt     <- max(f1_scores)
return(list(
umbral = umbral_opt,
F1     = f1_opt,
tabla  = data.frame(umbral = umbrales, f1 = f1_scores)
))
}
res_probit <- mejor_umbral_f1(pred_probit_test2, test2$Exited)
res_probit$umbral
res_probit$F1
# Predicciones de TEST2
pred_logit_test2  <- predict(modelo_logit,  newdata = test2, type = "response")
pred_probit_test2 <- predict(modelo_probit, newdata = test2, type = "response")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.2, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.2, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
z
f1_probit_test2 <- calcular_f1(z)
cat("F1 Probit - test2:", f1_probit_test2, "\n")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.5, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.5, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
# --- F1 de test2
f1_logit_test2  <- calcular_f1(z)
cat("F1 Logit - test2:", f1_logit_test2, "\n")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.7, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.7, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.5, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
# --- F1 de test2
f1_logit_test2  <- calcular_f1(z)
cat("F1 Logit - test2:", f1_logit_test2, "\n")
# Convertir a clases segun umbral
class_logit_test2  <- ifelse(pred_logit_test2  > 0.4, 1, 0)
class_probit_test2 <- ifelse(pred_probit_test2 > 0.4, 1, 0)
# Matriz de confusión
# Convertir predicción binaria a "No"/"Yes"
class_logit_test2_factor <- ifelse(class_logit_test2 == 1, "Yes", "No")
class_logit_test2_factor <- factor(class_logit_test2_factor,
levels = c("No", "Yes"))
test2$Exited <- factor(test2$Exited,
levels = c("No", "Yes"))
# Ahora sí funciona
z <- confusionMatrix(class_logit_test2_factor, test2$Exited, positive = "Yes")
a<-confusionMatrix(class_probit_test2, test2$Exited,positive = "Yes")
# --- F1 de test2
f1_logit_test2  <- calcular_f1(z)
f1_probit_test2 <- calcular_f1(a)
cat("F1 Logit - test2:", f1_logit_test2, "\n")
z
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))
class_logit_train2 <- ifelse(pred_logit_train2 > 0.5, 1, 0)
class_probit_train2 <- ifelse(pred_probit_train2 > 0.5, 1, 0)
class_logit_train2 <- factor(class_logit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
class_probit_train2 <- factor(class_probit_train2,
levels = c("0","1"),
labels = c("No","Yes"))
(conf_logit_train2 <- confusionMatrix(class_logit_train2, train2_balanceada$Exited,positive = "Yes"))

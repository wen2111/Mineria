<<<<<<< HEAD
=======
library(Matrix)
library(pROC)
library(ggplot2)
library(dplyr)
library(scales)
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
mydata <- data_imputado
####################CAMBIOS MARIO###############
# Regla 1: ComplaintsCount_bin = No_quejas, NumOfProducts = 3
AR1 <- ifelse(mydata$ComplaintsCount == 0 &
mydata$NumOfProducts == 3, 1, 0)
# Regla 3: Gender = Female, Age = 46-65, HasCrCard = 0, NumOfProducts = 1
AR3 <- ifelse(mydata$Gender == "Female" &
mydata$Age >= 46 & mydata$Age <= 65 &
mydata$HasCrCard == 0 &
mydata$NumOfProducts == 1, 1, 0)
# Regla 4: Age = 46-65, Geography = Germany, NumOfProducts = 1
AR4 <- ifelse(mydata$Age >= 46 & mydata$Age <= 65 &
mydata$Geography == "Germany" &
mydata$NumOfProducts == 1, 1, 0)
# Regla 5: Age = 46-65, HasCrCard = 0, ComplaintsCount_bin = No_quejas, NumOfProducts = 1
AR5 <- ifelse(mydata$Age >= 46 & mydata$Age <= 65 &
mydata$HasCrCard == 0 &
mydata$ComplaintsCount==0 &
mydata$NumOfProducts == 1, 1, 0)
AR6 <- ifelse(mydata$Age >= 46 & mydata$Age <= 55 &
mydata$IsActiveMember == 0 &
mydata$SavingsAccountFlag == 1 &
mydata$NumOfProducts == 1, 1, 0)
# Regla 7: Age = 46-55, IsActiveMember = 0, Balance = Muy Alto (50K+), NumOfProducts = 1
AR7 <- ifelse(mydata$Age >= 46 & mydata$Age <= 55 &
mydata$IsActiveMember == 0 &
mydata$Balance == "Muy Alto (50K+)" &
mydata$NumOfProducts == 1, 1, 0)
# Regla 8: Age = 46-55, IsActiveMember = 0, NumOfProducts = 1
AR8 <- ifelse(mydata$Age >= 46 & mydata$Age <= 55 &
mydata$IsActiveMember == 0 &
mydata$NumOfProducts == 1, 1, 0)
# Regla 9: TransactionFrequency = 21-30, Age = 46-55, IsActiveMember = 0, Balance = Muy Alto (50K+)
AR9 <- ifelse(mydata$TransactionFrequency >= 21 & mydata$TransactionFrequency <= 30 &
mydata$Age >= 46 & mydata$Age <= 55 &
mydata$IsActiveMember == 0 &
mydata$Balance == "Muy Alto (50K+)", 1, 0)
# Regla 10: Age = 46-55, Geography = Germany, NumOfProducts = 1
AR10 <- ifelse(mydata$Age >= 46 & mydata$Age <= 55 &
mydata$Geography == "Germany" &
mydata$NumOfProducts == 1, 1, 0)
# Regla 11
AR11 <- ifelse(mydata$LoanStatus == "Default risk" &
mydata$NetPromoterScore >= 0 & mydata$NetPromoterScore <= 6 &
mydata$Age >= 26 & mydata$Age <= 35 &
mydata$NumOfProducts == 2, 1, 0)
# Regla 12
AR12 <- ifelse(mydata$Age >= 26 & mydata$Age <= 35 &
mydata$DigitalEngagementScore >= 76 & mydata$DigitalEngagementScore <= 100 &
mydata$Balance <1000 &
mydata$NumOfProducts == 2, 1, 0)
# Regla 13
AR13 <- ifelse(mydata$Age >= 26 & mydata$Age <= 35 &
mydata$MaritalStatus == "Married" &
mydata$DigitalEngagementScore >= 76 & mydata$DigitalEngagementScore <= 100 &
mydata$NumOfProducts == 2, 1, 0)
# Regla 14
AR14 <- ifelse(mydata$Age >= 26 & mydata$Age <= 35 &
mydata$DigitalEngagementScore >= 76 & mydata$DigitalEngagementScore <= 100 &
mydata$ComplaintsCount == 0 &
mydata$NumOfProducts == 2, 1, 0)
# Regla 15
AR15 <- ifelse(mydata$Age >= 26 & mydata$Age <= 35 &
mydata$HasCrCard == 1 &
mydata$DigitalEngagementScore >= 76 & mydata$DigitalEngagementScore <= 100 &
mydata$NumOfProducts == 2, 1, 0)
# Regla 16
AR16 <- ifelse(mydata$Age >= 26 & mydata$Age <= 35 &
mydata$DigitalEngagementScore >= 76 & mydata$DigitalEngagementScore <= 100 &
mydata$SavingsAccountFlag == 1 &
mydata$NumOfProducts == 2, 1, 0)
# Regla 17
AR17 <- ifelse(mydata$LoanStatus == "Default risk" &
mydata$Age >= 26 & mydata$Age <= 35 &
mydata$NumOfProducts == 2, 1, 0)
# Regla 18
AR18 <- ifelse(mydata$LoanStatus == "No loan" &
mydata$Age >= 26 & mydata$Age <= 35 &
mydata$DigitalEngagementScore >= 76 & mydata$DigitalEngagementScore <= 100 &
mydata$NumOfProducts == 2, 1, 0)
# Regla 19
AR19 <- ifelse(mydata$Age >= 26 & mydata$Age <= 35 &
mydata$Geography == "France" &
mydata$DigitalEngagementScore >= 76 & mydata$DigitalEngagementScore <= 100 &
mydata$NumOfProducts == 2, 1, 0)
# Regla 20
AR20 <- ifelse(mydata$Age >= 18 & mydata$Age <= 25 &
mydata$DigitalEngagementScore >= 51 & mydata$DigitalEngagementScore <= 75 &
mydata$Balance <1000, 1, 0)
# Añadir estos vectores al data frame df_AR (asumiendo que ya existe)
df_AR <- data.frame(
AR1 = AR1,
AR3 = AR3,
AR4 = AR4,
AR5 = AR5,
AR6 = AR6,
AR7 = AR7,
AR8 = AR8,
AR9 = AR9,
AR10 = AR10,
AR11 = AR11,
AR12 = AR12,
AR13 = AR13,
AR14 = AR14,
AR15 = AR15,
AR16 = AR16,
AR17 = AR17,
AR18 = AR18,
AR19 = AR19,
AR20 = AR20
)
df_AR_resumen <- data.frame(
AR_1_10_n = rowSums(
df_AR[, c("AR1","AR3","AR4","AR5","AR6","AR7","AR8","AR9","AR10")]
),
AR_11_20_n = rowSums(
df_AR[, c("AR11","AR12","AR13","AR14","AR15","AR16","AR17","AR18","AR19","AR20")]
)
)
load("~/GitHub/Mineria/Entrega_2/Boosting/xgboot_melissa/data_reducida_con_ID.RData")
mydata<-cbind(data_reducida, df_AR_resumen)
#####################################################
# PREPARACIÓN
#####################################################
# Separar Train y Test (para Kaggle, Exited vacía)
train <- subset(mydata, group == "train")
test  <- subset(mydata, group == "test")
# Guardar ID para submission y eliminar variables innecesarias para el modelo
test_submission_id <- test$ID
variables_eliminar <- c("group", "Surname", "ID")
train <- train[, !names(train) %in% variables_eliminar]
test  <- test[, !names(test) %in% c("group", "Surname", "ID")]
#####################################################
### FEATURE ENGINEERING
#####################################################
# Creamos HasBalance porque los que tienen saldo se van mas
# XGBoost prefiere números: 1 si tiene saldo, 0 si no.
train$HasBalance <- ifelse(train$Balance > 0, 1, 0)
test$HasBalance  <- ifelse(test$Balance > 0, 1, 0)
# Convertir la variable objetivo a numérica 0/1 para XGBoost
# "Yes" o "1" es la clase positiva.
train$Exited <- ifelse(train$Exited == "Yes" | train$Exited == "1", 1, 0)
#####################################################
### 3. PARTICIÓN INTERNA (TRAIN2 / TEST2)
#####################################################
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ]
test2  <- train[-index, ]
#####################################################
### 4. TRANSFORMACIÓN NUMÉRICA (ONE-HOT ENCODING)
#####################################################
# XGBoost no acepta factores. Convertimos todo a matriz numérica.
# Importante: Creamos el "molde" solo con los datos de entrenamiento (train2).
# Separamos predictores de la variable objetivo
predictors_train2 <- train2[, !names(train2) %in% "Exited"]
predictors_test2  <- test2[, !names(test2) %in% "Exited"]
predictors_kaggle <- test[, !names(test) %in% "Exited"] # El test de Kaggle no tiene Exited
# Creamos el esquema de transformación
dummy_obj <- dummyVars(~ ., data = predictors_train2, fullRank = FALSE)
# Aplicamos el esquema a los 3 conjuntos de datos
>>>>>>> b5974282ad8e31d8736f6d7f328036da82e164e4
mat_train2  <- predict(dummy_obj, newdata = predictors_train2)
mat_test2   <- predict(dummy_obj, newdata = predictors_test2)
mat_kaggle  <- predict(dummy_obj, newdata = predictors_kaggle)
# Convertimos a formato nativo de XGBoost (DMatrix)
# test2 y train2 llevan etiqueta (label), kaggle no.
dtrain2 <- xgb.DMatrix(data = mat_train2, label = train2$Exited)
dtest2  <- xgb.DMatrix(data = mat_test2, label = test2$Exited)
dkaggle <- xgb.DMatrix(data = mat_kaggle)
#####################################################
### 5. ENTRENAMIENTO XGBOOST CON CV
#####################################################
# Configuración para subir el F1 (Atención a scale_pos_weight)
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
eta = 0.05,              # Aprendizaje lento para evitar overfitting
max_depth = 4,           # Árboles no muy profundos
subsample = 0.7,
colsample_bytree = 0.7,
scale_pos_weight = 4     # CLAVE: Compensar desbalanceo (aprox 80/20 ratio)
)
<<<<<<< HEAD
set.seed(777)
=======
set.seed(123)
>>>>>>> b5974282ad8e31d8736f6d7f328036da82e164e4
# Cross Validation para encontrar el número óptimo de rondas
cv_res <- xgb.cv(
params = params,
data = dtrain2,
nrounds = 1000,
nfold = 5,
stratified = TRUE,       # CLAVE: Evita el error de "dataset empty"
early_stopping_rounds = 50,
print_every_n = 50,
maximize = TRUE
)
# Entrenar modelo final con la mejor iteración encontrada
modelo_xgb <- xgb.train(
params = params,
data = dtrain2,
nrounds = cv_res$best_iteration
)
#####################################################
### 6. EVALUACIÓN Y UMBRAL ÓPTIMO (TEST2)
#####################################################
# Predecimos probabilidades sobre la validación interna
probs_test2 <- predict(modelo_xgb, dtest2)
# Curva ROC para buscar el mejor punto de corte
roc_obj <- roc(test2$Exited, probs_test2)
# Buscamos el umbral que maximiza la suma de Sensibilidad y Especificidad
coords_optimas <- coords(roc_obj, "best",
ret = c("threshold", "sensitivity", "specificity"),
best.method = "closest.topleft")
umbral_optimo <- coords_optimas$threshold
cat("El umbral optimizado es:", umbral_optimo, "\n")
################## GRAFICO ROC ########################
plot(roc_obj, print.auc = TRUE, print.thres = "best", col="blue", main="ROC Curve (Validation)")
#####################################################
### 7. CÁLCULO DE KPIS (TRAIN2 VS TEST2)
#####################################################
# Obtenemos predicciones también para train2 para ver si hay overfitting
probs_train2 <- predict(modelo_xgb, dtrain2)
# Convertimos probabilidad a clase usando el umbral optimizado
pred_class_train2 <- ifelse(probs_train2 > umbral_optimo, 1, 0)
pred_class_test2  <- ifelse(probs_test2 > umbral_optimo, 1, 0)
# Convertimos a Factor para caret (asegurando niveles 0 y 1)
f_pred_train2 <- factor(pred_class_train2, levels = c(0, 1))
f_pred_test2  <- factor(pred_class_test2, levels = c(0, 1))
f_real_train2 <- factor(train2$Exited, levels = c(0, 1))
f_real_test2  <- factor(test2$Exited, levels = c(0, 1))
# Matrices de Confusión
cm_train <- confusionMatrix(f_pred_train2, f_real_train2, positive = "1", mode = "prec_recall")
cm_test  <- confusionMatrix(f_pred_test2, f_real_test2, positive = "1", mode = "prec_recall")
# Tabla Resumen
resultados <- data.frame(
Dataset = c("Train2", "Test2"),
Accuracy = c(cm_train$overall["Accuracy"], cm_test$overall["Accuracy"]),
Precision = c(cm_train$byClass["Precision"], cm_test$byClass["Precision"]),
Recall = c(cm_train$byClass["Sensitivity"], cm_test$byClass["Sensitivity"]),
F1_Score = c(cm_train$byClass["F1"], cm_test$byClass["F1"])
)
print(resultados)
<<<<<<< HEAD
modelo_xgb$params
cv_res$best_iteration
#####################################################
### 5. ENTRENAMIENTO XGBOOST CON CV
#####################################################
param_bounds <- list(
eta = c(0.4,0.3,0.5),
max_depth = c(3L, 5L),
min_child_weight = c(1, 10),
subsample = c(0.7, 0.8),
colsample_bytree = c(0.6, 0.7)
)
library(ParBayesianOptimization)
install.packages(ParBayesianOptimization)
install.packages("ParBayesianOptimization")
#install.packages("ParBayesianOptimization")
library(ParBayesianOptimization)
cv_xgb <- function(eta, max_depth, min_child_weight, subsample, colsample_bytree) {
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
eta = eta,
max_depth = max_depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample_bytree,
scale_pos_weight = 4
)
cv <- xgb.cv(
params = params,
data = dtrain2,
nrounds = 1000,
nfold = 5,
stratified = TRUE,
early_stopping_rounds = 50,
maximize = TRUE,
verbose = 0
)
list(Score = cv$evaluation_log[cv$best_iteration]$test_auc_mean)
}
opt <- bayesOpt(
FUN = cv_xgb,
bounds = param_bounds,
initPoints = 10,
iters.n = 30
)
#####################################################
### 5. ENTRENAMIENTO XGBOOST CON CV
#####################################################
bounds <- list(
eta = c(0.01, 0.1),
max_depth = c(3L, 6L),
min_child_weight = c(1L, 5L),
subsample = c(0.6, 0.9),
colsample_bytree = c(0.6, 0.9)
)
cv_xgb <- function(eta, max_depth, min_child_weight, subsample, colsample_bytree) {
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
eta = eta,
max_depth = max_depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample_bytree,
scale_pos_weight = 4
)
cv <- xgb.cv(
params = params,
data = dtrain2,
nrounds = 400,
nfold = 5,
stratified = TRUE,
early_stopping_rounds = 50,
maximize = TRUE,
verbose = 0
)
list(Score = cv$evaluation_log[cv$best_iteration]$test_auc_mean)
}
opt <- bayesOpt(
FUN = cv_xgb,
bounds = param_bounds,
initPoints = 10,
iters.n = 30
)
#####################################################
### 5. ENTRENAMIENTO XGBOOST CON CV
#####################################################
bounds <- list(
eta = c(0.01, 0.1),
max_depth = c(3L, 6L),
min_child_weight = c(1L, 5L),
subsample = c(0.6, 0.9),
colsample_bytree = c(0.6, 0.9)
)
cv_xgb <- function(eta, max_depth, min_child_weight, subsample, colsample_bytree) {
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
eta = eta,
max_depth = max_depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample_bytree,
scale_pos_weight = 4
)
cv <- xgb.cv(
params = params,
data = dtrain2,
nrounds = 400,
nfold = 5,
stratified = TRUE,
early_stopping_rounds = 50,
maximize = TRUE,
verbose = 0
)
list(Score = cv$evaluation_log[cv$best_iteration]$test_auc_mean)
}
opt <- bayesOpt(
FUN = cv_xgb,
bounds = param_bounds,
initPoints = 10,
iters.n = 30
)
opt <- bayesOpt(
FUN = cv_xgb,
bounds = bounds,
initPoints = 10,
iters.n = 30
)
opt
# Mejores parámetros encontrados
best_params <- getBestPars(opt)
print(best_params)
final_cv <- xgb.cv(
params = c(best_params,
list(objective = "binary:logistic",
eval_metric = "auc",
scale_pos_weight = 4)),
data = dtrain2,
nrounds = 1000,
nfold = 5,
stratified = TRUE,
early_stopping_rounds = 50,
maximize = TRUE,
verbose = 0
)
best_nrounds <- final_cv$best_iteration
final_model <- xgb.train(
params = c(best_params,
list(objective = "binary:logistic",
eval_metric = "auc",
scale_pos_weight = 4)),
data = dtrain2,
nrounds = best_nrounds
)
#####################################################
### 6. EVALUACIÓN Y UMBRAL ÓPTIMO (TEST2)
#####################################################
# Predecimos probabilidades sobre la validación interna
probs_test2 <- predict(final_model, dtest2)
# Curva ROC para buscar el mejor punto de corte
roc_obj <- roc(test2$Exited, probs_test2)
# Buscamos el umbral que maximiza la suma de Sensibilidad y Especificidad
coords_optimas <- coords(roc_obj, "best",
ret = c("threshold", "sensitivity", "specificity"),
best.method = "closest.topleft")
umbral_optimo <- coords_optimas$threshold
cat("El umbral optimizado es:", umbral_optimo, "\n")
################## GRAFICO ROC ########################
plot(roc_obj, print.auc = TRUE, print.thres = "best", col="blue", main="ROC Curve (Validation)")
#####################################################
### 7. CÁLCULO DE KPIS (TRAIN2 VS TEST2)
#####################################################
# Obtenemos predicciones también para train2 para ver si hay overfitting
probs_train2 <- predict(final_model, dtrain2)
# Convertimos probabilidad a clase usando el umbral optimizado
pred_class_train2 <- ifelse(probs_train2 > umbral_optimo, 1, 0)
pred_class_test2  <- ifelse(probs_test2 > umbral_optimo, 1, 0)
# Convertimos a Factor para caret (asegurando niveles 0 y 1)
f_pred_train2 <- factor(pred_class_train2, levels = c(0, 1))
f_pred_test2  <- factor(pred_class_test2, levels = c(0, 1))
f_real_train2 <- factor(train2$Exited, levels = c(0, 1))
f_real_test2  <- factor(test2$Exited, levels = c(0, 1))
# Matrices de Confusión
cm_train <- confusionMatrix(f_pred_train2, f_real_train2, positive = "1", mode = "prec_recall")
cm_test  <- confusionMatrix(f_pred_test2, f_real_test2, positive = "1", mode = "prec_recall")
# Tabla Resumen
resultados <- data.frame(
Dataset = c("Train2", "Test2"),
Accuracy = c(cm_train$overall["Accuracy"], cm_test$overall["Accuracy"]),
Precision = c(cm_train$byClass["Precision"], cm_test$byClass["Precision"]),
Recall = c(cm_train$byClass["Sensitivity"], cm_test$byClass["Sensitivity"]),
F1_Score = c(cm_train$byClass["F1"], cm_test$byClass["F1"])
)
print(resultados)
final_model$params
final_cv$params
final_model$params
modelo_xgb$params
print(resultados)
final_model$nfeatures
final_model$feature_names
final_model$niter
final_model$callbacks
View(data_reducida)
mydata <- data_transformada
load("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
mydata <- data_transformada
vars <- c(
"Age",
"EstimatedSalary",
"AvgTransactionAmount",
"CreditScore",
"DigitalEngagementScore",
"Balance",
"NumOfProducts_grupo",
"TransactionFrequency",
"Tenure",
"NetPromoterScore",
"Geography",
"Gender",
"IsActiveMember",
"Exited"
)
mydata<-mydata[,vars]
#####################################################
# PREPARACIÓN
#####################################################
# Separar Train y Test (para Kaggle, Exited vacía)
train <- subset(mydata, group == "train")
test  <- subset(mydata, group == "test")
View(mydata)
mydata$group<-data_reducida$group
=======
View(mydata)
View(mat_train2)
#####################################################
######## XGBOOTING REDUCIDA CON HASBALANCE ##########
#############################################Melissa#
library(caret)
library(xgboost)
library(Matrix)
library(pROC)
library(ggplot2)
library(dplyr)
library(scales)
load("data_reducida_con_ID.RData")
setwd("~/GitHub/Mineria/Entrega_2/Boosting/xgboot_melissa")
load("data_reducida_con_ID.RData")
mydata <- data_reducida
>>>>>>> b5974282ad8e31d8736f6d7f328036da82e164e4
#####################################################
# PREPARACIÓN
#####################################################
# Separar Train y Test (para Kaggle, Exited vacía)
train <- subset(mydata, group == "train")
test  <- subset(mydata, group == "test")
# Guardar ID para submission y eliminar variables innecesarias para el modelo
test_submission_id <- test$ID
variables_eliminar <- c("group", "Surname", "ID")
train <- train[, !names(train) %in% variables_eliminar]
test  <- test[, !names(test) %in% c("group", "Surname", "ID")]
#####################################################
### FEATURE ENGINEERING
#####################################################
# Creamos HasBalance porque los que tienen saldo se van mas
# XGBoost prefiere números: 1 si tiene saldo, 0 si no.
train$HasBalance <- ifelse(train$Balance > 0, 1, 0)
test$HasBalance  <- ifelse(test$Balance > 0, 1, 0)
# Convertir la variable objetivo a numérica 0/1 para XGBoost
# "Yes" o "1" es la clase positiva.
train$Exited <- ifelse(train$Exited == "Yes" | train$Exited == "1", 1, 0)
#####################################################
### 3. PARTICIÓN INTERNA (TRAIN2 / TEST2)
#####################################################
<<<<<<< HEAD
set.seed(777)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ]
test2  <- train[-index, ]
save.image("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/Entrega_2/Boosting/xgboot_melissa/hola_tuning.RData")
mydata <- data_transformada
vars <- c(
"Age",
"EstimatedSalary",
"AvgTransactionAmount",
"CreditScore",
"DigitalEngagementScore",
"Balance",
"NumOfProducts_grupo",
"TransactionFrequency",
"Tenure",
"NetPromoterScore",
"Geography",
"Gender",
"IsActiveMember",
"Exited"
)
mydata<-mydata[,vars]
mydata$group<-data_reducida$group
#####################################################
# PREPARACIÓN
#####################################################
# Separar Train y Test (para Kaggle, Exited vacía)
train <- subset(mydata, group == "train")
test  <- subset(mydata, group == "test")
# Guardar ID para submission y eliminar variables innecesarias para el modelo
test_submission_id <- test$ID
variables_eliminar <- c("group", "Surname", "ID")
train <- train[, !names(train) %in% variables_eliminar]
test  <- test[, !names(test) %in% c("group", "Surname", "ID")]
#####################################################
### FEATURE ENGINEERING
#####################################################
# Creamos HasBalance porque los que tienen saldo se van mas
# XGBoost prefiere números: 1 si tiene saldo, 0 si no.
train$HasBalance <- ifelse(train$Balance > 0, 1, 0)
test$HasBalance  <- ifelse(test$Balance > 0, 1, 0)
# Convertir la variable objetivo a numérica 0/1 para XGBoost
# "Yes" o "1" es la clase positiva.
train$Exited <- ifelse(train$Exited == "Yes" | train$Exited == "1", 1, 0)
#####################################################
### 3. PARTICIÓN INTERNA (TRAIN2 / TEST2)
#####################################################
set.seed(777)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ]
test2  <- train[-index, ]
=======
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ]
test2  <- train[-index, ]
#####################################################
### 4. TRANSFORMACIÓN NUMÉRICA (ONE-HOT ENCODING)
#####################################################
# XGBoost no acepta factores. Convertimos todo a matriz numérica.
# Importante: Creamos el "molde" solo con los datos de entrenamiento (train2).
>>>>>>> b5974282ad8e31d8736f6d7f328036da82e164e4
# Separamos predictores de la variable objetivo
predictors_train2 <- train2[, !names(train2) %in% "Exited"]
predictors_test2  <- test2[, !names(test2) %in% "Exited"]
predictors_kaggle <- test[, !names(test) %in% "Exited"] # El test de Kaggle no tiene Exited
# Creamos el esquema de transformación
dummy_obj <- dummyVars(~ ., data = predictors_train2, fullRank = FALSE)
# Aplicamos el esquema a los 3 conjuntos de datos
mat_train2  <- predict(dummy_obj, newdata = predictors_train2)
mat_test2   <- predict(dummy_obj, newdata = predictors_test2)
mat_kaggle  <- predict(dummy_obj, newdata = predictors_kaggle)
# Convertimos a formato nativo de XGBoost (DMatrix)
# test2 y train2 llevan etiqueta (label), kaggle no.
dtrain2 <- xgb.DMatrix(data = mat_train2, label = train2$Exited)
dtest2  <- xgb.DMatrix(data = mat_test2, label = test2$Exited)
dkaggle <- xgb.DMatrix(data = mat_kaggle)
#####################################################
### 5. ENTRENAMIENTO XGBOOST CON CV
#####################################################
<<<<<<< HEAD
##############################################################
bounds <- list(
eta = c(0.01, 0.1),
max_depth = c(3L, 6L),
min_child_weight = c(1L, 5L),
subsample = c(0.6, 0.9),
colsample_bytree = c(0.6, 0.9)
)
cv_xgb <- function(eta, max_depth, min_child_weight, subsample, colsample_bytree) {
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
eta = eta,
max_depth = max_depth,
min_child_weight = min_child_weight,
subsample = subsample,
colsample_bytree = colsample_bytree,
scale_pos_weight = 4
)
cv <- xgb.cv(
params = params,
data = dtrain2,
nrounds = 400,
nfold = 5,
stratified = TRUE,
early_stopping_rounds = 50,
maximize = TRUE,
verbose = 0
)
list(Score = cv$evaluation_log[cv$best_iteration]$test_auc_mean)
}
opt <- bayesOpt(
FUN = cv_xgb,
bounds = bounds,
initPoints = 10,
iters.n = 30
)
# Mejores parámetros encontrados
best_params <- getBestPars(opt)
print(best_params)
final_cv <- xgb.cv(
params = c(best_params,
list(objective = "binary:logistic",
eval_metric = "auc",
scale_pos_weight = 4)),
data = dtrain2,
nrounds = 1000,
nfold = 5,
stratified = TRUE,
early_stopping_rounds = 50,
maximize = TRUE,
verbose = 0
)
best_nrounds <- final_cv$best_iteration
final_model <- xgb.train(
params = c(best_params,
list(objective = "binary:logistic",
eval_metric = "auc",
scale_pos_weight = 4)),
data = dtrain2,
nrounds = best_nrounds
=======
# Configuración para subir el F1 (Atención a scale_pos_weight)
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
eta = 0.05,              # Aprendizaje lento para evitar overfitting
max_depth = 4,           # Árboles no muy profundos
subsample = 0.7,
colsample_bytree = 0.7,
scale_pos_weight = 4     # CLAVE: Compensar desbalanceo (aprox 80/20 ratio)
)
set.seed(12345)
# Cross Validation para encontrar el número óptimo de rondas
cv_res <- xgb.cv(
params = params,
data = dtrain2,
nrounds = 1000,
nfold = 5,
stratified = TRUE,       # CLAVE: Evita el error de "dataset empty"
early_stopping_rounds = 50,
print_every_n = 50,
maximize = TRUE
)
# Entrenar modelo final con la mejor iteración encontrada
modelo_xgb <- xgb.train(
params = params,
data = dtrain2,
nrounds = cv_res$best_iteration
>>>>>>> b5974282ad8e31d8736f6d7f328036da82e164e4
)
#####################################################
### 6. EVALUACIÓN Y UMBRAL ÓPTIMO (TEST2)
#####################################################
# Predecimos probabilidades sobre la validación interna
<<<<<<< HEAD
probs_test2 <- predict(final_model, dtest2)
=======
probs_test2 <- predict(modelo_xgb, dtest2)
>>>>>>> b5974282ad8e31d8736f6d7f328036da82e164e4
# Curva ROC para buscar el mejor punto de corte
roc_obj <- roc(test2$Exited, probs_test2)
# Buscamos el umbral que maximiza la suma de Sensibilidad y Especificidad
coords_optimas <- coords(roc_obj, "best",
ret = c("threshold", "sensitivity", "specificity"),
best.method = "closest.topleft")
umbral_optimo <- coords_optimas$threshold
cat("El umbral optimizado es:", umbral_optimo, "\n")
################## GRAFICO ROC ########################
plot(roc_obj, print.auc = TRUE, print.thres = "best", col="blue", main="ROC Curve (Validation)")
#####################################################
### 7. CÁLCULO DE KPIS (TRAIN2 VS TEST2)
#####################################################
# Obtenemos predicciones también para train2 para ver si hay overfitting
<<<<<<< HEAD
probs_train2 <- predict(final_model, dtrain2)
=======
probs_train2 <- predict(modelo_xgb, dtrain2)
# Convertimos probabilidad a clase usando el umbral optimizado
pred_class_train2 <- ifelse(probs_train2 > umbral_optimo, 1, 0)
pred_class_test2  <- ifelse(probs_test2 > umbral_optimo, 1, 0)
# Convertimos a Factor para caret (asegurando niveles 0 y 1)
f_pred_train2 <- factor(pred_class_train2, levels = c(0, 1))
f_pred_test2  <- factor(pred_class_test2, levels = c(0, 1))
f_real_train2 <- factor(train2$Exited, levels = c(0, 1))
f_real_test2  <- factor(test2$Exited, levels = c(0, 1))
# Matrices de Confusión
cm_train <- confusionMatrix(f_pred_train2, f_real_train2, positive = "1", mode = "prec_recall")
cm_test  <- confusionMatrix(f_pred_test2, f_real_test2, positive = "1", mode = "prec_recall")
# Tabla Resumen
resultados <- data.frame(
Dataset = c("Train2", "Test2"),
Accuracy = c(cm_train$overall["Accuracy"], cm_test$overall["Accuracy"]),
Precision = c(cm_train$byClass["Precision"], cm_test$byClass["Precision"]),
Recall = c(cm_train$byClass["Sensitivity"], cm_test$byClass["Sensitivity"]),
F1_Score = c(cm_train$byClass["F1"], cm_test$byClass["F1"])
)
print(resultados)
#####################################################
######## XGBOOTING REDUCIDA CON HASBALANCE ##########
#############################################Melissa#
library(caret)
library(xgboost)
library(Matrix)
library(pROC)
library(ggplot2)
library(dplyr)
library(scales)
load("data_reducida_con_ID.RData")
mydata <- data_reducida
#####################################################
# PREPARACIÓN
#####################################################
# Separar Train y Test (para Kaggle, Exited vacía)
train <- subset(mydata, group == "train")
test  <- subset(mydata, group == "test")
# Guardar ID para submission y eliminar variables innecesarias para el modelo
test_submission_id <- test$ID
variables_eliminar <- c("group", "Surname", "ID")
train <- train[, !names(train) %in% variables_eliminar]
test  <- test[, !names(test) %in% c("group", "Surname", "ID")]
#####################################################
### FEATURE ENGINEERING
#####################################################
# Creamos HasBalance porque los que tienen saldo se van mas
# XGBoost prefiere números: 1 si tiene saldo, 0 si no.
train$HasBalance <- ifelse(train$Balance > 0, 1, 0)
test$HasBalance  <- ifelse(test$Balance > 0, 1, 0)
# Convertir la variable objetivo a numérica 0/1 para XGBoost
# "Yes" o "1" es la clase positiva.
train$Exited <- ifelse(train$Exited == "Yes" | train$Exited == "1", 1, 0)
#####################################################
### 3. PARTICIÓN INTERNA (TRAIN2 / TEST2)
#####################################################
set.seed(777)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ]
test2  <- train[-index, ]
#####################################################
### 4. TRANSFORMACIÓN NUMÉRICA (ONE-HOT ENCODING)
#####################################################
# XGBoost no acepta factores. Convertimos todo a matriz numérica.
# Importante: Creamos el "molde" solo con los datos de entrenamiento (train2).
# Separamos predictores de la variable objetivo
predictors_train2 <- train2[, !names(train2) %in% "Exited"]
predictors_test2  <- test2[, !names(test2) %in% "Exited"]
predictors_kaggle <- test[, !names(test) %in% "Exited"] # El test de Kaggle no tiene Exited
# Creamos el esquema de transformación
dummy_obj <- dummyVars(~ ., data = predictors_train2, fullRank = FALSE)
# Aplicamos el esquema a los 3 conjuntos de datos
mat_train2  <- predict(dummy_obj, newdata = predictors_train2)
mat_test2   <- predict(dummy_obj, newdata = predictors_test2)
mat_kaggle  <- predict(dummy_obj, newdata = predictors_kaggle)
# Convertimos a formato nativo de XGBoost (DMatrix)
# test2 y train2 llevan etiqueta (label), kaggle no.
dtrain2 <- xgb.DMatrix(data = mat_train2, label = train2$Exited)
dtest2  <- xgb.DMatrix(data = mat_test2, label = test2$Exited)
dkaggle <- xgb.DMatrix(data = mat_kaggle)
#####################################################
### 5. ENTRENAMIENTO XGBOOST CON CV
#####################################################
# Configuración para subir el F1 (Atención a scale_pos_weight)
params <- list(
objective = "binary:logistic",
eval_metric = "auc",
eta = 0.05,              # Aprendizaje lento para evitar overfitting
max_depth = 4,           # Árboles no muy profundos
subsample = 0.7,
colsample_bytree = 0.7,
scale_pos_weight = 4     # CLAVE: Compensar desbalanceo (aprox 80/20 ratio)
)
set.seed(12345)
# Cross Validation para encontrar el número óptimo de rondas
cv_res <- xgb.cv(
params = params,
data = dtrain2,
nrounds = 1000,
nfold = 5,
stratified = TRUE,       # CLAVE: Evita el error de "dataset empty"
early_stopping_rounds = 50,
print_every_n = 50,
maximize = TRUE
)
# Entrenar modelo final con la mejor iteración encontrada
modelo_xgb <- xgb.train(
params = params,
data = dtrain2,
nrounds = cv_res$best_iteration
)
#####################################################
### 6. EVALUACIÓN Y UMBRAL ÓPTIMO (TEST2)
#####################################################
# Predecimos probabilidades sobre la validación interna
probs_test2 <- predict(modelo_xgb, dtest2)
# Curva ROC para buscar el mejor punto de corte
roc_obj <- roc(test2$Exited, probs_test2)
# Buscamos el umbral que maximiza la suma de Sensibilidad y Especificidad
coords_optimas <- coords(roc_obj, "best",
ret = c("threshold", "sensitivity", "specificity"),
best.method = "closest.topleft")
umbral_optimo <- coords_optimas$threshold
cat("El umbral optimizado es:", umbral_optimo, "\n")
################## GRAFICO ROC ########################
plot(roc_obj, print.auc = TRUE, print.thres = "best", col="blue", main="ROC Curve (Validation)")
#####################################################
### 7. CÁLCULO DE KPIS (TRAIN2 VS TEST2)
#####################################################
# Obtenemos predicciones también para train2 para ver si hay overfitting
probs_train2 <- predict(modelo_xgb, dtrain2)
>>>>>>> b5974282ad8e31d8736f6d7f328036da82e164e4
# Convertimos probabilidad a clase usando el umbral optimizado
pred_class_train2 <- ifelse(probs_train2 > umbral_optimo, 1, 0)
pred_class_test2  <- ifelse(probs_test2 > umbral_optimo, 1, 0)
# Convertimos a Factor para caret (asegurando niveles 0 y 1)
f_pred_train2 <- factor(pred_class_train2, levels = c(0, 1))
f_pred_test2  <- factor(pred_class_test2, levels = c(0, 1))
f_real_train2 <- factor(train2$Exited, levels = c(0, 1))
f_real_test2  <- factor(test2$Exited, levels = c(0, 1))
# Matrices de Confusión
cm_train <- confusionMatrix(f_pred_train2, f_real_train2, positive = "1", mode = "prec_recall")
cm_test  <- confusionMatrix(f_pred_test2, f_real_test2, positive = "1", mode = "prec_recall")
# Tabla Resumen
resultados <- data.frame(
Dataset = c("Train2", "Test2"),
Accuracy = c(cm_train$overall["Accuracy"], cm_test$overall["Accuracy"]),
Precision = c(cm_train$byClass["Precision"], cm_test$byClass["Precision"]),
Recall = c(cm_train$byClass["Sensitivity"], cm_test$byClass["Sensitivity"]),
F1_Score = c(cm_train$byClass["F1"], cm_test$byClass["F1"])
)
print(resultados)
<<<<<<< HEAD
final_model$feature_names
colnames(mydata)
save.image("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/Entrega_2/Boosting/xgboot_melissa/tuning2.RData")
=======
>>>>>>> b5974282ad8e31d8736f6d7f328036da82e164e4

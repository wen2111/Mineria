---
title: "Boosting"
author: "Siling Guo"
date: "2025-11-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Carga de datos
```{r}
str(data_reducida_plus)
table(data_reducida_plus$Exited)
```

# Preparación de datos
```{r}
# subset y preparacion
train_reducida_plus <- subset(data_reducida_plus, group == "train") # 7000 obs
test_reducida_plus  <- subset(data_reducida_plus,
                         group == "test") 

vars_drop <- c("ID", "Surname", "group")
train_reducida_plus <- train_reducida_plus[, !(names(train_reducida_plus) %in% vars_drop)]
test_reducida_plus  <- test_reducida_plus[, !(names(test_reducida_plus) %in% vars_drop)]

# levels para "exited"
train_reducida_plus$Exited <- factor(train_reducida_plus$Exited,
                                levels = c("1","0"),
                                labels = c("Yes","No"))
```

# Modelo XGBoost con caret
```{r}
library(caret)

set.seed(123)

# cv
trControl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE, 
  summaryFunction = twoClassSummary 
)

# tuneGrid
tuneGrid <- expand.grid(
  nrounds          = 1000,   
  max_depth        = 4,     
  eta              = 0.05,
  gamma            = 0,
  colsample_bytree = 0.7,
  min_child_weight = 1,
  subsample        = 0.8
)

# XGBoost model de clasificación
xgb_fit <- train(
  Exited ~ .,              # Exited factor(Yes/No)
  data      = train_reducida_plus,
  method    = "xgbTree",
  trControl = trControl,
  tuneGrid  = tuneGrid,
  metric    = "ROC",
  verbosity = 0
)

xgb_fit
xgb_fit$bestTune
varImp(xgb_fit)
```

# Modelo GBM con caret
```{r}
library(caret)

set.seed(1)
trControl <- trainControl(method = "cv", number = 5)

caret.xgb <- train(
  Exited ~ .,
  method   = "xgbTree",
  data     = train_reducida_plus,
  trControl = trControl,
  verbosity = 0
)

caret.xgb
caret.xgb$bestTune

tuneGrid <- expand.grid(
  nrounds          = c(50, 100, 200),
  max_depth        = c(2, 4, 6),
  eta              = c(0.05, 0.1, 0.3),
  gamma            = 0,
  colsample_bytree = 0.8,
  min_child_weight = 1,
  subsample        = 0.8
)

caret.xgb <- train(
  Exited ~ .,
  method   = "xgbTree",
  data     = train_reducida_plus,
  trControl = trControl,
  tuneGrid  = tuneGrid,
  verbosity = 0
)
caret.xgb
caret.xgb$bestTune
```

```{r}
pred_test <- predict(caret.xgb, newdata = test_reducida_plus)
confusionMatrix(pred_test, test_reducida_plus$Exited)
```

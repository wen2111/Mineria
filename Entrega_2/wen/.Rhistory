cm_train <- confusionMatrix(pred_train_final, train2$Exited, positive = "Yes")
# Calcular F1
f1_train <- 2 * cm_train$byClass["Precision"] * cm_train$byClass["Sensitivity"] /
(cm_train$byClass["Precision"] + cm_train$byClass["Sensitivity"])
f1_test <- best_f1_test
# Crear tabla de resultados
kpis_xgb <- data.frame(
Dataset = c("Train2", "Test2"),
Threshold = c(round(best_threshold, 3), round(best_threshold, 3)),
Error_rate = c(
round(1 - cm_train$overall["Accuracy"], 4),
round(1 - cm_test$overall["Accuracy"], 4)
),
Accuracy = c(
round(cm_train$overall["Accuracy"], 4),
round(cm_test$overall["Accuracy"], 4)
),
Precision = c(
round(cm_train$byClass["Precision"], 4),
round(cm_test$byClass["Precision"], 4)
),
Recall_Sensitivity = c(
round(cm_train$byClass["Sensitivity"], 4),
round(cm_test$byClass["Sensitivity"], 4)
),
Specificity = c(
round(cm_train$byClass["Specificity"], 4),
round(cm_test$byClass["Specificity"], 4)
),
F1_Score = c(
round(f1_train, 4),
round(f1_test, 4)
),
AUC_ROC = c(
round(xgb_model$results$ROC[xgb_model$results$nrounds == xgb_model$bestTune$nrounds &
xgb_model$results$max_depth == xgb_model$bestTune$max_depth &
xgb_model$results$eta == xgb_model$bestTune$eta][1], 4),
NA  # AUC en test no se calcula automáticamente
)
)
# Mostrar resultados
cat("\n=== RESULTADOS XGBOOST ===\n")
cat("Mejor threshold:", round(best_threshold, 3), "\n\n")
cat("TRAIN2:\n")
cat("  Accuracy:", round(cm_train$overall["Accuracy"], 4), "\n")
cat("  F1:", round(f1_train, 4), "\n")
cat("  Recall:", round(cm_train$byClass["Sensitivity"], 4), "\n")
cat("  Precision:", round(cm_train$byClass["Precision"], 4), "\n\n")
cat("TEST2:\n")
cat("  Accuracy:", round(cm_test$overall["Accuracy"], 4), "\n")
cat("  F1:", round(f1_test, 4), "\n")
cat("  Recall:", round(cm_test$byClass["Sensitivity"], 4), "\n")
cat("  Precision:", round(cm_test$byClass["Precision"], 4), "\n\n")
mydata <- data_reducida
#######3
mydata$CustomerSegment<-NULL
mydata$LoanStatus<-NULL
mydata$IsActiveMember<-NULL
mydata$Gender<-NULL
mydata$ComplaintsCount<-NULL
mydata$HasCrCard<-NULL
mydata$SavingsAccountFlag<-NULL
####
mydata$group<-NULL
mydata$Surname<-NULL
mydata$ID<-NULL
#########
# SEPARAR TRAIN Y TEST
train <- mydata[1:7000,]
test <- mydata[7001:10000,]  # 3000 obs
# LABELS PARA EXITED
train$Exited <- factor(train$Exited,
levels = c("0","1"),
labels = c("No","Yes"))
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
library(caret)
# 1. Control optimizado para F1 (no ROC)
ctrl_rf_f1 <- trainControl(
method = "cv",
number = 5,            # 5-fold es suficiente
classProbs = TRUE,
summaryFunction = function(data, lev = NULL, model = NULL) {
# Calcular F1, Recall, Precision
precision <- posPredValue(data$pred, data$obs, positive = "Yes")
recall <- sensitivity(data$pred, data$obs, positive = "Yes")
f1 <- ifelse((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0)
c(F1 = f1, Precision = precision, Recall = recall)
},
sampling = "smote",    # Mantener balanceo
verboseIter = FALSE
)
# 2. Grid de tuning MÁS AMPLIO para F1
mtry.class <- sqrt(ncol(train2) - 1)
tuneGrid_rf <- expand.grid(
mtry = floor(c(mtry.class/2, mtry.class, mtry.class*1.5, mtry.class*2))
)
# 3. Entrenar optimizando F1
set.seed(123)
fit_rf_f1 <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "F1",          # ¡Cambiado a F1!
maximize = TRUE,        # Maximizar F1
trControl = ctrl_rf_f1,
tuneGrid = tuneGrid_rf,
ntree = 300,            # Reducido para más velocidad
nodesize = c(5, 10, 15),  # Probar diferentes
importance = TRUE,      # Para ver importancia
preProcess = c("center", "scale")
)
cat("Mejores parámetros Random Forest:\n")
print(fit_rf_f1$bestTune)
# 4. Encontrar MEJOR THRESHOLD para F1 (no fijo en 0.15)
ptest_probs <- predict(fit_rf_f1, test2, type = "prob")$Yes
ptrain_probs <- predict(fit_rf_f1, train2, type = "prob")$Yes
# Buscar threshold óptimo en test2
thresholds <- seq(0.1, 0.9, 0.02)
best_f1 <- 0
best_threshold <- 0.15  # Tu valor actual
for(th in thresholds) {
ptest <- ifelse(ptest_probs > th, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
cm_test <- confusionMatrix(ptest, test2$Exited, positive = "Yes")
precision <- cm_test$byClass["Precision"]
recall <- cm_test$byClass["Sensitivity"]
f1 <- 2 * precision * recall / (precision + recall)
if(f1 > best_f1) {
best_f1 <- f1
best_threshold <- th
}
}
cat("\nMejor threshold encontrado:", round(best_threshold, 3), "\n")
# 5. Evaluar con threshold óptimo
ptrain_final <- ifelse(ptrain_probs > best_threshold, "Yes", "No")
ptrain_final <- factor(ptrain_final, levels = c("No", "Yes"))
ptest_final <- ifelse(ptest_probs > best_threshold, "Yes", "No")
ptest_final <- factor(ptest_final, levels = c("No", "Yes"))
cm_train <- confusionMatrix(ptrain_final, train2$Exited, positive = "Yes")
cm_test <- confusionMatrix(ptest_final, test2$Exited, positive = "Yes")
# 6. Calcular F1
f1_train <- 2 * cm_train$byClass["Precision"] * cm_train$byClass["Sensitivity"] /
(cm_train$byClass["Precision"] + cm_train$byClass["Sensitivity"])
f1_test <- best_f1
# 7. Crear tabla de resultados
kpis_rf <- data.frame(
Dataset = c("Train2", "Test2"),
Threshold = round(best_threshold, 3),
Accuracy = c(
round(cm_train$overall["Accuracy"], 4),
round(cm_test$overall["Accuracy"], 4)
),
F1_Score = c(
round(f1_train, 4),
round(f1_test, 4)
),
Recall = c(
round(cm_train$byClass["Sensitivity"], 4),
round(cm_test$byClass["Sensitivity"], 4)
),
Precision = c(
round(cm_train$byClass["Precision"], 4),
round(cm_test$byClass["Precision"], 4)
),
Specificity = c(
round(cm_train$byClass["Specificity"], 4),
round(cm_test$byClass["Specificity"], 4)
)
)
cat("\n=== RESULTADOS RANDOM FOREST OPTIMIZADO ===\n")
print(kpis_rf)
# 8. Análisis de overfitting
cat("\n=== ANÁLISIS DE OVERFITTING ===\n")
cat("Diferencia F1 (Train-Test):", round(f1_train - f1_test, 4), "\n")
cat("Diferencia Accuracy:",
round(cm_train$overall["Accuracy"] - cm_test$overall["Accuracy"], 4), "\n")
best_threshold<-0.23
ptrain_final <- ifelse(ptrain_probs > best_threshold, "Yes", "No")
ptrain_final <- factor(ptrain_final, levels = c("No", "Yes"))
ptest_final <- ifelse(ptest_probs > best_threshold, "Yes", "No")
ptest_final <- factor(ptest_final, levels = c("No", "Yes"))
cm_train <- confusionMatrix(ptrain_final, train2$Exited, positive = "Yes")
cm_test <- confusionMatrix(ptest_final, test2$Exited, positive = "Yes")
# 6. Calcular F1
f1_train <- 2 * cm_train$byClass["Precision"] * cm_train$byClass["Sensitivity"] /
(cm_train$byClass["Precision"] + cm_train$byClass["Sensitivity"])
f1_test <- best_f1
# 7. Crear tabla de resultados
kpis_rf <- data.frame(
Dataset = c("Train2", "Test2"),
Threshold = round(best_threshold, 3),
Accuracy = c(
round(cm_train$overall["Accuracy"], 4),
round(cm_test$overall["Accuracy"], 4)
),
F1_Score = c(
round(f1_train, 4),
round(f1_test, 4)
),
Recall = c(
round(cm_train$byClass["Sensitivity"], 4),
round(cm_test$byClass["Sensitivity"], 4)
),
Precision = c(
round(cm_train$byClass["Precision"], 4),
round(cm_test$byClass["Precision"], 4)
),
Specificity = c(
round(cm_train$byClass["Specificity"], 4),
round(cm_test$byClass["Specificity"], 4)
)
)
cat("\n=== RESULTADOS RANDOM FOREST OPTIMIZADO ===\n")
print(kpis_rf)
# 8. Análisis de overfitting
cat("\n=== ANÁLISIS DE OVERFITTING ===\n")
cat("Diferencia F1 (Train-Test):", round(f1_train - f1_test, 4), "\n")
cat("Diferencia Accuracy:",
round(cm_train$overall["Accuracy"] - cm_test$overall["Accuracy"], 4), "\n")
mydata <- data_imputado
####
mydata$group<-NULL
mydata$Surname<-NULL
mydata$ID<-NULL
#########
# SEPARAR TRAIN Y TEST
train <- mydata[1:7000,]
test <- mydata[7001:10000,]  # 3000 obs
# LABELS PARA EXITED
train$Exited <- factor(train$Exited,
levels = c("0","1"),
labels = c("No","Yes"))
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
# 1. Control optimizado para F1 (no ROC)
ctrl_rf_f1 <- trainControl(
method = "cv",
number = 5,            # 5-fold es suficiente
classProbs = TRUE,
summaryFunction = function(data, lev = NULL, model = NULL) {
# Calcular F1, Recall, Precision
precision <- posPredValue(data$pred, data$obs, positive = "Yes")
recall <- sensitivity(data$pred, data$obs, positive = "Yes")
f1 <- ifelse((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0)
c(F1 = f1, Precision = precision, Recall = recall)
},
#sampling = "smote",    # Mantener balanceo
verboseIter = FALSE
)
# 2. Grid de tuning MÁS AMPLIO para F1
mtry.class <- sqrt(ncol(train2) - 1)
tuneGrid_rf <- expand.grid(
mtry = floor(c(mtry.class/2, mtry.class, mtry.class*1.5, mtry.class*2))
)
# 3. Entrenar optimizando F1
set.seed(123)
fit_rf_f1 <- train(
Exited ~ .,
data = train2,
method = "rf",
metric = "F1",          # ¡Cambiado a F1!
maximize = TRUE,        # Maximizar F1
trControl = ctrl_rf_f1,
tuneGrid = tuneGrid_rf,
ntree = 300,            # Reducido para más velocidad
nodesize = c(5, 10, 15),  # Probar diferentes
importance = TRUE,      # Para ver importancia
preProcess = c("center", "scale")
)
cat("Mejores parámetros Random Forest:\n")
print(fit_rf_f1$bestTune)
# 4. Encontrar MEJOR THRESHOLD para F1 (no fijo en 0.15)
ptest_probs <- predict(fit_rf_f1, test2, type = "prob")$Yes
ptrain_probs <- predict(fit_rf_f1, train2, type = "prob")$Yes
# Buscar threshold óptimo en test2
thresholds <- seq(0.1, 0.9, 0.02)
best_f1 <- 0
best_threshold <- 0.15  # Tu valor actual
for(th in thresholds) {
ptest <- ifelse(ptest_probs > th, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
cm_test <- confusionMatrix(ptest, test2$Exited, positive = "Yes")
precision <- cm_test$byClass["Precision"]
recall <- cm_test$byClass["Sensitivity"]
f1 <- 2 * precision * recall / (precision + recall)
if(f1 > best_f1) {
best_f1 <- f1
best_threshold <- th
}
}
best_threshold<-0.23
# 5. Evaluar con threshold óptimo
ptrain_final <- ifelse(ptrain_probs > best_threshold, "Yes", "No")
ptrain_final <- factor(ptrain_final, levels = c("No", "Yes"))
ptest_final <- ifelse(ptest_probs > best_threshold, "Yes", "No")
ptest_final <- factor(ptest_final, levels = c("No", "Yes"))
cm_train <- confusionMatrix(ptrain_final, train2$Exited, positive = "Yes")
cm_test <- confusionMatrix(ptest_final, test2$Exited, positive = "Yes")
# 6. Calcular F1
f1_train <- 2 * cm_train$byClass["Precision"] * cm_train$byClass["Sensitivity"] /
(cm_train$byClass["Precision"] + cm_train$byClass["Sensitivity"])
f1_test <- best_f1
# 7. Crear tabla de resultados
kpis_rf <- data.frame(
Dataset = c("Train2", "Test2"),
Threshold = round(best_threshold, 3),
Accuracy = c(
round(cm_train$overall["Accuracy"], 4),
round(cm_test$overall["Accuracy"], 4)
),
F1_Score = c(
round(f1_train, 4),
round(f1_test, 4)
),
Recall = c(
round(cm_train$byClass["Sensitivity"], 4),
round(cm_test$byClass["Sensitivity"], 4)
),
Precision = c(
round(cm_train$byClass["Precision"], 4),
round(cm_test$byClass["Precision"], 4)
),
Specificity = c(
round(cm_train$byClass["Specificity"], 4),
round(cm_test$byClass["Specificity"], 4)
)
)
cat("\n=== RESULTADOS RANDOM FOREST OPTIMIZADO ===\n")
print(kpis_rf)
print(fit_rf_f1$bestTune)
mydata <- data_reducida
#dummifico
x<-mydata[,-3] #quito la respuesta
x<-x[,1:4] # cojo solo las cat
x <- fastDummies::dummy_cols(x,
remove_first_dummy = TRUE,
remove_selected_columns = TRUE)
x<-cbind(x,mydata[,6:7]) # adjunto las numericas
x$Exited<-mydata$Exited # añado la respuesta
mydata<-x
# SEPARAR TRAIN Y TEST
train <- mydata[1:7000,]
test <- mydata[7001:10000,]  # 3000 obs
# LABELS PARA EXITED
train$Exited <- factor(train$Exited,
levels = c("0","1"),
labels = c("No","Yes"))
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
library(caret)
library(kernlab)
# 1. Control para optimizar F1
ctrl_svm <- trainControl(
method = "cv",
number = 5,            # 5-fold cross-validation
classProbs = TRUE,
summaryFunction = function(data, lev = NULL, model = NULL) {
precision <- posPredValue(data$pred, data$obs, positive = "Yes")
recall <- sensitivity(data$pred, data$obs, positive = "Yes")
f1 <- ifelse((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0)
c(F1 = f1, Precision = precision, Recall = recall)
},
verboseIter = FALSE
)
# 2. Grid de parámetros para SVM
# Probar kernel lineal y RBF
tuneGrid_svm <- expand.grid(
C = c(0.01, 0.1, 1, 10, 100),      # Parámetro de regularización
sigma = c(0.01, 0.1, 1)            # Solo para kernel RBF
)
# 3. Entrenar SVM con kernel RBF (más flexible)
set.seed(123)
svm_model <- train(
Exited ~ .,
data = train2,
method = "svmRadial",     # Kernel RBF
metric = "F1",
maximize = TRUE,
trControl = ctrl_svm,
tuneGrid = tuneGrid_svm,
preProcess = c("center", "scale"),  # ¡IMPORTANTE para SVM!
prob.model = TRUE                   # Para obtener probabilidades
)
cat("Mejores parámetros SVM (RBF):\n")
print(svm_model$bestTune)
# 4. Probar también SVM lineal (más simple)
set.seed(123)
svm_linear <- train(
Exited ~ .,
data = train2,
method = "svmLinear",
metric = "F1",
maximize = TRUE,
trControl = ctrl_svm,
tuneGrid = expand.grid(C = c(0.01, 0.1, 1, 10)),
preProcess = c("center", "scale"),
prob.model = TRUE
)
cat("\nMejores parámetros SVM (Lineal):\n")
print(svm_linear$bestTune)
# 5. Evaluar ambos modelos
models <- list(RBF = svm_model, Lineal = svm_linear)
results_svm <- data.frame()
for(model_name in names(models)) {
model <- models[[model_name]]
# Predecir probabilidades
probs_train <- predict(model, train2, type = "prob")$Yes
probs_test <- predict(model, test2, type = "prob")$Yes
# Encontrar mejor threshold para F1 en test2
thresholds <- seq(0.1, 0.9, 0.02)
best_f1 <- 0
best_threshold <- 0.5
for(th in thresholds) {
pred_test <- ifelse(probs_test > th, "Yes", "No")
pred_test <- factor(pred_test, levels = c("No", "Yes"))
cm_test <- confusionMatrix(pred_test, test2$Exited, positive = "Yes")
precision <- cm_test$byClass["Precision"]
recall <- cm_test$byClass["Sensitivity"]
f1 <- 2 * precision * recall / (precision + recall)
if(f1 > best_f1) {
best_f1 <- f1
best_threshold <- th
}
}
# Evaluar con mejor threshold
pred_test_final <- ifelse(probs_test > best_threshold, "Yes", "No")
pred_test_final <- factor(pred_test_final, levels = c("No", "Yes"))
cm_test <- confusionMatrix(pred_test_final, test2$Exited, positive = "Yes")
pred_train_final <- ifelse(probs_train > best_threshold, "Yes", "No")
pred_train_final <- factor(pred_train_final, levels = c("No", "Yes"))
cm_train <- confusionMatrix(pred_train_final, train2$Exited, positive = "Yes")
# Calcular F1
f1_train <- 2 * cm_train$byClass["Precision"] * cm_train$byClass["Sensitivity"] /
(cm_train$byClass["Precision"] + cm_train$byClass["Sensitivity"])
# Guardar resultados
results_svm <- rbind(results_svm, data.frame(
Modelo = model_name,
Kernel = ifelse(model_name == "RBF", "RBF", "Lineal"),
C = model$bestTune$C,
Sigma = ifelse(model_name == "RBF", model$bestTune$sigma, NA),
Threshold = round(best_threshold, 3),
F1_Train = round(f1_train, 4),
F1_Test = round(best_f1, 4),
Recall_Test = round(cm_test$byClass["Sensitivity"], 4),
Precision_Test = round(cm_test$byClass["Precision"], 4),
Accuracy_Test = round(cm_test$overall["Accuracy"], 4)
))
}
best_threshold<-0.25
pred_test_final <- ifelse(probs_test > best_threshold, "Yes", "No")
pred_test_final <- factor(pred_test_final, levels = c("No", "Yes"))
cm_test <- confusionMatrix(pred_test_final, test2$Exited, positive = "Yes")
pred_train_final <- ifelse(probs_train > best_threshold, "Yes", "No")
pred_train_final <- factor(pred_train_final, levels = c("No", "Yes"))
cm_train <- confusionMatrix(pred_train_final, train2$Exited, positive = "Yes")
# Calcular F1
f1_train <- 2 * cm_train$byClass["Precision"] * cm_train$byClass["Sensitivity"] /
(cm_train$byClass["Precision"] + cm_train$byClass["Sensitivity"])
# Guardar resultados
results_svm <- rbind(results_svm, data.frame(
Modelo = model_name,
Kernel = ifelse(model_name == "RBF", "RBF", "Lineal"),
C = model$bestTune$C,
Sigma = ifelse(model_name == "RBF", model$bestTune$sigma, NA),
Threshold = round(best_threshold, 3),
F1_Train = round(f1_train, 4),
F1_Test = round(best_f1, 4),
Recall_Test = round(cm_test$byClass["Sensitivity"], 4),
Precision_Test = round(cm_test$byClass["Precision"], 4),
Accuracy_Test = round(cm_test$overall["Accuracy"], 4)
))
}
for(model_name in names(models)) {
model <- models[[model_name]]
# Predecir probabilidades
probs_train <- predict(model, train2, type = "prob")$Yes
probs_test <- predict(model, test2, type = "prob")$Yes
best_threshold<-0.25
# Evaluar con mejor threshold
pred_test_final <- ifelse(probs_test > best_threshold, "Yes", "No")
pred_test_final <- factor(pred_test_final, levels = c("No", "Yes"))
cm_test <- confusionMatrix(pred_test_final, test2$Exited, positive = "Yes")
pred_train_final <- ifelse(probs_train > best_threshold, "Yes", "No")
pred_train_final <- factor(pred_train_final, levels = c("No", "Yes"))
cm_train <- confusionMatrix(pred_train_final, train2$Exited, positive = "Yes")
# Calcular F1
f1_train <- 2 * cm_train$byClass["Precision"] * cm_train$byClass["Sensitivity"] /
(cm_train$byClass["Precision"] + cm_train$byClass["Sensitivity"])
# Guardar resultados
results_svm <- rbind(results_svm, data.frame(
Modelo = model_name,
Kernel = ifelse(model_name == "RBF", "RBF", "Lineal"),
C = model$bestTune$C,
Sigma = ifelse(model_name == "RBF", model$bestTune$sigma, NA),
Threshold = round(best_threshold, 3),
F1_Train = round(f1_train, 4),
F1_Test = round(best_f1, 4),
Recall_Test = round(cm_test$byClass["Sensitivity"], 4),
Precision_Test = round(cm_test$byClass["Precision"], 4),
Accuracy_Test = round(cm_test$overall["Accuracy"], 4)
))
}
cat("\n=== COMPARATIVA SVM ===\n")
print(results_svm)
print(results_svm)

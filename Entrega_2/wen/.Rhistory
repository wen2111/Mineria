r <- multiclass.roc(test2$Exited, pred, percent = TRUE)
roc <- r[['rocs']]
r1 <- roc[[1]]
plot.roc(r1,print.auc=TRUE,
auc.polygon=TRUE,
grid=c(0.1, 0.2),
grid.col=c("green", "red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightblue",
print.thres=TRUE,
main= 'ROC Curve')
obs <- test2$Exited
caret::postResample(pred4, obs)
summary(ptrain$Yes)
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
summary(ptrain$Yes)
ptrain <- ifelse(ptrain[,2] > 0.03499, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.03499, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
ptest <- predict(fit_rf, test2, type = 'prob')
ptrain <- predict(fit_rf, train2, type = 'prob')
ptrain <- ifelse(ptrain[,2] > 0.03499, "Yes", "No")
ptrain <- factor(ptrain, levels = c("No", "Yes"))
ptest <- ifelse(ptest[,2] > 0.03499, "Yes", "No")
ptest <- factor(ptest, levels = c("No", "Yes"))
confusionMatrix(ptrain, train2$Exited, positive="Yes")
(cm<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
precision <- cm$byClass["Precision"]     # TP / (TP + FP)
recall <- cm$byClass["Sensitivity"]      # TP / (TP + FN)
f1 <- 2 * (precision * recall) / (precision + recall)
f1
f1_train<-confusionMatrix(ptrain, train2$Exited, positive="Yes")
(f1_test<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
kpis <- data.frame(
Dataset = c("Train2", "Test2"),
Error_rate = c(1 - conf_train$overall["Accuracy"],
1 - conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"],
conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"],
conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"],
conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"],
conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
kpis
kpis
conf_train<-confusionMatrix(ptrain, train2$Exited, positive="Yes")
(conf_test<-confusionMatrix(ptest, test2$Exited, positive="Yes"))
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
kpis <- data.frame(
Dataset = c("Train2", "Test2"),
Error_rate = c(1 - conf_train$overall["Accuracy"],
1 - conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"],
conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"],
conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"],
conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"],
conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
kpis
## ============================================================
##  ROSE + GLM + XGB, 流水线版本（带 KPIs / CM / submission）
## ============================================================
library(caret)
library(ROSE)
library(xgboost)
library(pROC)
library(MLmetrics)
set.seed(123)
## 0. 数据准备：保留你原来选的变量 --------------------------
# data_reducida: 假设列结构和你之前一样：
#  - 第3列是 Exited
#  - 1:4 是分类变量
#  - 6:7 是数值变量
mydata <- data_reducida
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/Entrega_2/siling/dataaaaaaaaaaaaaa.RData")
## ============================================================
##  ROSE + GLM + XGB, 流水线版本（带 KPIs / CM / submission）
## ============================================================
library(caret)
library(ROSE)
library(xgboost)
library(pROC)
library(MLmetrics)
set.seed(123)
## 0. 数据准备：保留你原来选的变量 --------------------------
# data_reducida: 假设列结构和你之前一样：
#  - 第3列是 Exited
#  - 1:4 是分类变量
#  - 6:7 是数值变量
mydata <- data_reducida
x <- mydata[ , -3]      # quitar la respuesta
x <- x[ , 1:4]          # categóricas
x <- cbind(x, mydata[ , 6:7]) # numéricas
x$Exited <- mydata$Exited     # añadir la respuesta
mydata <- x
## 1. 外层分割：train / test (7000 / 3000) -------------------
train <- mydata[1:7000, ]
test  <- mydata[7001:10000, ]  # 3000 obs
# Clase positiva = "Yes"， negativa = "No"
train$Exited <- factor(train$Exited,
levels = c("0", "1"),
labels = c("No", "Yes"))
test$Exited  <- factor(test$Exited,
levels = c("0", "1"),
labels = c("No", "Yes"))
## 2. 内部分割：train2 / test2 -------------------------------
set.seed(123)
index  <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ]
test2  <- train[-index, ]
## 3. dummyVars：保证所有集的列结构一致 ---------------------
dmy <- dummyVars(Exited ~ ., data = train2, fullRank = TRUE)
# 内部 train/test（用于模型选择 + 阈值选择）
train2_x <- as.data.frame(predict(dmy, newdata = train2))
train2_x$Exited <- train2$Exited
test2_x <- as.data.frame(predict(dmy, newdata = test2))
test2_x$Exited <- test2$Exited
# 外层 train/test（用于最终训练和 submission）
train_full_x <- as.data.frame(predict(dmy, newdata = train))
train_full_x$Exited <- train$Exited
test_full_x <- as.data.frame(predict(dmy, newdata = test))
## 4. trainControl
ctrl_rose <- trainControl(
method          = "cv",
number          = 5,
classProbs      = TRUE,
summaryFunction = twoClassSummary,  # ROC, Sens, Spec
sampling        = "rose",
verboseIter     = FALSE,
savePredictions = "final"
)
## 5. GLM (logit) + ROSE --------------------------------
set.seed(123)
fit_glm <- train(
Exited ~ .,
data      = train2_x,
method    = "glm",
family    = binomial(),  # binomial + logit link = logistic regression
metric    = "ROC",
trControl = ctrl_rose
)
library(caret)
library(fastDummies)
library(ROSE)
library(xgboost)
library(pROC)
library(MLmetrics)
set.seed(123)
mydata <- data_reducida
# dummificación
x <- mydata[ , -3] # quitar la respuesta
x <- x[ , 1:4] # categóricas
x <- fastDummies::dummy_cols(
x,
remove_first_dummy     = TRUE,
remove_selected_columns = TRUE
)
x <- cbind(x, mydata[ , 6:7]) # adjuntar las numericas
x$Exited <- mydata$Exited     # añadir la respuesta
mydata <- x
# SEPARAR TRAIN Y TEST
train <- mydata[1:7000, ]
test  <- mydata[7001:10000, ]  # 3000 obs
# LABELS PARA EXITED
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# test labels
test$Exited  <- factor(test$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# PARTICION TRAIN2/TEST2
set.seed(123)
index  <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ]   # train interno
test2  <- train[-index, ]  # test interno
set.seed(123)
rose_res <- ROSE(Exited ~ ., data = train2, seed = 123)
rose_res$data
train2
train2
str(train2)
names(mydata) <- make.names(names(mydata))
# SEPARAR TRAIN Y TEST
train <- mydata[1:7000, ]
test  <- mydata[7001:10000, ]  # 3000 obs
# LABELS PARA EXITED
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# test labels
test$Exited  <- factor(test$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# PARTICION TRAIN2/TEST2
set.seed(123)
index  <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ]   # train interno
test2  <- train[-index, ]  # test interno
str(train2)
set.seed(123)
rose_res <- ROSE(Exited ~ ., data = train2, seed = 123)
train2   <- rose_res$data
train2$Exited <- factor(train2$Exited,
levels = c("Yes","No"))
# para ver el balance
table(train2$Exited)
table(test2$Exited)
# PREPARAR MATRICES PARA XGBOOST
ctrl_xgb <- trainControl(
method          = "cv",
number          = 5,
classProbs      = TRUE,
summaryFunction = twoClassSummary,
verboseIter     = FALSE,
savePredictions = "final"
)
tuneGrid_xgb <- expand.grid(
nrounds          = c(100, 200),
max_depth        = c(3, 5),
eta              = c(0.05, 0.1),
gamma            = 0,
colsample_bytree = 0.8,
min_child_weight = 1,
subsample        = 0.8
)
## 3. train2(ROSE) + XGBoost
set.seed(123)
fit_xgb <- train(
Exited ~ .,
data      = train2,
method    = "xgbTree",
metric    = "ROC",
trControl = ctrl_xgb,
tuneGrid  = tuneGrid_xgb
)
fit_xgb
plot(fit_xgb)
## 4. en test2
# P(Exited == "Yes")
prob_test2 <- predict(fit_xgb, newdata = test2, type = "prob")[, "Yes"]
# buscar el mejor threshold según F1
find_best_threshold <- function(probs, actuals) {
thresholds <- seq(0.1, 0.9, by = 0.01)
f1_scores <- sapply(thresholds, function(t) {
preds <- factor(ifelse(probs > t, "Yes", "No"),
levels = c("No", "Yes"))
F1_Score(y_true = actuals, y_pred = preds, positive = "Yes")
})
best <- thresholds[which.max(f1_scores)]
return(best)
}
best_thresh <- find_best_threshold(prob_test2, test2$Exited)
cat("Mejor umbral según F1 en test2:", best_thresh, "\n")
check_threshold <- function(th) {
preds <- factor(ifelse(prob_test2 > th, "Yes", "No"),
levels = c("No", "Yes"))
cm <- confusionMatrix(preds, test2$Exited, positive = "Yes")
data.frame(
threshold   = th,
Sensitivity = cm$byClass["Sensitivity"],
Precision   = cm$byClass["Precision"],
BalancedAcc = mean(c(cm$byClass["Sensitivity"],
cm$byClass["Specificity"]))
)
}
ths <- seq(0.2, 0.6, by = 0.05)
res <- do.call(rbind, lapply(ths, check_threshold))
print(res)
# KPIs en test2
pred_test2_best <- ifelse(prob_test2 > best_thresh, "Yes", "No")
pred_test2_best <- factor(pred_test2_best, levels = c("No", "Yes"))
cm_test2 <- confusionMatrix(pred_test2_best, test2$Exited, positive = "Yes")
cm_test2
roc_xgb <- roc(response  = test2$Exited,
predictor = prob_test2,
levels    = c("No", "Yes"))
auc_xgb <- auc(roc_xgb)
cat("AUC XGB en test2:", auc_xgb, "\n")
auc_xgb
auc_xgb <- auc(roc_xgb)
auc_xgb
# ==============================
best_grid <- fit_xgb$bestTune
# ROSE to full train
set.seed(123)
train_full_rose <- ROSE(Exited ~ ., data = train, seed = 123)$data
train_full_rose$Exited <- factor(train_full_rose$Exited,
levels = c("Yes","No"))
set.seed(123)
final_xgb <- train(
Exited ~ .,
data      = train_full_rose,
method    = "xgbTree",
metric    = "ROC",
trControl = trainControl(
method          = "none",  # best_grid
classProbs      = TRUE
),
tuneGrid  = best_grid
)
final_xgb
## 7. con train
prob_train_full <- predict(final_xgb, newdata = train, type = "prob")[, "Yes"]
pred_train_full <- ifelse(prob_train_full > best_thresh, "Yes", "No")
pred_train_full <- factor(pred_train_full, levels = c("No", "Yes"))
cm_train <- confusionMatrix(pred_train_full, train$Exited, positive = "Yes")
cm_train
# buscar el mejor threshold según F1
metrics_by_threshold <- function(probs, actuals) {
ths <- seq(0.05, 0.6, by = 0.01)
res <- lapply(ths, function(t) {
preds <- factor(ifelse(probs > t, "Yes", "No"),
levels = levels(actuals))
cm <- confusionMatrix(preds, actuals, positive = "Yes")
data.frame(
threshold   = t,
Sensitivity = cm$byClass["Sensitivity"],
Specificity = cm$byClass["Specificity"],
Precision   = cm$byClass["Precision"],
BalancedAcc = mean(c(cm$byClass["Sensitivity"],
cm$byClass["Specificity"]))
)
})
do.call(rbind, res)
}
res_all <- metrics_by_threshold(prob_test2, test2$Exited)
# con el mismo best_thresh_recall recalcular cm de train / test
pred_train_full <- factor(
ifelse(prob_train_full > best_thresh_recall, "Yes", "No"),
levels = levels(train$Exited)
)
## 3. train2(ROSE) + XGBoost
set.seed(123)
fit_xgb <- train(
Exited ~ .,
data      = train2,
method    = "xgbTree",
metric    = "ROC",
trControl = ctrl_xgb,
tuneGrid  = tuneGrid_xgb
)
fit_xgb
fit_xgb
plot(fit_xgb)
## 4. en test2
# P(Exited == "Yes")
prob_test2 <- predict(fit_xgb, newdata = test2, type = "prob")[, "Yes"]
# buscar el mejor threshold según F1
metrics_by_threshold <- function(probs, actuals) {
ths <- seq(0.05, 0.6, by = 0.01)
res <- lapply(ths, function(t) {
preds <- factor(ifelse(probs > t, "Yes", "No"),
levels = levels(actuals))
cm <- confusionMatrix(preds, actuals, positive = "Yes")
data.frame(
threshold   = t,
Sensitivity = cm$byClass["Sensitivity"],
Specificity = cm$byClass["Specificity"],
Precision   = cm$byClass["Precision"],
BalancedAcc = mean(c(cm$byClass["Sensitivity"],
cm$byClass["Specificity"]))
)
})
do.call(rbind, res)
}
res_all <- metrics_by_threshold(prob_test2, test2$Exited)
# con el mismo best_thresh_recall recalcular cm de train / test
pred_train_full <- factor(
ifelse(prob_train_full > best_thresh_recall, "Yes", "No"),
levels = levels(train$Exited)
)
prob_test2
test2$Exited
prob_test2
# View(res_all)
# head(res_all[order(-res_all$Sensitivity), ])
# min Precision acceptable, ejemplo >= 0.35
target_min_precision <- 0.35
cand <- subset(res_all, Precision >= target_min_precision)
# Precision, maximizar Sensitivity
best_idx <- which.max(cand$Sensitivity)
best_row <- cand[best_idx, ]
best_row
best_thresh_recall <- best_row$threshold
best_thresh_recall
best_grid <- fit_xgb$bestTune
set.seed(123)
train_full_rose <- ROSE(Exited ~ ., data = train, seed = 123)$data
train_full_rose$Exited <- factor(train_full_rose$Exited,
levels = c("Yes","No"))
set.seed(123)
final_xgb <- train(
Exited ~ .,
data      = train_full_rose,
method    = "xgbTree",
metric    = "ROC",
trControl = trainControl(method = "none", classProbs = TRUE),
tuneGrid  = best_grid
)
final_xgb
tuneGrid_xgb <- expand.grid(
nrounds          = c(100, 200),
max_depth        = c(3, 5),
eta              = c(0.05, 0.1),
gamma            = 0,
colsample_bytree = 0.8,
min_child_weight = 1,
subsample        = 0.8
)
## 3. train2(ROSE) + XGBoost
set.seed(123)
fit_xgb <- train(
Exited ~ .,
data      = train2,
method    = "xgbTree",
metric    = "ROC",
trControl = ctrl_xgb,
tuneGrid  = tuneGrid_xgb
)
fit_xgb
plot(fit_xgb)
## 4. en test2
# P(Exited == "Yes")
prob_test2 <- predict(fit_xgb, newdata = test2, type = "prob")[, "Yes"]
# buscar el mejor threshold según F1
metrics_by_threshold <- function(probs, actuals) {
ths <- seq(0.05, 0.6, by = 0.01)
res <- lapply(ths, function(t) {
preds <- factor(ifelse(probs > t, "Yes", "No"),
levels = levels(actuals))
cm <- confusionMatrix(preds, actuals, positive = "Yes")
data.frame(
threshold   = t,
Sensitivity = cm$byClass["Sensitivity"],
Specificity = cm$byClass["Specificity"],
Precision   = cm$byClass["Precision"],
BalancedAcc = mean(c(cm$byClass["Sensitivity"],
cm$byClass["Specificity"]))
)
})
do.call(rbind, res)
}
res_all <- metrics_by_threshold(prob_test2, test2$Exited)
# View(res_all)
# head(res_all[order(-res_all$Sensitivity), ])
# min Precision acceptable, ejemplo >= 0.35
target_min_precision <- 0.35
cand <- subset(res_all, Precision >= target_min_precision)
# Precision, maximizar Sensitivity
best_idx <- which.max(cand$Sensitivity)
best_row <- cand[best_idx, ]
best_row
best_thresh_recall <- best_row$threshold
best_thresh_recall
best_grid <- fit_xgb$bestTune
set.seed(123)
train_full_rose <- ROSE(Exited ~ ., data = train, seed = 123)$data
train_full_rose$Exited <- factor(train_full_rose$Exited,
levels = c("Yes","No"))
set.seed(123)
final_xgb <- train(
Exited ~ .,
data      = train_full_rose,
method    = "xgbTree",
metric    = "ROC",
trControl = trainControl(method = "none", classProbs = TRUE),
tuneGrid  = best_grid
)
final_xgb
## train(7000)
prob_train_full <- predict(final_xgb, newdata = train, type = "prob")[, "Yes"]
pred_train_full <- factor(
ifelse(prob_train_full > best_thresh_recall, "Yes", "No"),
levels = levels(train$Exited)
)
cm_train <- confusionMatrix(pred_train_full, train$Exited, positive = "Yes")
cm_train
## test(3000)
prob_test_full <- predict(final_xgb, newdata = test, type = "prob")[, "Yes"]
pred_test_full <- factor(
ifelse(prob_test_full > best_thresh_recall, "Yes", "No"),
levels = levels(test$Exited)
)
cm_test <- confusionMatrix(pred_test_full, test$Exited, positive = "Yes")
cm_test
## submission
class_test_xgb <- pred_test_full
submission <- data.frame(
ID     = data_reducida$ID[7001:10000],
Exited = class_test_xgb
)
write.csv(submission, "submission_xgb_rose.csv", row.names = FALSE)

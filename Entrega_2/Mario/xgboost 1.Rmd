---
title: "xgboost mario"
output: html_document
date: "2025-12-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
load("~/GitHub/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
bd<-data_reducida
rec <- recipe(Exited ~ ., data = bd) %>%
  step_dummy(all_nominal_predictors(), -group, one_hot = TRUE, keep_original_cols = FALSE)%>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors())
bd<- prep(rec) %>% bake(new_data = NULL)
trainbase<-bd[bd$group=="train",]
trainbase$group<-NULL
testbase<-bd[bd$group=="test",]
testbase$group<-NULL
ind <- sample(1:nrow(trainbase), 0.7*nrow(trainbase))
train <- trainbase[ind,]
test <- trainbase[-ind,]
```


```{r}
library(tidymodels)
library(xgboost)

train$Exited <- factor(train$Exited)
test$Exited  <- factor(test$Exited)

levels(train$Exited)

```


```{r}
set.seed(123)
folds <- vfold_cv(trainbase, v = 10) # 10-fold Cross-Validation
metricas <- metric_set(roc_auc, f_meas, recall, accuracy)
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  min_n = tune(),
  sample_size = tune(),
  mtry = tune()
) %>%
  set_engine("xgboost") %>%
  set_mode("classification")


xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_formula(Exited ~ .)
set.seed(123)

set.seed(123)

xgb_grid <- grid_random(
  trees(range = c(300, 1200)),
  tree_depth(range = c(3, 10)),
  learn_rate(range = c(-3, -0.5)),   # log10
  loss_reduction(range = c(0, 10)),
  min_n(range = c(5, 30)),
  sample_prop(range = c(0.6, 1)),    # üëà AQU√ç
  mtry(range = c(5, ncol(train) - 1)),
  size = 30
)

xgb_tuned <- tune_grid(
  xgb_wf,
  resamples = folds,
  grid = xgb_grid,
  metrics = metricas
)
collect_metrics(xgb_tuned)
show_best(xgb_tuned, metric = "f_meas", n = 10)
autoplot(xgb_tuned, metric = "f_meas")
autoplot(xgb_tuned, metric = "recall")
```
```{r}
library(dplyr)
library(caret)

# La funci√≥n toma un umbral y devuelve las m√©tricas clave
evaluate_threshold <- function(data, threshold) {
  # Recalcula la clase predicha basada en el nuevo umbral
  pred_class_new <- factor(
    ifelse(data$.pred_1 >= threshold, "1", "0"),
    levels = c("0", "1") # Asegura el orden correcto
  )

  # Crea la Matriz de Confusi√≥n
  cm <- confusionMatrix(pred_class_new, data$Exited)

  # Calcula F1 (asumiendo que '1' es la clase positiva)
  f1 <- 2 * cm$byClass["Sensitivity"] * cm$byClass["Precision"] /
        (cm$byClass["Sensitivity"] + cm$byClass["Precision"])

  # Devuelve una fila de resultados
  return(data.frame(
    Umbral = threshold,
    F1_Score = f1,
    Sensitivity = cm$byClass["Sensitivity"],
    Specificity = cm$byClass["Specificity"],
    Accuracy = cm$overall["Accuracy"]
  ))
}
# Generamos una secuencia de umbrales a probar (0.05 a 0.95)
thresholds_to_test <- seq(from = 0.05, to = 0.5, by = 0.01)

# Aplicamos la funci√≥n a todos los umbrales
results_thresholds <- bind_rows(
  lapply(thresholds_to_test, function(t) {
    evaluate_threshold(pred_test, t) # Usamos pred_test que contiene .pred_1
  })
)

# Mostrar los 10 mejores resultados seg√∫n F1
print("Resultados de Umbrales (Top 10 F1)")
results_thresholds %>%
  arrange(desc(F1_Score)) %>%
  head(10)
```

```{r}
library(ggplot2)
library(tidyr)

results_long <- results_thresholds %>%
  pivot_longer(
    cols = c(F1_Score, Sensitivity, Specificity),
    names_to = "M√©trica",
    values_to = "Valor"
  )

# Gr√°fico para ver el punto √≥ptimo
ggplot(results_long, aes(x = Umbral, y = Valor, color = M√©trica)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  scale_color_manual(values = c("F1_Score" = "purple", "Sensitivity" = "blue", "Specificity" = "red")) +
  geom_vline(xintercept = results_thresholds[which.max(results_thresholds$F1_Score),]$Umbral, 
             linetype = "dashed", color = "black") +
  labs(
    title = "Rendimiento del Modelo vs. Umbral de Clasificaci√≥n",
    y = "Valor de la M√©trica",
    x = "Umbral de Probabilidad para Clase '1'"
  ) +
  theme_minimal()
```

```{r}
library(tidymodels)
library(caret)
best_params <- select_best(xgb_tuned, metric = "f_meas")

# 2Ô∏è‚É£ Finalizar el workflow con los mejores hiperpar√°metros
xgb_final_wf <- xgb_wf %>% 
  finalize_workflow(best_params)

# 3Ô∏è‚É£ Entrenar en todo el set de entrenamiento


xgb_final_fit <- fit(xgb_best_wf, data = train)

# 5Ô∏è‚É£ Predicciones
pred_train <- predict(xgb_final_fit, train, type = "prob") %>%
  bind_cols(train %>% select(Exited))
pred_train_class <- predict(xgb_final_fit, train) %>%
  bind_cols(train %>% select(Exited))

pred_test <- predict(xgb_final_fit, test, type = "prob") %>%
  bind_cols(test %>% select(Exited))
pred_test_class <- predict(xgb_final_fit, test) %>%
  bind_cols(test %>% select(Exited))

# 6Ô∏è‚É£ Matriz de confusi√≥n
cm_train <- confusionMatrix(pred_train_class$.pred_class, train$Exited)
cm_test  <- confusionMatrix(pred_test_class$.pred_class, test$Exited)

# 7Ô∏è‚É£ Funci√≥n F1 personalizada
f1_score <- function(cm) {
  2 * cm$byClass["Sensitivity"] * cm$byClass["Precision"] /
    (cm$byClass["Sensitivity"] + cm$byClass["Precision"])
}

# 8Ô∏è‚É£ Crear tabla resumen
results <- data.frame(
  p = best_params,  # si quieres mostrar los hiperpar√°metros
  F1_Train = f1_score(cm_train),
  F1_Test  = f1_score(cm_test),
  Sensitivity_Train = cm_train$byClass["Sensitivity"],
  Sensitivity_Test  = cm_test$byClass["Sensitivity"],
  Specificity_Train = cm_train$byClass["Specificity"],
  Specificity_Test  = cm_test$byClass["Specificity"],
  Accuracy_Train = cm_train$overall["Accuracy"],
  Accuracy_Test  = cm_test$overall["Accuracy"]
)

results
cm_train
cm_test
```


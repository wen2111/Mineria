res.famd <- FAMD(train,ncp = 20, graph = FALSE)
print(res.famd)
eig.val <- get_eigenvalue(res.famd)
eig.val
train<-train[,-1]
View(train)
res.famd <- FAMD(train,ncp = 20, graph = FALSE)
print(res.famd)
eig.val <- get_eigenvalue(res.famd)
eig.val
famd_features <- res.famd$ind$coord[1:19]
famd_features
famd_features <- res.famd$ind$coord[1:19,]
var <- get_famd_var(res.famd)
head(var$coord)
head(var$cos2)
head(var$contrib)
fviz_famd_var(res.famd, repel = TRUE)
View(famd_features)
eig.val <- get_eigenvalue(res.famd)
eig.val
famd_features <- res.famd$ind$coord[1:19,]
dim(famd_features)
eig.val <- get_eigenvalue(res.famd)
eig.val
famd_features <- res.famd$ind$coord[,1:19]
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
set.seed(123)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
knitr::opts_chunk$set(echo = TRUE)
library(vcd) # Para la matriz de confusión
library(caret) # Para la matriz de confusión
library(pROC) # Para calculos de la ROC
library(caret)
library(class)
set.seed(123)
load("C:/Users/95409/OneDrive/ESTADISTICA/4r/1/MD/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
data <- data_transformada
summary(data)
data<-data_imputado
lapply(data[data$group=="train",], anyNA)
plot(data$Exited)
(table(as.character(data$Exited))/sum(table(as.character(data$Exited))))*100
set.seed(123)
train_df <- data[data$group == "train" & !is.na(data$Exited), ]
train_df <- subset(train_df, select = -c(ID-, Surname, group))
set.seed(123)
train_df <- data[data$group == "train" & !is.na(data$Exited), ]
train_df <- subset(train_df, select = -c(ID, Surname, group))
train_df$Exited <- factor(train_df$Exited, levels = c("1","0"))
ind_col <- which(names(train_df) == "Exited") # columna de indicador
default_idx <- createDataPartition(train_df$Exited, p = 0.7, list = FALSE) # 70% para entrenamiento
X_trainC <- train_df[default_idx, ]
X_testC  <- train_df[-default_idx, ]
y_testC <- X_testC[, ind_col]
X_testC <- X_testC[, -ind_col]
# muestra <- sample(1:nrow(train_df), size = nrow(train_df)/3)
# train<-data.frame(train_df[-muestra,])
# test<-data.frame(train_df[muestra,])
# cl<-data$Exited[-muestra]  # variable output que queremos pronosticar.
library(caret)
# Crear preprocesador para centrado y escalado
preproc <- preProcess(X_trainC, method = c("center", "scale"))
# Aplicar transformación
X_trainC <- predict(preproc, X_trainC)
X_testC <- predict(preproc, X_testC)
y_trainC <- factor(ifelse(as.character(X_trainC$Exited) == "1", "Yes", "No"), levels = c("Yes","No"))
y_testC  <- factor(ifelse(as.character(y_testC) == "1", "Yes", "No"), levels = levels(y_trainC))
X_train_feat <- X_trainC[, setdiff(names(X_trainC), "Exited")]
X_test_feat  <- X_testC
# Convertir variables categóricas en dummies
dmy <- dummyVars(~ ., data = X_train_feat, fullRank = TRUE)
train_num <- data.frame(predict(dmy, newdata = X_train_feat))
test_num  <- data.frame(predict(dmy, newdata = X_test_feat))
# Escalar las variables numéricas
preproc <- preProcess(train_num, method = c("center","scale"))
trainScaled <- predict(preproc, train_num)
testScaled  <- predict(preproc, test_num)
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)
entrenamiento <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = seq(1, 20, by = 1))
)
entrenamiento
entrenamiento$modelType
pred_prob  <- predict(entrenamiento, newdata = testScaled, type = "prob")
head(pred_prob, 10)
get_best_result = function(caret_fit) {
best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
best_result = caret_fit$results[best, ]
rownames(best_result) = NULL
best_result
}
head(entrenamiento$results, 16)
plot(entrenamiento)
pred_label <- predict(entrenamiento, newdata = testScaled)
cm_cv <- confusionMatrix(pred_label, y_testC, positive = "Yes")
cm_cv
fit_knn_15 <- train(
x = trainScaled,
y = y_trainC,
method = "knn",
trControl = trainControl(method="cv", number=5, classProbs=TRUE),
tuneGrid = data.frame(k = 15)
)
fit_knn_15
confusionMatrix(predict(fit_knn_15, newdata=testScaled), y_testC)
ctrl <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "down")
set.seed(123)
model_rf_under <- train(classes ~ .,
data = train_data,
method = "rf",
preProcess = c("scale", "center"),
trControl = ctrl)
famd_features <- res.famd$ind$coord
# añadir "group" a data_reducida (no estaba)
data_reducida$group <- data_imputado$group
identical(rownames(data_reducida), rownames(data_imputado)) # comprobar que coinciden las filas
# subset y preparacion
train_reducida <- subset(data_reducida, group == "train") # 7000 obs
test_reducida  <- subset(data_reducida,
group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("ID", "Surname", "group")
train_reducida <- train_reducida[, !(names(train_reducida) %in% vars_drop)]
test_reducida  <- test_reducida[,  !(names(test_reducida) %in% vars_drop)]
# levels para "exited"
train_reducida$Exited <- factor(train_reducida$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# niveles en orden correcto para la conf matrix
levels(train_reducida$Exited)
# Debería mostrar: [1] "Yes" "No"
# particion
set.seed(1234)
index <- createDataPartition(train_reducida$Exited, p = 0.7, list = FALSE)
train_reducida2 <- train_reducida[index, ] # train interno
test_reducida2  <- train_reducida[-index, ] # test interno
# verifico no hay missings por si a caso
gg_miss_var(train_reducida2)
gg_miss_var(test_reducida2)
# verifico coinciden proporciones de "Exited"
prop.table(table(train_reducida$Exited))
prop.table(table(train_reducida2$Exited))
prop.table(table(test_reducida2$Exited))
# MODELADO
##################
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "down")
mod9 <- train(
y = train_reducida2$Exited,
x = train_reducida2[, !names(train_reducida2) %in% "Exited"],  # Todas menos la variable respuesta
method = "nb",
preProcess = c("scale", "center"),
trControl = control
)
mod9
# Predicciones
train_pred <- predict(mod9, train_reducida2, type = "raw")
test_pred  <- predict(mod9, test_reducida2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train_reducida2$Exited)
conf_test  <- confusionMatrix(test_pred, test_reducida2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
f1_train
f1_test
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
###### CONCLUSION: F1-Score<0.5 descartado #######
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "rose")
mod9 <- train(
y = train_reducida2$Exited,
x = train_reducida2[, !names(train_reducida2) %in% "Exited"],  # Todas menos la variable respuesta
method = "nb",
preProcess = c("scale", "center"),
trControl = control
)
mod9
# Predicciones
train_pred <- predict(mod9, train_reducida2, type = "raw")
test_pred  <- predict(mod9, test_reducida2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train_reducida2$Exited)
conf_test  <- confusionMatrix(test_pred, test_reducida2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
f1_train
f1_test
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
###### CONCLUSION: F1-Score<0.5 descartado #######
load("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
View(data_transformada)
View(data_transformada)
View(data_reducida)
View(data_transformada)
data_reducida_plus$EstimatedSalary<-data_transformada$EstimatedSalary
data_reducida_plus$CreditScore<-data_transformada$CreditScore
# Bayes
data_reducida_plus<-data_reducida
data_reducida_plus$EstimatedSalary<-data_transformada$EstimatedSalary
data_reducida_plus$CreditScore<-data_transformada$CreditScore
save.image("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
View(data_reducida_plus)
View(data_transformada)
data_reducida$group<-data_transformada$group
data_reducida$group<-data_transformada$group
data_reducida_plus$group<-data_transformada$group
save.image("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
View(data_transformada)
View(data_imputado)
vars_drop <- c("ID", "Surname")
data_transformada<-data_transformada[, !(names(data_transformada) %in% vars_drop)]
data_imputado <- data_imputado[,  !(names(data_imputado) %in% vars_drop)]
View(data_imputado)
View(data_transformada)
save.image("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
#DATA
data<-data_reducida_plus
# Separacion
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group")
train <- train[, !(names(train) %in% vars_drop)]
test <- test[,  !(names(test) %in% vars_drop)]
summç
str(data_reducida_plus)
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
View(train2)
View(test2)
View(data_imputado)
View(data_reducida)
View(data_reducida_plus)
View(data_transformada)
#DATA
data<-data_reducida_plus
data <- data %>% select(Exited, everything())
data <- data %>% select(Exited, everything())
library(dplyr)
#DATA
data<-data_reducida_plus
data <- data %>% select(Exited, everything())
# Extraccion de train i test(kaggle)
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group")
train <- train[, !(names(train) %in% vars_drop)]
test <- test[,  !(names(test) %in% vars_drop)]
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "rose")
hiperparametros <- data.frame(usekernel = FALSE, fL = 1, adjust=0)
mod <- train(y=train2$Exited, x= train2[,c(2:21)],
data = train_imputado2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "rose")
hiperparametros <- data.frame(usekernel = FALSE, fL = 1, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
mod <- train(y=train2$Exited, x= train2[,c(2:10)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
#DATA
data<-data_reducida_plus
data <- data %>% select(Exited, everything())
# Extraccion de train i test(kaggle)
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group")
train <- train[, !(names(train) %in% vars_drop)]
test <- test[,  !(names(test) %in% vars_drop)]
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
# TRAIN I TEST NUESTRO
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
# MODELADO
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "rose")
hiperparametros <- data.frame(usekernel = FALSE, fL = 1, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
mod <- train(y=train2$Exited, x= train2[,c(2:10)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
dim(train2)
View(data_reducida_plus)
View(data)
#DATA
data<-data_reducida_plus
#DATA
data<-data_reducida_plus
load("C:/Users/34688/OneDrive - Universitat de Barcelona/Escritorio/Mineria/DATA/dataaaaaaaaaaaaaa.RData")
#DATA
data<-data_reducida_plus
data <- data %>% select(Exited, everything())
# Extraccion de train i test(kaggle)
train <- subset(data, group == "train") # 7000 obs
test <- subset(data,group == "test") # 3000 obs variable respuesta vacia
vars_drop <- c("group")
train <- train[, !(names(train) %in% vars_drop)]
test <- test[,  !(names(test) %in% vars_drop)]
# levels para "exited" porque lo exige bayes
train$Exited <- factor(train$Exited,
levels = c("1","0"),
labels = c("Yes","No"))
set.seed(123)
index <- createDataPartition(train$Exited, p = 0.7, list = FALSE)
train2 <- train[index, ] # train interno
test2  <- train[-index, ] # test interno
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "rose")
hiperparametros <- data.frame(usekernel = FALSE, fL = 1, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
mod <- train(y=train2$Exited, x= train2[,c(2:10)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
mod <- train(y=train2$Exited, x= train2[,c(2:9)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# Predicciones
train_pred <- predict(mod7, train2, type = "raw")
test_pred  <- predict(mod7, test2, type = "raw")
# Predicciones
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
hiperparametros <- data.frame(usekernel = TRUE, fL = 1, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
mod <- train(y=train2$Exited, x= train2[,c(2:9)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "rose")
hiperparametros <- data.frame(usekernel = TRUE, fL = 1, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
mod <- train(y=train2$Exited, x= train2[,c(2:9)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
control <- trainControl(method = "repeatedcv",
number = 10,
repeats = 10,
verboseIter = FALSE,
sampling = "rose")
hiperparametros <- data.frame(usekernel = TRUE, fL = 0, adjust=0)
# tener cuidado con la x, hay que tener en cuenta el rango de datos
mod <- train(y=train2$Exited, x= train2[,c(2:9)],
data = train2,
method = "nb",
tuneGrid = hiperparametros,
metric = "Accuracy",
trControl = control)
# Predicciones
train_pred <- predict(mod, train2, type = "raw")
test_pred  <- predict(mod, test2, type = "raw")
# Matrices de confusión
conf_train <- confusionMatrix(train_pred, train2$Exited)
conf_test  <- confusionMatrix(test_pred, test2$Exited)
# F1-score
f1_score <- function(cm){
precision <- cm$byClass["Precision"]
recall    <- cm$byClass["Sensitivity"]
f1 <- 2 * (precision * recall) / (precision + recall)
return(as.numeric(f1))
}
f1_train <- f1_score(conf_train)
f1_test  <- f1_score(conf_test)
# KPIs
data.frame(
Dataset = c("Train", "Test"),
Error_rate = c(1-conf_train$overall["Accuracy"], 1-conf_test$overall["Accuracy"]),
Accuracy = c(conf_train$overall["Accuracy"], conf_test$overall["Accuracy"]),
Precision = c(conf_train$byClass["Pos Pred Value"], conf_test$byClass["Pos Pred Value"]),
Recall_Sensitivity = c(conf_train$byClass["Sensitivity"], conf_test$byClass["Sensitivity"]),
Specificity = c(conf_train$byClass["Specificity"], conf_test$byClass["Specificity"]),
F1_Score = c(f1_train, f1_test)
)
